{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"An Hackable Games' Engine Welcome to the HackaGames documentation. HackaGames is an open game engine dedicated to the development of Artificial Intelligence (AI) based on Combinatorial Optimization (CO) technique. The philosophy of hackagames is to permit developers to easily work in any language of thier choice. For that, the project is based on a communication protocol relying on ZeroMQ and is developed accordingly to KISS (Keep It Stupid Simple) principle. The main feature of this project is to permit the game, players and AIs to work on their own process potentially distributed over different machines. In other terms, HackaGames implements a simple client/server architecture to permit AI to take a seat on a game by agreeing on a communication protocol. HackaGames is seen as an API for game development. Several games are proposed for example: Py421 (Python): A very simple one player dice game to get the concept of AI implementation (not a core HackaGames client/server game). TicTacToe (Python): Classic and Ultimate TicTacToe game. Risky (Python): a simple turn-based strategic game. The project itself and it source code are available on github as a git repository. Install Simplely clone the git repository and use python pip . git clone https://github.com/ktorz-net/hackagames pip install ./hackagames For a more detail see the install page of this tutorial. Getting Started The easiest way is to play with one of the proposed python3 games, Py421 for instance. Concurency: HackaGame is not what you are looking for ? Try those solutions: ludii a general game system designed to play, evaluate and design a wide range of games\" (JAVA) pettingzoo of farama multi-agent learning framework (Python) pommerman a hackable Bomberman game (Python) codingame web-based environment for NPC development (complete solution for one file codes). Roblox an online game platform and game creation system that allows users to program games and play games created by other users. Godot Open Source Game engine (dev in Cpp) (or even more K.I.S with RayLib - dev in C).","title":"Home"},{"location":"#an-hackable-games-engine","text":"Welcome to the HackaGames documentation. HackaGames is an open game engine dedicated to the development of Artificial Intelligence (AI) based on Combinatorial Optimization (CO) technique. The philosophy of hackagames is to permit developers to easily work in any language of thier choice. For that, the project is based on a communication protocol relying on ZeroMQ and is developed accordingly to KISS (Keep It Stupid Simple) principle. The main feature of this project is to permit the game, players and AIs to work on their own process potentially distributed over different machines. In other terms, HackaGames implements a simple client/server architecture to permit AI to take a seat on a game by agreeing on a communication protocol. HackaGames is seen as an API for game development. Several games are proposed for example: Py421 (Python): A very simple one player dice game to get the concept of AI implementation (not a core HackaGames client/server game). TicTacToe (Python): Classic and Ultimate TicTacToe game. Risky (Python): a simple turn-based strategic game. The project itself and it source code are available on github as a git repository.","title":"An Hackable Games' Engine"},{"location":"#install","text":"Simplely clone the git repository and use python pip . git clone https://github.com/ktorz-net/hackagames pip install ./hackagames For a more detail see the install page of this tutorial.","title":"Install"},{"location":"#getting-started","text":"The easiest way is to play with one of the proposed python3 games, Py421 for instance.","title":"Getting Started"},{"location":"#concurency","text":"HackaGame is not what you are looking for ? Try those solutions: ludii a general game system designed to play, evaluate and design a wide range of games\" (JAVA) pettingzoo of farama multi-agent learning framework (Python) pommerman a hackable Bomberman game (Python) codingame web-based environment for NPC development (complete solution for one file codes). Roblox an online game platform and game creation system that allows users to program games and play games created by other users. Godot Open Source Game engine (dev in Cpp) (or even more K.I.S with RayLib - dev in C).","title":"Concurency:"},{"location":"f.a.q/","text":"F.A.Q Are we forced to use the client-server architecture ? It is possible to shunt network architecture if the game and all the players' codes are in the same language (in python for instance). In that case, a game can generally be started in a test mode with all its players in one unique process (cf. the start-interactive scripts). For instance, for Py421 solo game, you have to edit a python script launcherPy421.py . The code requires to import the game and the players (only one here) and to instantiate them with the testPlayer method: #!env python3 from hackagames.gamePy421.gameEngine import GameSolo as Game from tutos.myPy421Bot import AutonomousPlayer as Player # Instanciate and start 100 games game= Game() player= Player() results= game.testPlayer( player, 100 ) That it, you can execute your script: python3 ./tutos/launcherPy421.py which calls your player. The second attribute in testPlayer method of game instance ( 100 here) is the number of games the players will play before the process end. Is there some courses ? yes, here: on the repo of the courses","title":"F.A.Q"},{"location":"f.a.q/#faq","text":"","title":"F.A.Q"},{"location":"f.a.q/#are-we-forced-to-use-the-client-server-architecture","text":"It is possible to shunt network architecture if the game and all the players' codes are in the same language (in python for instance). In that case, a game can generally be started in a test mode with all its players in one unique process (cf. the start-interactive scripts). For instance, for Py421 solo game, you have to edit a python script launcherPy421.py . The code requires to import the game and the players (only one here) and to instantiate them with the testPlayer method: #!env python3 from hackagames.gamePy421.gameEngine import GameSolo as Game from tutos.myPy421Bot import AutonomousPlayer as Player # Instanciate and start 100 games game= Game() player= Player() results= game.testPlayer( player, 100 ) That it, you can execute your script: python3 ./tutos/launcherPy421.py which calls your player. The second attribute in testPlayer method of game instance ( 100 here) is the number of games the players will play before the process end.","title":"Are we forced to use the client-server architecture ?"},{"location":"f.a.q/#is-there-some-courses","text":"yes, here: on the repo of the courses","title":"Is there some courses ?"},{"location":"games/connect4/","text":"Connect4 Connect4 is a HackaGames game. Connect4 is simple two-player games where each player tries to align 4 of their pieces. Try the game: The start-interactive script starts the game with an interactive interface in a shell playing against an artificial player playerFirstAI.py ( ./hackagames/gameTictactoe/start-interactive ). The player can perform one and only one action at its turn, and the game stops automatically with a winner or when no more pieces can be set on the grid. The actions consist of positioning a player's piece on one of the grid columns. There are 7 possible actions: A , B , C , D , E , F and G for the corresponding column. Example of the game at some point: A B C D E F G | | | | | | | | | | | | | | | | | | | | | | | | | | | | O | | | | | | | X | X | | | | | X | O | O | O | | | X | ----------------------------- 1:O, 2:X First player play 'O' and the second 'X'. The first player aligning 4 of its pieces, in any direction win the game. Initialize an Autonomous Player Tutorial of 4.2.1. Solo game presents step by step what a bot player should look like. And more importantly, the tutorial explains all the 4 Player's class methods required by HackaGames ( wakeUp , perceive and sleep ). Here, we propose a first player for Connect4 , playing randomly (supposingly in a tutos directory, aside of hackagames ): import sys, random sys.path.insert(1, __file__.split('tutos')[0]) import hackagames.hackapy.player as pl from hackagames.gameConnect4.gameEngine import Grid def main(): player= BotPlayer() result= player.takeASeat() print( f\"Average: {sum(result)/len(result)}\" ) class BotPlayer( pl.AbsPlayer ): def __init__(self): super().__init__() self.grid= Grid() self.playerId= 0 # Player interface : def wakeUp(self, playerId, numberOfPlayers, gamePod): self.playerId= playerId assert( gamePod.family() == 'Connect4') def perceive(self, gameState): # update the game state: self.grid.fromPod( gameState ) def decide(self): options = self.grid.possibilities() action = random.choice( options ) return action def sleep(self, result): pass # script if __name__ == '__main__' : main() Compared to other games, Connect4 players can take advantage of an object modeling the grid. The grid is updated during the perception phase, and it is requested during the decision phase. In this example, the player get all the possible actions ( self.grid.possibilities() ) A , B , C etc. Then an action is selected randomly. To play again your bot (into 3 shell:) # In the first shell: python3 ./hackagames/gameConnect4/start-server # In the second shell: python3 ./tutos/yourConnect4Bot.py # In the third shell: python3 ./hackagames/connect-shell Customaize your AI: You can explore the grid to select the best action possible. Some useful Gird instance methods: columnSize(self) : The number of columns ( 7 in classical configuration) heightMax(self) : The height of the grid ( 6 position per column in classical configuration) height(self, iColumn) : the actual height of the iColumn column (considered the played piece in this column) column(self, iColumn) : a list of integers modeling the iColumn column pieces ( 0 no player, 1 player 1 and 2 player 2 ) position(self, c, h) : The value at column c height h ( 0 , 1 or 2 ). To test a move you will need: copy(self) : returning a deep copy of the grid before to make some experiments playerPlay(self, iPlayer, aLetter) : to alter the grid considering that player iPlayer play on aLetter column. winner(self) : returns the player Id winning the game if 4 of his pieces are aligned. possibilities(self) : returns the list of possible move (letters), if the columns are not full. To notice that you can move from column letter to the column identifier and vice versa with ch and ord functions (for instance: ord(aLetter)-ord('A') and chr( ord('A')+iColumn ) ).","title":"Connect4"},{"location":"games/connect4/#connect4","text":"Connect4 is a HackaGames game. Connect4 is simple two-player games where each player tries to align 4 of their pieces.","title":"Connect4"},{"location":"games/connect4/#try-the-game","text":"The start-interactive script starts the game with an interactive interface in a shell playing against an artificial player playerFirstAI.py ( ./hackagames/gameTictactoe/start-interactive ). The player can perform one and only one action at its turn, and the game stops automatically with a winner or when no more pieces can be set on the grid. The actions consist of positioning a player's piece on one of the grid columns. There are 7 possible actions: A , B , C , D , E , F and G for the corresponding column. Example of the game at some point: A B C D E F G | | | | | | | | | | | | | | | | | | | | | | | | | | | | O | | | | | | | X | X | | | | | X | O | O | O | | | X | ----------------------------- 1:O, 2:X First player play 'O' and the second 'X'. The first player aligning 4 of its pieces, in any direction win the game.","title":"Try the game:"},{"location":"games/connect4/#initialize-an-autonomous-player","text":"Tutorial of 4.2.1. Solo game presents step by step what a bot player should look like. And more importantly, the tutorial explains all the 4 Player's class methods required by HackaGames ( wakeUp , perceive and sleep ). Here, we propose a first player for Connect4 , playing randomly (supposingly in a tutos directory, aside of hackagames ): import sys, random sys.path.insert(1, __file__.split('tutos')[0]) import hackagames.hackapy.player as pl from hackagames.gameConnect4.gameEngine import Grid def main(): player= BotPlayer() result= player.takeASeat() print( f\"Average: {sum(result)/len(result)}\" ) class BotPlayer( pl.AbsPlayer ): def __init__(self): super().__init__() self.grid= Grid() self.playerId= 0 # Player interface : def wakeUp(self, playerId, numberOfPlayers, gamePod): self.playerId= playerId assert( gamePod.family() == 'Connect4') def perceive(self, gameState): # update the game state: self.grid.fromPod( gameState ) def decide(self): options = self.grid.possibilities() action = random.choice( options ) return action def sleep(self, result): pass # script if __name__ == '__main__' : main() Compared to other games, Connect4 players can take advantage of an object modeling the grid. The grid is updated during the perception phase, and it is requested during the decision phase. In this example, the player get all the possible actions ( self.grid.possibilities() ) A , B , C etc. Then an action is selected randomly. To play again your bot (into 3 shell:) # In the first shell: python3 ./hackagames/gameConnect4/start-server # In the second shell: python3 ./tutos/yourConnect4Bot.py # In the third shell: python3 ./hackagames/connect-shell","title":"Initialize an Autonomous Player"},{"location":"games/connect4/#customaize-your-ai","text":"You can explore the grid to select the best action possible. Some useful Gird instance methods: columnSize(self) : The number of columns ( 7 in classical configuration) heightMax(self) : The height of the grid ( 6 position per column in classical configuration) height(self, iColumn) : the actual height of the iColumn column (considered the played piece in this column) column(self, iColumn) : a list of integers modeling the iColumn column pieces ( 0 no player, 1 player 1 and 2 player 2 ) position(self, c, h) : The value at column c height h ( 0 , 1 or 2 ). To test a move you will need: copy(self) : returning a deep copy of the grid before to make some experiments playerPlay(self, iPlayer, aLetter) : to alter the grid considering that player iPlayer play on aLetter column. winner(self) : returns the player Id winning the game if 4 of his pieces are aligned. possibilities(self) : returns the list of possible move (letters), if the columns are not full. To notice that you can move from column letter to the column identifier and vice versa with ch and ord functions (for instance: ord(aLetter)-ord('A') and chr( ord('A')+iColumn ) ).","title":"Customaize your AI:"},{"location":"games/moveIt/","text":"MoveIt MoveIt is a HackaGames game. MoveIt is a based on collision-free path planning problems, where several mobiles has each a position reach in a clustered environment. In the simplest form, you control a robot in an environment with humans. This tutorial is based only on Python3 . Try the game: The start-interactive script starts the game with an interactive interface in a shell. python3 hackagames/gameMoveIt/start-interactive Example of game configuration: \u2581 \u2581 \u2581 \u2581 \u2581 \u2581 \u2596\u259d\u2581\u2581\u2581\u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2584\u259f\u2588\u2588\u2588\u2599\u2584 \u2588 \u239bH \u239e \u2588 \u2588 \u2588 \u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588 \u239d 2\u23a0 \u2588 \u2588 \u2588 \u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2584\u259f\u2588\u2588\u2588\u2599\u2584\u2594\u2594\u2594\u2596\u259d\u2581\u2581\u2581\u2598\u2597 \u2584\u259f\u2588\u2588\u2588\u2599\u2584 \u2584\u259f\u2588\u2588\u2588\u2599\u2584 \u2584\u259f\u2588\u2588\u2588\u2599\u2584 \u2596\u259d \u2580\u259c\u2588\u2588\u2588\u259b\u2580 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u239bH \u239e \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u23a1 \u23a4 \u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u239d 3\u23a0 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u23a3 \u23a61\u2588 \u2580\u259c\u2588\u2588\u2588\u259b\u2580 \u2598\u2597\u2594\u2594\u2594\u2596\u259d \u2580\u259c\u2588\u2588\u2588\u259b\u2580 \u2580\u259c\u2588\u2588\u2588\u259b\u2580\u2581\u2581\u2581\u2580\u259c\u2588\u2588\u2588\u259b\u2580 \u2598\u2597 \u2596\u259d \u2598\u2597 \u2588 \u2588 \u2588 \u2588 \u239bR \u239e \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u239d 1\u23a0 \u2588 \u2588 \u2588 \u2584\u259f\u2588\u2588\u2588\u2599\u2584 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597\u2594\u2594\u2594\u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2580\u259c\u2588\u2588\u2588\u259b\u2580 \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2594 \u2594 \u2594 \u2594 \u2594 \u2594 The controlled robot is modeled as R 1 and want to reach the position labialized has 1 . The environment also includes 2 humans H 1 and H 2 with unknown goals position. The robot can move to one of the 6 cells next to its own position: move X with \\(X \\in [0, 1, 2, 3, 4, 5, 6]\\) ( 0 means the robots stay on it position). The robot can also pass . This special action makes it abandon the mission and trigger a new task to fulfill. \u2581 \u2581 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2588 \u2588 \u2588 \u2588 6 \u2588 1 \u2588 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2588 \u2588 \u239bR \u239e \u2588 \u2588 \u2588 5 \u2588 \u239d 1\u23a0 \u2588 2 \u2588 \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2588 \u2588 \u2588 \u2588 4 \u2588 3 \u2588 \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2594 \u2594 The robot has to reach a maximum of tasks before the end of the counter while avoiding any collisions with the humans. Initialize a Bot environment First we can prepare a launcher for MoveIt game. Put the next code into a file launch-moveIt.py aside hackagames directory: from hackagames.gameMoveIt.gameEngine import GameMoveIt from tutos.basicBot import Bot game= GameMoveIt() results= game.testPlayer( Bot(), 4 ) print( f\"Average score: {sum(results)/len(results)}\" ) This script import moveIt game, a player and start \\(4\\) games. It requires that you instantiate a MoveIt bot on a tutos/basicBot.py file. This class Bot should implement the wakeUp , perceive , decide and sleep methods. To simplify the development, the bot can rely on MoveIt Hexaboard and Mobile classes to model the world and the mobile elements (the robot and the \\(2\\) humans). First you have to import access to those elements: #!env python3 \"\"\" HackaGame player interface \"\"\" import sys, os sys.path.insert( 1, __file__.split('tutos')[0] ) import hackagames.gameMoveIt.gameEngine as ge A first Basic Bot: Then you have to create the require Bot attributes in the wakeUp method: # Player interface : def wakeUp(self, playerId, numberOfPlayers, gamePod): # Initialize from gamePod: self._board= ge.Hexaboard() self._board.fromPod( gamePod ) nbRobots, nbMobiles= gamePod.flag(3), gamePod.flag(4) self._mobiles= ge.defineMobiles( nbRobots, nbMobiles ) self._board.setupMobiles( self._mobiles ) # Initialize state variable: self._countTic= 0 self._countCycle= 0 self._score= 0 This way the update of the game state and the computation of a path in the environment will be very simple: def perceive(self, statePod): # update the game state: self._board.mobilesFromPod( statePod ) self._countTic= statePod.flag(1) self._countCycle= statePod.flag(2) self._score= statePod.value(1) def decide(self): action= \"move\" robot= self._mobiles[0] path= self._board.path( robot.x(), robot.y(), robot.goalx(), robot.goaly() ) dir= path[0] action+= \" \" + str(dir) return action For debugging purpose you can add print based on board method. For instance, before to return the action: print( self._board.shell() ) print( f\"counters{(self._countTic, self._countCycle)}, score({self._score})\" ) print( f\"Action: {action}\" ) A more Social Bot: You can now explore the state information to better control the robot. i.e: at least do not enter a cell already occupied by a human... MoveIt Class Accessors : Source code: bitbucket.org/imt-mobisyst/hackagames Mobile : # Accessor: def number(self): def x(self): def y(self): def position(self): def direction(self): def goal(self): def goalx(self): def goaly(self): def isRobot(self): def isHuman(self): Hexaboard : # Accessors: def size(self): # return a tuple. The number of cells in a lines and the number of lines def at(self, x, y): # return the Cell at position x, y def at_dir(self, x, y, i): # return the coordinates in the direction `dir` from the position x, y # dirx, diry= board.at_dir( x, y, dir ) def movesFrom(self, x, y): # return the possible move direction from position x, y def isCoordinate(self, x, y):# return TRUE if x, y is on the board. def cellsType(self, aType ): # return all the list of coordinates # of all the cells of a given type (Cell.TYPE_FREE or Cell.TYPE_OBSTACLES) def cellsEmpty( self ): # return all the list of coordinates # Cell.TYPE_FREE and empty cells def isObstacleOkAt(self, x, y): # return TRUE if cell at x, y is an obstacle def teleportMobile(self, x, y, tx, ty): # teleport a mobile def moveMobileAt_dir(self, x, y, dir): # move a mobile in a given direction def path(self, x1, y1, x2, y2): # compute a obstacle free path betwen a start and a target positions def shell(self): # Generate shell drawing of the game Cells : TYPE_FREE= 0 TYPE_OBSTACLE= 1 # Accessors: def mobile(self): # return the mobile in the board cell (False if no mobile present) def isObstacle(self): # if the cell is an obstacle def isAvailable(self):# if the cell is free and without a mobile present","title":"MoveIt"},{"location":"games/moveIt/#moveit","text":"MoveIt is a HackaGames game. MoveIt is a based on collision-free path planning problems, where several mobiles has each a position reach in a clustered environment. In the simplest form, you control a robot in an environment with humans. This tutorial is based only on Python3 .","title":"MoveIt"},{"location":"games/moveIt/#try-the-game","text":"The start-interactive script starts the game with an interactive interface in a shell. python3 hackagames/gameMoveIt/start-interactive Example of game configuration: \u2581 \u2581 \u2581 \u2581 \u2581 \u2581 \u2596\u259d\u2581\u2581\u2581\u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2584\u259f\u2588\u2588\u2588\u2599\u2584 \u2588 \u239bH \u239e \u2588 \u2588 \u2588 \u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588 \u239d 2\u23a0 \u2588 \u2588 \u2588 \u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2584\u259f\u2588\u2588\u2588\u2599\u2584\u2594\u2594\u2594\u2596\u259d\u2581\u2581\u2581\u2598\u2597 \u2584\u259f\u2588\u2588\u2588\u2599\u2584 \u2584\u259f\u2588\u2588\u2588\u2599\u2584 \u2584\u259f\u2588\u2588\u2588\u2599\u2584 \u2596\u259d \u2580\u259c\u2588\u2588\u2588\u259b\u2580 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u239bH \u239e \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u23a1 \u23a4 \u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u239d 3\u23a0 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u23a3 \u23a61\u2588 \u2580\u259c\u2588\u2588\u2588\u259b\u2580 \u2598\u2597\u2594\u2594\u2594\u2596\u259d \u2580\u259c\u2588\u2588\u2588\u259b\u2580 \u2580\u259c\u2588\u2588\u2588\u259b\u2580\u2581\u2581\u2581\u2580\u259c\u2588\u2588\u2588\u259b\u2580 \u2598\u2597 \u2596\u259d \u2598\u2597 \u2588 \u2588 \u2588 \u2588 \u239bR \u239e \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u239d 1\u23a0 \u2588 \u2588 \u2588 \u2584\u259f\u2588\u2588\u2588\u2599\u2584 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597\u2594\u2594\u2594\u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2580\u259c\u2588\u2588\u2588\u259b\u2580 \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2594 \u2594 \u2594 \u2594 \u2594 \u2594 The controlled robot is modeled as R 1 and want to reach the position labialized has 1 . The environment also includes 2 humans H 1 and H 2 with unknown goals position. The robot can move to one of the 6 cells next to its own position: move X with \\(X \\in [0, 1, 2, 3, 4, 5, 6]\\) ( 0 means the robots stay on it position). The robot can also pass . This special action makes it abandon the mission and trigger a new task to fulfill. \u2581 \u2581 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2588 \u2588 \u2588 \u2588 6 \u2588 1 \u2588 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2588 \u2588 \u239bR \u239e \u2588 \u2588 \u2588 5 \u2588 \u239d 1\u23a0 \u2588 2 \u2588 \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2588 \u2588 \u2588 \u2588 4 \u2588 3 \u2588 \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2594 \u2594 The robot has to reach a maximum of tasks before the end of the counter while avoiding any collisions with the humans.","title":"Try the game:"},{"location":"games/moveIt/#initialize-a-bot-environment","text":"First we can prepare a launcher for MoveIt game. Put the next code into a file launch-moveIt.py aside hackagames directory: from hackagames.gameMoveIt.gameEngine import GameMoveIt from tutos.basicBot import Bot game= GameMoveIt() results= game.testPlayer( Bot(), 4 ) print( f\"Average score: {sum(results)/len(results)}\" ) This script import moveIt game, a player and start \\(4\\) games. It requires that you instantiate a MoveIt bot on a tutos/basicBot.py file. This class Bot should implement the wakeUp , perceive , decide and sleep methods. To simplify the development, the bot can rely on MoveIt Hexaboard and Mobile classes to model the world and the mobile elements (the robot and the \\(2\\) humans). First you have to import access to those elements: #!env python3 \"\"\" HackaGame player interface \"\"\" import sys, os sys.path.insert( 1, __file__.split('tutos')[0] ) import hackagames.gameMoveIt.gameEngine as ge","title":"Initialize a Bot environment"},{"location":"games/moveIt/#a-first-basic-bot","text":"Then you have to create the require Bot attributes in the wakeUp method: # Player interface : def wakeUp(self, playerId, numberOfPlayers, gamePod): # Initialize from gamePod: self._board= ge.Hexaboard() self._board.fromPod( gamePod ) nbRobots, nbMobiles= gamePod.flag(3), gamePod.flag(4) self._mobiles= ge.defineMobiles( nbRobots, nbMobiles ) self._board.setupMobiles( self._mobiles ) # Initialize state variable: self._countTic= 0 self._countCycle= 0 self._score= 0 This way the update of the game state and the computation of a path in the environment will be very simple: def perceive(self, statePod): # update the game state: self._board.mobilesFromPod( statePod ) self._countTic= statePod.flag(1) self._countCycle= statePod.flag(2) self._score= statePod.value(1) def decide(self): action= \"move\" robot= self._mobiles[0] path= self._board.path( robot.x(), robot.y(), robot.goalx(), robot.goaly() ) dir= path[0] action+= \" \" + str(dir) return action For debugging purpose you can add print based on board method. For instance, before to return the action: print( self._board.shell() ) print( f\"counters{(self._countTic, self._countCycle)}, score({self._score})\" ) print( f\"Action: {action}\" )","title":"A first Basic Bot:"},{"location":"games/moveIt/#a-more-social-bot","text":"You can now explore the state information to better control the robot. i.e: at least do not enter a cell already occupied by a human...","title":"A more Social Bot:"},{"location":"games/moveIt/#moveit-class-accessors","text":"Source code: bitbucket.org/imt-mobisyst/hackagames Mobile : # Accessor: def number(self): def x(self): def y(self): def position(self): def direction(self): def goal(self): def goalx(self): def goaly(self): def isRobot(self): def isHuman(self): Hexaboard : # Accessors: def size(self): # return a tuple. The number of cells in a lines and the number of lines def at(self, x, y): # return the Cell at position x, y def at_dir(self, x, y, i): # return the coordinates in the direction `dir` from the position x, y # dirx, diry= board.at_dir( x, y, dir ) def movesFrom(self, x, y): # return the possible move direction from position x, y def isCoordinate(self, x, y):# return TRUE if x, y is on the board. def cellsType(self, aType ): # return all the list of coordinates # of all the cells of a given type (Cell.TYPE_FREE or Cell.TYPE_OBSTACLES) def cellsEmpty( self ): # return all the list of coordinates # Cell.TYPE_FREE and empty cells def isObstacleOkAt(self, x, y): # return TRUE if cell at x, y is an obstacle def teleportMobile(self, x, y, tx, ty): # teleport a mobile def moveMobileAt_dir(self, x, y, dir): # move a mobile in a given direction def path(self, x1, y1, x2, y2): # compute a obstacle free path betwen a start and a target positions def shell(self): # Generate shell drawing of the game Cells : TYPE_FREE= 0 TYPE_OBSTACLE= 1 # Accessors: def mobile(self): # return the mobile in the board cell (False if no mobile present) def isObstacle(self): # if the cell is an obstacle def isAvailable(self):# if the cell is free and without a mobile present","title":"MoveIt Class Accessors :"},{"location":"games/py421-duo/","text":"Py421, Duo Mode Py421 is an HackaGames game. After mastering the solo version of Py421 , it is times for a 2-players' confrontation. Tow player version of Py421 can be started with duo parameters. Then you require to start 2 different players in separated shells. For instance, an AI and the Shell Human interface. # In the first shell: python3 ./hackagames/gamePy421/start-server duo # In the second shell: python3 ./hackagames/gamePy421/playerFirstAI.py # In the third shell: python3 ./hackagames/connect-shell In this version of the game, it is not necessary to reach the best combination possible. Players just wan the best one between them. The score at the end would be: \\(1\\) , \\(-1\\) , or \\(0\\) depending on who win the game and if there is a winner. 2 Player Rules The Py421 is an asymmetric player game. The first player fix the game parameters for the second. The first player has an interface very similar to 421 Solo . He has 2 rolling step possibility to optimize the combination the second player has to beat. The second player, at its turn, knows the combination to beat. However, he cannot use more dices throws than the number used by the first player. In other terms, the first player fix the number of time the second player can roll again its dices. For instance, the first player gets the combination 6-5-4 at beginning and choose to keep it. The second player will not have the possibility to roll again its dices neither. 6-5-4 is not the best combination possible but the probability that player-2 get a better one with only one throw is low. Modification at Perception Step In practice, compared to 421 Solo , the player can observe the opponent dices. The gameState attribute of the perceive methods comes with a third child for the opponent dices. def perceive(self, gameState): self.horizon= gameState.child(1).flag(1) self.dices= gameState.child(2).flags() self.op_dices= gameState.child(3).flags() print( f'H: {self.horizon} DICES: {self.dices} OPPONENTS {self.op_dices}' ) To notice that opponent dices ( self.op_dices ) would be tree \\(0\\) , if the player is the first player. For the second player, self.op_dices informs on the conbinaison to beat. However, the horizon will not necessarily be \\(2\\) at it first player activation. In fact, he can see the end of the game coming while he never decide anything (the first player keep-keep-keep at its first action). Optimize your AI A good AI for 421 Solo game will not play so bad in Duo mode but should be easy to beat, in average. Starting from your 421 Solo Bot, we recommend to tunes two different decision-making methods. cp ./tutos/myPy421Bot.py ./tutos/myPy421DuoBot.py One when your player plays first trying to stop with a good enough combination with a minimum of dices throws. The second, when your player plays second, with a strategy not focused on the best combination any more, but on the probability to reach a better one compared to the opponent combination. Most of the time, a simple \\(6\\) beat a \\(5\\) .","title":"Py421, Duo Mode"},{"location":"games/py421-duo/#py421-duo-mode","text":"Py421 is an HackaGames game. After mastering the solo version of Py421 , it is times for a 2-players' confrontation. Tow player version of Py421 can be started with duo parameters. Then you require to start 2 different players in separated shells. For instance, an AI and the Shell Human interface. # In the first shell: python3 ./hackagames/gamePy421/start-server duo # In the second shell: python3 ./hackagames/gamePy421/playerFirstAI.py # In the third shell: python3 ./hackagames/connect-shell In this version of the game, it is not necessary to reach the best combination possible. Players just wan the best one between them. The score at the end would be: \\(1\\) , \\(-1\\) , or \\(0\\) depending on who win the game and if there is a winner.","title":"Py421, Duo Mode"},{"location":"games/py421-duo/#2-player-rules","text":"The Py421 is an asymmetric player game. The first player fix the game parameters for the second. The first player has an interface very similar to 421 Solo . He has 2 rolling step possibility to optimize the combination the second player has to beat. The second player, at its turn, knows the combination to beat. However, he cannot use more dices throws than the number used by the first player. In other terms, the first player fix the number of time the second player can roll again its dices. For instance, the first player gets the combination 6-5-4 at beginning and choose to keep it. The second player will not have the possibility to roll again its dices neither. 6-5-4 is not the best combination possible but the probability that player-2 get a better one with only one throw is low.","title":"2 Player Rules"},{"location":"games/py421-duo/#modification-at-perception-step","text":"In practice, compared to 421 Solo , the player can observe the opponent dices. The gameState attribute of the perceive methods comes with a third child for the opponent dices. def perceive(self, gameState): self.horizon= gameState.child(1).flag(1) self.dices= gameState.child(2).flags() self.op_dices= gameState.child(3).flags() print( f'H: {self.horizon} DICES: {self.dices} OPPONENTS {self.op_dices}' ) To notice that opponent dices ( self.op_dices ) would be tree \\(0\\) , if the player is the first player. For the second player, self.op_dices informs on the conbinaison to beat. However, the horizon will not necessarily be \\(2\\) at it first player activation. In fact, he can see the end of the game coming while he never decide anything (the first player keep-keep-keep at its first action).","title":"Modification at Perception Step"},{"location":"games/py421-duo/#optimize-your-ai","text":"A good AI for 421 Solo game will not play so bad in Duo mode but should be easy to beat, in average. Starting from your 421 Solo Bot, we recommend to tunes two different decision-making methods. cp ./tutos/myPy421Bot.py ./tutos/myPy421DuoBot.py One when your player plays first trying to stop with a good enough combination with a minimum of dices throws. The second, when your player plays second, with a strategy not focused on the best combination any more, but on the probability to reach a better one compared to the opponent combination. Most of the time, a simple \\(6\\) beat a \\(5\\) .","title":"Optimize your AI"},{"location":"games/py421-solo/","text":"Py421, Solo Mode Py421 is an HackaGames game, a simple dice game where players try to optimize their dices combination after a maximum of 2 roll dice steps. This tutorial is based only on Python3 . Try the game: The play/py421 script starts the game with an interactive interface in a shell. python3 hackagames/play/py421 Py421 is a 3-dice game. The player can roll several times to get a combination. The player can perform one and only one action on his turn, and the game stops automatically after 2 turns. The actions consist in keeping or rolling each of the 3 dices. So there are 8 possible actions: keep-keep-keep , keep-keep-roll , keep-roll-keep , keep-roll-roll , roll-keep-keep , roll-keep-roll , roll-roll-keep and roll-roll-roll The goal is to optimize the combination of dices before the end of the 2 turns. The best combination ever is 4-2-1 for 800 points. But you can explore other combinations. Initialize an Autonomous Player In your workspace, we encourage you to create a new directory for your experiences linked to our tutorials ( tutos for instance, aside from hackagames directory), and to create your new Py421 player in this directory. mkdir tutos touch tutos/myPy421Bot.py You now have to edit myPy421Bot.py script and create a Py421 player. Player Script Architecture The script must begin by importing hackagames elements ( hackapy ) and implement an hackagames Abstract Player . To import hackapy you have first to modify the python path resource to add your workspace directory (i.e. the directory including tutos in which your AI is positioned). # Local HackaGame: import sys sys.path.insert( 1, __file__.split('tutos')[0] ) import hackagames.hackapy as hg The first proposed script selects a random action and also requires the adequate python tool: import random Then your first player will inherit from hackay Abstract Player and implement the 4 player methods wakeUp , perceive , decide and sleep required to play any Hackagames's game : class AutonomousPlayer( hg.AbsPlayer ) : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): [...] def perceive(self, gameState): [...] def decide(self): [...] def sleep(self, result): [...] It is possible to add an extra \\(4\\) lines to permit myPy421Bot.py file to connect a player when it is executed. This script section is activated only if the file is the main Python entrance. When you interpret it python3 myPy421Bot.py rather than import it into another code. # script : if __name__ == '__main__' : player= AutonomousPlayer() results= player.takeASeat() print( f\"Results: {results}\" ) A Bot for Py421 Solo A first version of myPy421Bot.py player could be copied from the hackagames/gamePy421/playerFirstAI , and your final first script would be: # Local HackaGame: import sys sys.path.insert( 1, __file__.split('tutos')[0] ) import hackagames.hackapy as hg import random class AutonomousPlayer( hg.AbsPlayer ) : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): print( f'---\\nWake-up player-{playerId} ({numberOfPlayers} players)') print( gameConf ) def perceive(self, gameState): self.horizon= gameState.child(1).flag(1) self.dices= gameState.child(2).flags() print( f'H: {self.horizon} DICES: {self.dices}' ) def decide(self): actions= ['keep-keep-keep', 'keep-keep-roll', 'keep-roll-keep', 'keep-roll-roll', 'roll-keep-keep', 'roll-keep-roll', 'roll-roll-keep', 'roll-roll-roll' ] action= random.choice( actions ) print( f'Action: {action}' ) return action def sleep(self, result): print( f'--- Results: {str(result)}' ) # script : if __name__ == '__main__' : player= AutonomousPlayer() results= player.takeASeat() print( f\"Results: {results}\" ) Test your Player HackaGames is designed to work as a client-server architecture to make the game and the players completely independent. Each one in it own process and potentially on different machines. So the classical way to test your autonomous player is to start a game server then to launch your bot in separated shells. # In a first shell: python3 hackagames/gamePy421/start-server # In a second shell: python3 tutos/myPy421Bot.py Make Your Own Autonomous Player: To notice that method parameters are referencing hackapy objects (Pod) not presented in this tutorial. The perception already records the game variables horizon (the number of remaining roll-again) and dices (a list of the 3 dices values) into instance attributes. Actually in decide method, an action is chosen randomly, the goal now is to propose heuristic choice of actions to optimize average score over 10000 games. As a minimal start, it is possible to force a stop action (action keep-keep-keep ) each time the player reaches a good combination ( 4-2-1 or 1-1-1 for instance). The takeASeat method return the list game results. We can now print and analyze the reached results (compute the average score for instance): # Analysis average= sum(results)/len(results) print( f\"Average score: {average}\") A game server command takes a specific number of games to play with the option -n : # In a first shell: python3 hackagames/gamePy421/start-server -n 10000 As a minimal start, it is possible to force a stop action (action keep-keep-keep ) each time the player reaches a good combination ( 4-2-1 or 1-1-1 for instance). Then you can progressively try to optimize the decisions in order to target those 2 combinations.","title":"Py421, Solo Mode"},{"location":"games/py421-solo/#py421-solo-mode","text":"Py421 is an HackaGames game, a simple dice game where players try to optimize their dices combination after a maximum of 2 roll dice steps. This tutorial is based only on Python3 .","title":"Py421, Solo Mode"},{"location":"games/py421-solo/#try-the-game","text":"The play/py421 script starts the game with an interactive interface in a shell. python3 hackagames/play/py421 Py421 is a 3-dice game. The player can roll several times to get a combination. The player can perform one and only one action on his turn, and the game stops automatically after 2 turns. The actions consist in keeping or rolling each of the 3 dices. So there are 8 possible actions: keep-keep-keep , keep-keep-roll , keep-roll-keep , keep-roll-roll , roll-keep-keep , roll-keep-roll , roll-roll-keep and roll-roll-roll The goal is to optimize the combination of dices before the end of the 2 turns. The best combination ever is 4-2-1 for 800 points. But you can explore other combinations.","title":"Try the game:"},{"location":"games/py421-solo/#initialize-an-autonomous-player","text":"In your workspace, we encourage you to create a new directory for your experiences linked to our tutorials ( tutos for instance, aside from hackagames directory), and to create your new Py421 player in this directory. mkdir tutos touch tutos/myPy421Bot.py You now have to edit myPy421Bot.py script and create a Py421 player.","title":"Initialize an Autonomous Player"},{"location":"games/py421-solo/#player-script-architecture","text":"The script must begin by importing hackagames elements ( hackapy ) and implement an hackagames Abstract Player . To import hackapy you have first to modify the python path resource to add your workspace directory (i.e. the directory including tutos in which your AI is positioned). # Local HackaGame: import sys sys.path.insert( 1, __file__.split('tutos')[0] ) import hackagames.hackapy as hg The first proposed script selects a random action and also requires the adequate python tool: import random Then your first player will inherit from hackay Abstract Player and implement the 4 player methods wakeUp , perceive , decide and sleep required to play any Hackagames's game : class AutonomousPlayer( hg.AbsPlayer ) : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): [...] def perceive(self, gameState): [...] def decide(self): [...] def sleep(self, result): [...] It is possible to add an extra \\(4\\) lines to permit myPy421Bot.py file to connect a player when it is executed. This script section is activated only if the file is the main Python entrance. When you interpret it python3 myPy421Bot.py rather than import it into another code. # script : if __name__ == '__main__' : player= AutonomousPlayer() results= player.takeASeat() print( f\"Results: {results}\" )","title":"Player Script Architecture"},{"location":"games/py421-solo/#a-bot-for-py421-solo","text":"A first version of myPy421Bot.py player could be copied from the hackagames/gamePy421/playerFirstAI , and your final first script would be: # Local HackaGame: import sys sys.path.insert( 1, __file__.split('tutos')[0] ) import hackagames.hackapy as hg import random class AutonomousPlayer( hg.AbsPlayer ) : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): print( f'---\\nWake-up player-{playerId} ({numberOfPlayers} players)') print( gameConf ) def perceive(self, gameState): self.horizon= gameState.child(1).flag(1) self.dices= gameState.child(2).flags() print( f'H: {self.horizon} DICES: {self.dices}' ) def decide(self): actions= ['keep-keep-keep', 'keep-keep-roll', 'keep-roll-keep', 'keep-roll-roll', 'roll-keep-keep', 'roll-keep-roll', 'roll-roll-keep', 'roll-roll-roll' ] action= random.choice( actions ) print( f'Action: {action}' ) return action def sleep(self, result): print( f'--- Results: {str(result)}' ) # script : if __name__ == '__main__' : player= AutonomousPlayer() results= player.takeASeat() print( f\"Results: {results}\" )","title":"A Bot for Py421 Solo"},{"location":"games/py421-solo/#test-your-player","text":"HackaGames is designed to work as a client-server architecture to make the game and the players completely independent. Each one in it own process and potentially on different machines. So the classical way to test your autonomous player is to start a game server then to launch your bot in separated shells. # In a first shell: python3 hackagames/gamePy421/start-server # In a second shell: python3 tutos/myPy421Bot.py","title":"Test your Player"},{"location":"games/py421-solo/#make-your-own-autonomous-player","text":"To notice that method parameters are referencing hackapy objects (Pod) not presented in this tutorial. The perception already records the game variables horizon (the number of remaining roll-again) and dices (a list of the 3 dices values) into instance attributes. Actually in decide method, an action is chosen randomly, the goal now is to propose heuristic choice of actions to optimize average score over 10000 games. As a minimal start, it is possible to force a stop action (action keep-keep-keep ) each time the player reaches a good combination ( 4-2-1 or 1-1-1 for instance). The takeASeat method return the list game results. We can now print and analyze the reached results (compute the average score for instance): # Analysis average= sum(results)/len(results) print( f\"Average score: {average}\") A game server command takes a specific number of games to play with the option -n : # In a first shell: python3 hackagames/gamePy421/start-server -n 10000 As a minimal start, it is possible to force a stop action (action keep-keep-keep ) each time the player reaches a good combination ( 4-2-1 or 1-1-1 for instance). Then you can progressively try to optimize the decisions in order to target those 2 combinations.","title":"Make Your Own Autonomous Player:"},{"location":"games/py421/","text":"Py421 Py421 is an HackaGames game, a simple dice game where players try to optimize their dices combination after a maximum of 2 roll dice steps. This tutorial is based only on Python3 . Try the game: The play/py421 script starts the game with an interactive interface in a shell. python3 hackagames/play/py421 Py421 is a 3-dice game. The player can roll several times to get a combination. The player can perform one and only one action on his turn, and the game stops automatically after 2 turns. The actions consist in keeping or rolling each of the 3 dices. So there are 8 possible actions: keep-keep-keep , keep-keep-roll , keep-roll-keep , keep-roll-roll , roll-keep-keep , roll-keep-roll , roll-roll-keep and roll-roll-roll The goal is to optimize the combination of dices before the end of the 2 turns. The best combination ever is 4-2-1 for 800 points. But you can explore other combinations. Initialize a Bot If you are implementing your first bot, please follow the first bot tutorial. As for any Hackagames bots, a Py421 bot implements the the 4 player/bot methods wakeUp , perceive , decide and sleep . Theire is no information to get from The gameConf . The gameState parameter of the perceive includes 2 elements (2 gameState childs). First child model the roll-again horizon (one integer value), the second child the dices (3 interger value and a floating point value for the score of the combinaison). import random class Bot : def actions(self): return [ 'keep-keep-keep', 'keep-keep-roll', 'keep-roll-keep', 'keep-roll-roll', 'roll-keep-keep', 'roll-keep-roll', 'roll-roll-keep', 'roll-roll-roll' ] # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): self._horizon= 3 self._dices= [0, 0, 0] self._score= 0.0 def perceive(self, gameState): self._horizon= gameState.child(1).flag(1) self._dices= gameState.child(2).flags() self._score= gameState.child(2).value(1) def decide(self): return random.choice( self.actions() ) def sleep(self, result): print( \"> result: \"+ str(result) )","title":"Py4.2.1"},{"location":"games/py421/#py421","text":"Py421 is an HackaGames game, a simple dice game where players try to optimize their dices combination after a maximum of 2 roll dice steps. This tutorial is based only on Python3 .","title":"Py421"},{"location":"games/py421/#try-the-game","text":"The play/py421 script starts the game with an interactive interface in a shell. python3 hackagames/play/py421 Py421 is a 3-dice game. The player can roll several times to get a combination. The player can perform one and only one action on his turn, and the game stops automatically after 2 turns. The actions consist in keeping or rolling each of the 3 dices. So there are 8 possible actions: keep-keep-keep , keep-keep-roll , keep-roll-keep , keep-roll-roll , roll-keep-keep , roll-keep-roll , roll-roll-keep and roll-roll-roll The goal is to optimize the combination of dices before the end of the 2 turns. The best combination ever is 4-2-1 for 800 points. But you can explore other combinations.","title":"Try the game:"},{"location":"games/py421/#initialize-a-bot","text":"If you are implementing your first bot, please follow the first bot tutorial. As for any Hackagames bots, a Py421 bot implements the the 4 player/bot methods wakeUp , perceive , decide and sleep . Theire is no information to get from The gameConf . The gameState parameter of the perceive includes 2 elements (2 gameState childs). First child model the roll-again horizon (one integer value), the second child the dices (3 interger value and a floating point value for the score of the combinaison). import random class Bot : def actions(self): return [ 'keep-keep-keep', 'keep-keep-roll', 'keep-roll-keep', 'keep-roll-roll', 'roll-keep-keep', 'roll-keep-roll', 'roll-roll-keep', 'roll-roll-roll' ] # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): self._horizon= 3 self._dices= [0, 0, 0] self._score= 0.0 def perceive(self, gameState): self._horizon= gameState.child(1).flag(1) self._dices= gameState.child(2).flags() self._score= gameState.child(2).value(1) def decide(self): return random.choice( self.actions() ) def sleep(self, result): print( \"> result: \"+ str(result) )","title":"Initialize a Bot"},{"location":"games/risky/","text":"Risky Risky is an HackaGames game. It is a strategic turn-based game where two armies (or more) fights for a territory. Try the game: The start-interactive script starts the game with an interactive interface in a shell playing against an artificial player playerFirstAI.py . python3 hackagames/gameRisky/start-interactive The world is composed by interconnected nodes forming a tabletop as, for instance: .' '. | | '. .3 / \\ .' '. .' '. | |-----| | '. .1 '. .2 \\ / .' '. | | '. .4 The 2 players are referenced as a player A and player B ; starting respectively in positions 1 and 2. When an army is on a node, the information is presented as below: .'A'. # Player ID |1- 12| # army action and force '. .4 # node ID In this example, an army of player A is on node 4 . The army has 1 action-point and is composed by 12 soldiers. Each army has 2 main attributes: its action counter (the number of action it can perform - max 2) its force (the size of the army - max 24) At its turn the player can make several actions (in the limit of action counters): Moving: move X Y FORCE to move FORCE units from cell X to cell Y Growing: grow X to grow the army on nodes X . The increase of the army is depending on the initial army size and the connections to occupied friend nodes. Sleeping: sleep to increase the action counter by one for all the armies. To notice that a moving action that will move an army toward an adversarial node trigger a fight. Iteratively, each force point of the attack and the defense has a chance to deal one damage. Defenses have an increased chance than attack. However if the attack is greater than the defense than each extra point count double. The fight is running until one of the army is destroyed. For instance, with a move 1 2 10 with a defense of 8 on the node 2 , the fight will start by considering an attack force of 12 ( \\(2\\times 10-8\\) ) times 1 chance over 2 against a defense of 8 times 2 chances over 3. The exact amount of damages at the end of the fight remains uncertain. Initialize an Autonomous Player Into your workspace, we encourage you to create a new directory for your experiences linked to our tutorials ( tutos for instance, aside of hackagames directory), and to create your new Risky player in this directory. mkdir tutos touch tutos/myRiskyPlayer.py You now have to edit myRiskyPlayer.py script and create a Risky player. The script must begin by importing hackagames elements ( hackapy and the Risky gameEngine ) and implement an hackagames Abstract Player . To import hackapy and the gameEngine you have first to modify the python path resource to add your workspace directory (i.e. the directory including tutos in which your AI in positioned). # Local HackaGame: import sys sys.path.insert( 1, __file__.split('tutos')[0] ) import hackagames.hackapy as hg import hackagames.gameRisky.gameEngine as game The first script we propose selects a random action and also requires the adequate python tool: import random Then your first player will inherit from hackay Abstract Player and implement the 4 player methods wakeUp , perceive , decide and sleep required to play any Hackagames's game : class AutonomousPlayer( hg.AbsPlayer ) : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): [...] def perceive(self, gameState): [...] def decide(self): [...] def sleep(self, result): [...] A first version of myRiskyPlayer.py player could be a copy-paste of the hackagames/gameRisky/playerFirstAI , and your final first script would be: # Local HackaGame: import sys sys.path.insert( 1, __file__.split('tutos')[0] ) import hackagames.hackapy as hg import hackagames.gameRisky.gameEngine as game import random class AutonomousPlayer(hg.AbsPlayer) : # Player interface : def wakeUp(self, iPlayer, numberOfPlayers, gameConf): print( f'---\\nwake-up player-{iPlayer} ({numberOfPlayers} players)') self.playerId= chr( ord(\"A\")+iPlayer-1 ) self.game= game.GameRisky().fromPod( gameConf ) self.viewer= game.ViewerTerminal( self.game ) def perceive(self, gameState): self.game.fromPod( gameState ) self.viewer.print( self.playerId ) def decide(self): actions= self.game.searchActions( self.playerId ) action= random.choice( actions ) if action[0] == 'move': action[3]= random.randint(1, action[3]) action= ' '.join( [ str(x) for x in action ] ) print( \"Do: \"+ action ) return action def sleep(self, result): print( f'---\\ngame end\\nresult: {result}') Here the gameEngine permit the player to instantiate a copy self.game of the game at the reached configuration. It is first used to search for available actions in decide method and to get one at random. Test your Player HackaGames is designed to work as a client-server architecture to make the game and the player completely independent. However, it is also possible start a python game in a test mode for a player in a single process with a simple script. Aside to hackagames and your tutos directories create your own launcher: touch launcherRisky.py Edit the python script. The code requires to import the game and the player and an opponent to your player. then to instantiate them and to start testPlayer method: #!env python3 from hackagames.gameRisky.gameEngine import GameRisky from hackagames.gameRisky.playerFirstAI import AutonomousPlayer as Opponent from tutos.myRiskyPlayer import AutonomousPlayer as Player # Instanciate and start 1 games game= GameRisky( 2, \"board-4\" ) player= Player() opponent1= Opponent() results= game.testPlayer( player, 100, [opponent1] ) print(results) That it, you can execute your script: python3 ./launcherRisky.py which calls your player. The second attribute in testPlayer method of game instance ( 100 here) is the number of games the players will play before the process end. The game.testPlayer method return the list game results. We can now print and analyze the reached results (compute the average score for instance): # Analysis average= sum(results)/len(results) print( f\"Average score: {average}\") The result should be very close to zero. It is the same AI... Customaize your AI: To customize your AI you can use the game engine copy (cf. risky.py ) Somme of the available methods: def update( self, board ): # Update the board from the perception. def searchActions(self, playerId): # List all the current possible actions from the configuration of the armies def cellIds(self): # return the list of cell identifiers def edgesFrom(self, iCell): # return the list of connected cell identifiers from the iCell cell. def armyOn(self, iCell) : # return an army as a Pod object, if an army is on the iCell cell (and False otherwise). def playerLetter(self, iPlayer): # return the player letter (A, B, C ...) of the ith player (1, 2, ...) An army is a Pod object where the owner is recorded in the status and the 2 attributes is for action counter and force : for iCell in self.game.cellIds() : army= self.game.armyOn(iCell) # The army on the cell 1 if army : owner= army.status() action= army.flag(1) force= army.flag(2) print( f\"Army-{owner} ({action}, {force}) on {iCells}\" ) The goal now is to compute that information in order to propose an AI winning the PlayerFirstAI. Confront your AI: In the launcher script, you can replace the imported opponent AI to an interactive interface. from hackagames.gameRisky.gameEngine.players import PlayerShell as Opponent","title":"Risky"},{"location":"games/risky/#risky","text":"Risky is an HackaGames game. It is a strategic turn-based game where two armies (or more) fights for a territory.","title":"Risky"},{"location":"games/risky/#try-the-game","text":"The start-interactive script starts the game with an interactive interface in a shell playing against an artificial player playerFirstAI.py . python3 hackagames/gameRisky/start-interactive The world is composed by interconnected nodes forming a tabletop as, for instance: .' '. | | '. .3 / \\ .' '. .' '. | |-----| | '. .1 '. .2 \\ / .' '. | | '. .4 The 2 players are referenced as a player A and player B ; starting respectively in positions 1 and 2. When an army is on a node, the information is presented as below: .'A'. # Player ID |1- 12| # army action and force '. .4 # node ID In this example, an army of player A is on node 4 . The army has 1 action-point and is composed by 12 soldiers. Each army has 2 main attributes: its action counter (the number of action it can perform - max 2) its force (the size of the army - max 24) At its turn the player can make several actions (in the limit of action counters): Moving: move X Y FORCE to move FORCE units from cell X to cell Y Growing: grow X to grow the army on nodes X . The increase of the army is depending on the initial army size and the connections to occupied friend nodes. Sleeping: sleep to increase the action counter by one for all the armies. To notice that a moving action that will move an army toward an adversarial node trigger a fight. Iteratively, each force point of the attack and the defense has a chance to deal one damage. Defenses have an increased chance than attack. However if the attack is greater than the defense than each extra point count double. The fight is running until one of the army is destroyed. For instance, with a move 1 2 10 with a defense of 8 on the node 2 , the fight will start by considering an attack force of 12 ( \\(2\\times 10-8\\) ) times 1 chance over 2 against a defense of 8 times 2 chances over 3. The exact amount of damages at the end of the fight remains uncertain.","title":"Try the game:"},{"location":"games/risky/#initialize-an-autonomous-player","text":"Into your workspace, we encourage you to create a new directory for your experiences linked to our tutorials ( tutos for instance, aside of hackagames directory), and to create your new Risky player in this directory. mkdir tutos touch tutos/myRiskyPlayer.py You now have to edit myRiskyPlayer.py script and create a Risky player. The script must begin by importing hackagames elements ( hackapy and the Risky gameEngine ) and implement an hackagames Abstract Player . To import hackapy and the gameEngine you have first to modify the python path resource to add your workspace directory (i.e. the directory including tutos in which your AI in positioned). # Local HackaGame: import sys sys.path.insert( 1, __file__.split('tutos')[0] ) import hackagames.hackapy as hg import hackagames.gameRisky.gameEngine as game The first script we propose selects a random action and also requires the adequate python tool: import random Then your first player will inherit from hackay Abstract Player and implement the 4 player methods wakeUp , perceive , decide and sleep required to play any Hackagames's game : class AutonomousPlayer( hg.AbsPlayer ) : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): [...] def perceive(self, gameState): [...] def decide(self): [...] def sleep(self, result): [...] A first version of myRiskyPlayer.py player could be a copy-paste of the hackagames/gameRisky/playerFirstAI , and your final first script would be: # Local HackaGame: import sys sys.path.insert( 1, __file__.split('tutos')[0] ) import hackagames.hackapy as hg import hackagames.gameRisky.gameEngine as game import random class AutonomousPlayer(hg.AbsPlayer) : # Player interface : def wakeUp(self, iPlayer, numberOfPlayers, gameConf): print( f'---\\nwake-up player-{iPlayer} ({numberOfPlayers} players)') self.playerId= chr( ord(\"A\")+iPlayer-1 ) self.game= game.GameRisky().fromPod( gameConf ) self.viewer= game.ViewerTerminal( self.game ) def perceive(self, gameState): self.game.fromPod( gameState ) self.viewer.print( self.playerId ) def decide(self): actions= self.game.searchActions( self.playerId ) action= random.choice( actions ) if action[0] == 'move': action[3]= random.randint(1, action[3]) action= ' '.join( [ str(x) for x in action ] ) print( \"Do: \"+ action ) return action def sleep(self, result): print( f'---\\ngame end\\nresult: {result}') Here the gameEngine permit the player to instantiate a copy self.game of the game at the reached configuration. It is first used to search for available actions in decide method and to get one at random.","title":"Initialize an Autonomous Player"},{"location":"games/risky/#test-your-player","text":"HackaGames is designed to work as a client-server architecture to make the game and the player completely independent. However, it is also possible start a python game in a test mode for a player in a single process with a simple script. Aside to hackagames and your tutos directories create your own launcher: touch launcherRisky.py Edit the python script. The code requires to import the game and the player and an opponent to your player. then to instantiate them and to start testPlayer method: #!env python3 from hackagames.gameRisky.gameEngine import GameRisky from hackagames.gameRisky.playerFirstAI import AutonomousPlayer as Opponent from tutos.myRiskyPlayer import AutonomousPlayer as Player # Instanciate and start 1 games game= GameRisky( 2, \"board-4\" ) player= Player() opponent1= Opponent() results= game.testPlayer( player, 100, [opponent1] ) print(results) That it, you can execute your script: python3 ./launcherRisky.py which calls your player. The second attribute in testPlayer method of game instance ( 100 here) is the number of games the players will play before the process end. The game.testPlayer method return the list game results. We can now print and analyze the reached results (compute the average score for instance): # Analysis average= sum(results)/len(results) print( f\"Average score: {average}\") The result should be very close to zero. It is the same AI...","title":"Test your Player"},{"location":"games/risky/#customaize-your-ai","text":"To customize your AI you can use the game engine copy (cf. risky.py ) Somme of the available methods: def update( self, board ): # Update the board from the perception. def searchActions(self, playerId): # List all the current possible actions from the configuration of the armies def cellIds(self): # return the list of cell identifiers def edgesFrom(self, iCell): # return the list of connected cell identifiers from the iCell cell. def armyOn(self, iCell) : # return an army as a Pod object, if an army is on the iCell cell (and False otherwise). def playerLetter(self, iPlayer): # return the player letter (A, B, C ...) of the ith player (1, 2, ...) An army is a Pod object where the owner is recorded in the status and the 2 attributes is for action counter and force : for iCell in self.game.cellIds() : army= self.game.armyOn(iCell) # The army on the cell 1 if army : owner= army.status() action= army.flag(1) force= army.flag(2) print( f\"Army-{owner} ({action}, {force}) on {iCells}\" ) The goal now is to compute that information in order to propose an AI winning the PlayerFirstAI.","title":"Customaize your AI:"},{"location":"games/risky/#confront-your-ai","text":"In the launcher script, you can replace the imported opponent AI to an interactive interface. from hackagames.gameRisky.gameEngine.players import PlayerShell as Opponent","title":"Confront your AI:"},{"location":"games/tictactoe/","text":"TicTacToe TicTacToe is an HackaGames game. TicTacToe is simple two-player games where each player tries to align trees of their pieces. It comes with two modes: classic and ultimate . Try the game: The start-interactive script starts the game in classic mode with an interactive interface in a shell playing against an artificial player playerFirstAI.py ( ./hackagames/gameTictactoe/start-interactive ). The player can perform one and only one action at its turn, and the game stops automatically with a winner or when no more pieces can be set on the tabletop. The actions consist in positionning a player's piece on the grid with the form: coordinateLetter-coordinateNumber . Their are 3 times 3 actions in classic mode: A-1 , A-2 , A-3 , B-1 , B-2 , B-3 , C-1 , C-2 and C-3 . Exemple of grid at some point: x: A B C 1 x 2 o 3 o x The first player aligning 3 of its pieces win the game. Ultimate TicTacToe The ultimate mode is a hierarchical 2-levels TicTacToe . It is possible to activate the mode with a command parrameter: start-interactive ultimate the grid is composed off 9 times 9 cells, so potentially 9 times 9 actions for the players: A-1 , A-2 , ... , A-9 , B-1 , ... , `I-9. In practice, most of the time, only a sub-number of actions are available. The particularity in hierarchical TicTacToe is that, the took position by a player in the classic grid indicates the next grid to play for the next player turn. Exemple of grid at some point: x: A B C D E F G H I 1 x | | 2 | | 3 o | | -------|-------|------- 4 | | 5 | | 6 | o | -------|-------|------- 7 | | 8 | x | 9 | | actions: D:F-7:9 The line actions: D:F-7:9 indicate that it is possible to play in any free possition between D and F and 7 and 9 , so : D-7 , D-8 , D-9 , E-7 , E-9 , F-7 , F-8 or F-9 . At the begining of the game it is possible to play in a corner grid, a side grid or the center grid. That for the action line indicate: actions: A:C-1:3, A:C-4:6, D:F-4:6 The players have to win 3 aligned classic grids to win an Ultimate TicTacToe . Initialize an Autonomous Player Customaize your AI:","title":"TicTacToe"},{"location":"games/tictactoe/#tictactoe","text":"TicTacToe is an HackaGames game. TicTacToe is simple two-player games where each player tries to align trees of their pieces. It comes with two modes: classic and ultimate .","title":"TicTacToe"},{"location":"games/tictactoe/#try-the-game","text":"The start-interactive script starts the game in classic mode with an interactive interface in a shell playing against an artificial player playerFirstAI.py ( ./hackagames/gameTictactoe/start-interactive ). The player can perform one and only one action at its turn, and the game stops automatically with a winner or when no more pieces can be set on the tabletop. The actions consist in positionning a player's piece on the grid with the form: coordinateLetter-coordinateNumber . Their are 3 times 3 actions in classic mode: A-1 , A-2 , A-3 , B-1 , B-2 , B-3 , C-1 , C-2 and C-3 . Exemple of grid at some point: x: A B C 1 x 2 o 3 o x The first player aligning 3 of its pieces win the game.","title":"Try the game:"},{"location":"games/tictactoe/#ultimate-tictactoe","text":"The ultimate mode is a hierarchical 2-levels TicTacToe . It is possible to activate the mode with a command parrameter: start-interactive ultimate the grid is composed off 9 times 9 cells, so potentially 9 times 9 actions for the players: A-1 , A-2 , ... , A-9 , B-1 , ... , `I-9. In practice, most of the time, only a sub-number of actions are available. The particularity in hierarchical TicTacToe is that, the took position by a player in the classic grid indicates the next grid to play for the next player turn. Exemple of grid at some point: x: A B C D E F G H I 1 x | | 2 | | 3 o | | -------|-------|------- 4 | | 5 | | 6 | o | -------|-------|------- 7 | | 8 | x | 9 | | actions: D:F-7:9 The line actions: D:F-7:9 indicate that it is possible to play in any free possition between D and F and 7 and 9 , so : D-7 , D-8 , D-9 , E-7 , E-9 , F-7 , F-8 or F-9 . At the begining of the game it is possible to play in a corner grid, a side grid or the center grid. That for the action line indicate: actions: A:C-1:3, A:C-4:6, D:F-4:6 The players have to win 3 aligned classic grids to win an Ultimate TicTacToe .","title":"Ultimate TicTacToe"},{"location":"games/tictactoe/#initialize-an-autonomous-player","text":"","title":"Initialize an Autonomous Player"},{"location":"games/tictactoe/#customaize-your-ai","text":"","title":"Customaize your AI:"},{"location":"hello/03-pod/","text":"Piece-Of-Data - the core Class In the interoperable process between a game and players, information transits. Games send what players can perceive and players send action description in return. That information is structured as Pod (Piece-Of-Data) to be serialized, transmitted and rebuilt. a Pod includes raw data (sequence of characters, intergers, floating points values) and cand be a part of a tree structure of information. Implementation: hackapy: pod.py The example here are presented accordingly to the Python implementation (hackapy). Pod Components Pod is composed of 5 elements of fixed type: family: an string permitting to identifiate the type Pod . It is generally used as a key to make the other elements interpretable. status: an string describing the individual in the family, its current state. flags: a vector of integer values. values: a vector of floating points values. children: a vector of other Pod elements, sub-part of the Pod. Accessor methods permit to get string elements as list : aPod.family() , aPod.status() and list elements: aPod.flags() , aPod.values() , aPod.children() . It is also possible to get a specific element is the lists with: aPod.flag(anInteger) , aPod.value(anInteger) , aPod.child(anInteger) . To notice that, in HackaGames conventions element in list are indexed starting from 1 . The first element is accessed with index 1 : ( aPod.flags(1) for instance). Tree Structure Children componnents of a Pod permits to defines complexe information structured as trees. For more information about tree data structure, Wikipedia is a good entrance point. Trees are very famous in Web technoligies for instance through HTML , XML , Json , Yaml etc formats. In HackaGames most of the game states perceived by players (one Pod ) are composed by several elements each one modeled by child Pod. For instance, a Board would be composed with Cells and Cells would welcome Pieces . The pieces would be the childrens of a Cell , Cells the children of a Board etc. Interface versus Inheritance HackaGames conventions prefer to separate game elements in the implementations from Pod. A Pod imposes specific structuration of elements (flags, values, ...). This structuration is not always efficient in the game mechanisms and not even readable in its implementations. That for, we discourage developers from using inheritance of Pod in game and player implementations and we propose interface mechanisms to generate pods from the game elements and vice versa, when it is pertinent to do it. Game elements should include Pod interface methods: # Pod interface: def asPod(self, family=\"Pod\"): # Should return a Pod describing self. # ... pass def fromPod(self, aPod): # Should regenerate self form a pod description # HackaGames conventions aim to return self at the end of the method. # ... pass","title":"Piece-Of-Data - the core Class"},{"location":"hello/03-pod/#piece-of-data-the-core-class","text":"In the interoperable process between a game and players, information transits. Games send what players can perceive and players send action description in return. That information is structured as Pod (Piece-Of-Data) to be serialized, transmitted and rebuilt. a Pod includes raw data (sequence of characters, intergers, floating points values) and cand be a part of a tree structure of information. Implementation: hackapy: pod.py The example here are presented accordingly to the Python implementation (hackapy).","title":"Piece-Of-Data - the core Class"},{"location":"hello/03-pod/#pod-components","text":"Pod is composed of 5 elements of fixed type: family: an string permitting to identifiate the type Pod . It is generally used as a key to make the other elements interpretable. status: an string describing the individual in the family, its current state. flags: a vector of integer values. values: a vector of floating points values. children: a vector of other Pod elements, sub-part of the Pod. Accessor methods permit to get string elements as list : aPod.family() , aPod.status() and list elements: aPod.flags() , aPod.values() , aPod.children() . It is also possible to get a specific element is the lists with: aPod.flag(anInteger) , aPod.value(anInteger) , aPod.child(anInteger) . To notice that, in HackaGames conventions element in list are indexed starting from 1 . The first element is accessed with index 1 : ( aPod.flags(1) for instance).","title":"Pod Components"},{"location":"hello/03-pod/#tree-structure","text":"Children componnents of a Pod permits to defines complexe information structured as trees. For more information about tree data structure, Wikipedia is a good entrance point. Trees are very famous in Web technoligies for instance through HTML , XML , Json , Yaml etc formats. In HackaGames most of the game states perceived by players (one Pod ) are composed by several elements each one modeled by child Pod. For instance, a Board would be composed with Cells and Cells would welcome Pieces . The pieces would be the childrens of a Cell , Cells the children of a Board etc.","title":"Tree Structure"},{"location":"hello/03-pod/#interface-versus-inheritance","text":"HackaGames conventions prefer to separate game elements in the implementations from Pod. A Pod imposes specific structuration of elements (flags, values, ...). This structuration is not always efficient in the game mechanisms and not even readable in its implementations. That for, we discourage developers from using inheritance of Pod in game and player implementations and we propose interface mechanisms to generate pods from the game elements and vice versa, when it is pertinent to do it. Game elements should include Pod interface methods: # Pod interface: def asPod(self, family=\"Pod\"): # Should return a Pod describing self. # ... pass def fromPod(self, aPod): # Should regenerate self form a pod description # HackaGames conventions aim to return self at the end of the method. # ... pass","title":"Interface versus Inheritance"},{"location":"hello/04-protocol/","text":"","title":"04 protocol"},{"location":"hello/first-bot/","text":"First Bot In this tutorial, we will learn about how the games and the players/bots communicate together. In Hackagames several players can play simultaneously to games. That for, the loop control of the program is handled by the game. Then, the player bots have to generate the appropriate responses (string formed actions) to situations. To do so, a player bot can re-implement the 4 expected methods: wakeUp , perceive , decide and sleep . Initialize a Bot environment First let implement a simple script, instantiating a game and a player. Put the next code into a script file launch-py421.py . # Setup a game: from hacka.games.py421 import GameSolo as Game # Setup a bot: from hacka.games.py421.firstBot import Bot bot= Bot() # Instanciate and start 100 games game= Game() results= game.testPlayer( bot, 1000 ) # Analyze the result print( f\"Average score: {sum(results)/len(results)}\" ) This script import py421.py game and a bot, then it starts \\(1000\\) games. Test it in a terminal: python3 launch-py421.py Initialize a Bot, i.e. an Autonomous Player For playing py421.py , our script must implement an hackagames formated Bot . A first simple solution consists in modifying the one proposed by the Py421 HackaGames . Py421 FirstBot . Py421 FirstBot plays randomly, an interesting update should be to score each time a \\(4 2 1\\) is reached : import random import hacka.games.py421.firstBot as firstBot # Setup a bot: class MyBot( firstBot.Bot ) : def decide( self ): if self.dices() == [4, 2, 1] : return \"keep-keep-keep\" return random.choice( self.actions() ) Replace the # Setup a player: section into your launch-py421.py script and try your bot. The average score should be increased. Understand Bot methods : Games and Bots rely on hacka.pylib library, to exchange information about the game and the player intentions. Game Loop: A game mainly relies on perceive and decide capability for players and bots to update the information about the game and to ask for player action. The perceive method gives \\(1\\) argument: a gameState , the current game state from the point of view of the player. In fact gameState can be different from a player to another in multiplayer games with partial observation (typically: card games). In the basic configuration of Py421 , there is only one player and the gameConf informs about the 3 dice values and the horizon counter (the number of remaining roll again possibilities). The decide method does not have an argument, but should return the player action. decide is always triggered after one or several perceive . The action is formatted as a string. Game Server : Furthermore, A Game server can start several games sequentially. That for, the method wakeUp and sleep are triggered respectively when a new game is started and stopped. There are triggered 1000 times in our script. During a Game's game, players are activated at turn with perceive and decide method The wakeUp has \\(3\\) parameters: playerId , numberOfPlayers , gameConf informing in the place of the player in the game, the number of players and the initial configuration. At this stage, there is no information to get from gameConf in Py421 . Finally, the sleep inform about the obtained score with it parameters result . The score is computed for the player regarding the ending game. Example of first py421 Bot : As an example, this next Bot is a verbose version for \"Py421\" playing randomly: import random class Bot : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): print( f'---\\nWake-up player-{playerId} ({numberOfPlayers} players)') print( \"conf: \" + str(gameConf) ) def perceive(self, gameState): self._horizon= gameState.child(1).flag(1) self._dices= gameState.child(2).flags() self._score= gameState.child(2).value(1) print( f'\\t> H: {self._horizon}, DICES: {self._dices}, SCORE: {self._score}' ) def decide(self): actions= ['keep-keep-keep', 'keep-keep-roll', 'keep-roll-keep', 'keep-roll-roll', 'roll-keep-keep', 'roll-keep-roll', 'roll-roll-keep', 'roll-roll-roll' ] action= random.choice( actions ) print( f'\\t< Action: {action}' ) return action def sleep(self, result): print( f'--- Results: {str(result)}' ) To notice that the parameters gameConf and gameState are formated as hackapy.Pod (Piece Of Data), a tree data structure. It is not required to understand Pod . Examples are always provided to get the game information from Pod . However, you can go on Under the hood - Pod page for more detail on this core element of HackaGames .","title":"First Bot"},{"location":"hello/first-bot/#first-bot","text":"In this tutorial, we will learn about how the games and the players/bots communicate together. In Hackagames several players can play simultaneously to games. That for, the loop control of the program is handled by the game. Then, the player bots have to generate the appropriate responses (string formed actions) to situations. To do so, a player bot can re-implement the 4 expected methods: wakeUp , perceive , decide and sleep .","title":"First Bot"},{"location":"hello/first-bot/#initialize-a-bot-environment","text":"First let implement a simple script, instantiating a game and a player. Put the next code into a script file launch-py421.py . # Setup a game: from hacka.games.py421 import GameSolo as Game # Setup a bot: from hacka.games.py421.firstBot import Bot bot= Bot() # Instanciate and start 100 games game= Game() results= game.testPlayer( bot, 1000 ) # Analyze the result print( f\"Average score: {sum(results)/len(results)}\" ) This script import py421.py game and a bot, then it starts \\(1000\\) games. Test it in a terminal: python3 launch-py421.py","title":"Initialize a Bot environment"},{"location":"hello/first-bot/#initialize-a-bot-ie-an-autonomous-player","text":"For playing py421.py , our script must implement an hackagames formated Bot . A first simple solution consists in modifying the one proposed by the Py421 HackaGames . Py421 FirstBot . Py421 FirstBot plays randomly, an interesting update should be to score each time a \\(4 2 1\\) is reached : import random import hacka.games.py421.firstBot as firstBot # Setup a bot: class MyBot( firstBot.Bot ) : def decide( self ): if self.dices() == [4, 2, 1] : return \"keep-keep-keep\" return random.choice( self.actions() ) Replace the # Setup a player: section into your launch-py421.py script and try your bot. The average score should be increased.","title":"Initialize a Bot, i.e. an Autonomous Player"},{"location":"hello/first-bot/#understand-bot-methods","text":"Games and Bots rely on hacka.pylib library, to exchange information about the game and the player intentions.","title":"Understand Bot methods :"},{"location":"hello/first-bot/#game-loop","text":"A game mainly relies on perceive and decide capability for players and bots to update the information about the game and to ask for player action. The perceive method gives \\(1\\) argument: a gameState , the current game state from the point of view of the player. In fact gameState can be different from a player to another in multiplayer games with partial observation (typically: card games). In the basic configuration of Py421 , there is only one player and the gameConf informs about the 3 dice values and the horizon counter (the number of remaining roll again possibilities). The decide method does not have an argument, but should return the player action. decide is always triggered after one or several perceive . The action is formatted as a string.","title":"Game Loop:"},{"location":"hello/first-bot/#game-server","text":"Furthermore, A Game server can start several games sequentially. That for, the method wakeUp and sleep are triggered respectively when a new game is started and stopped. There are triggered 1000 times in our script. During a Game's game, players are activated at turn with perceive and decide method The wakeUp has \\(3\\) parameters: playerId , numberOfPlayers , gameConf informing in the place of the player in the game, the number of players and the initial configuration. At this stage, there is no information to get from gameConf in Py421 . Finally, the sleep inform about the obtained score with it parameters result . The score is computed for the player regarding the ending game.","title":"Game Server :"},{"location":"hello/first-bot/#example-of-first-py421-bot","text":"As an example, this next Bot is a verbose version for \"Py421\" playing randomly: import random class Bot : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): print( f'---\\nWake-up player-{playerId} ({numberOfPlayers} players)') print( \"conf: \" + str(gameConf) ) def perceive(self, gameState): self._horizon= gameState.child(1).flag(1) self._dices= gameState.child(2).flags() self._score= gameState.child(2).value(1) print( f'\\t> H: {self._horizon}, DICES: {self._dices}, SCORE: {self._score}' ) def decide(self): actions= ['keep-keep-keep', 'keep-keep-roll', 'keep-roll-keep', 'keep-roll-roll', 'roll-keep-keep', 'roll-keep-roll', 'roll-roll-keep', 'roll-roll-roll' ] action= random.choice( actions ) print( f'\\t< Action: {action}' ) return action def sleep(self, result): print( f'--- Results: {str(result)}' ) To notice that the parameters gameConf and gameState are formated as hackapy.Pod (Piece Of Data), a tree data structure. It is not required to understand Pod . Examples are always provided to get the game information from Pod . However, you can go on Under the hood - Pod page for more detail on this core element of HackaGames .","title":"Example of first py421 Bot :"},{"location":"hello/install/","text":"Install Process A quick tutorial to make HackaGames running on your computer. Nota bene : HackaGames is natively developed on Linux systems. Documentation is built regarding Ubuntu-like distributions. Commands are given in bash syntaxe. On Windows: You can use git for windows and its git bash . Install Simplely clone the git repository and use python pip . git clone https://github.com/ktorz-net/hackagames pip install ./hackagames or python -m pip install ./hackagames Thats it. You can play to several included games (the ones developed on top of hackapy ), and implement some Bots (cf. First Bot section). Dependencies The project is distributed using git solution and relies on Python3 language. Make sure you have them installed. HackaGames includes python3 librairy: hakapy for making game-servers and player-clients. The network protocol of HackaGames relies on zmq library. Process-bars are implemented via tqdm . Getting Started The easiest way is to play with one of the proposed python3 games, Py421 for instance. Each python3 game comes with play script permitting to start the game with an interactive interface in a shell. python3 hackagames/play/py421 The Py421 is a tree dice game. The player can roll several times to get the best combination. The actions consist in keeping or rolling each of the 3 dices. So there are 8 possible actions: keep-keep-keep , keep-keep-roll , keep-roll-keep , keep-roll-roll , roll-keep-keep , roll-keep-roll , roll-roll-keep and roll-roll-roll The goal is to optimize the combination of dices before the end of the second turn. The best combination ever is 4-2-1 . But you can explore other combinations. You can then follow the tutorial of First Bot to learn how to implement an autonomous player for Py421 the game.","title":"Install"},{"location":"hello/install/#install-process","text":"A quick tutorial to make HackaGames running on your computer. Nota bene : HackaGames is natively developed on Linux systems. Documentation is built regarding Ubuntu-like distributions. Commands are given in bash syntaxe. On Windows: You can use git for windows and its git bash .","title":"Install Process"},{"location":"hello/install/#install","text":"Simplely clone the git repository and use python pip . git clone https://github.com/ktorz-net/hackagames pip install ./hackagames or python -m pip install ./hackagames Thats it. You can play to several included games (the ones developed on top of hackapy ), and implement some Bots (cf. First Bot section).","title":"Install"},{"location":"hello/install/#dependencies","text":"The project is distributed using git solution and relies on Python3 language. Make sure you have them installed. HackaGames includes python3 librairy: hakapy for making game-servers and player-clients. The network protocol of HackaGames relies on zmq library. Process-bars are implemented via tqdm .","title":"Dependencies"},{"location":"hello/install/#getting-started","text":"The easiest way is to play with one of the proposed python3 games, Py421 for instance. Each python3 game comes with play script permitting to start the game with an interactive interface in a shell. python3 hackagames/play/py421 The Py421 is a tree dice game. The player can roll several times to get the best combination. The actions consist in keeping or rolling each of the 3 dices. So there are 8 possible actions: keep-keep-keep , keep-keep-roll , keep-roll-keep , keep-roll-roll , roll-keep-keep , roll-keep-roll , roll-roll-keep and roll-roll-roll The goal is to optimize the combination of dices before the end of the second turn. The best combination ever is 4-2-1 . But you can explore other combinations. You can then follow the tutorial of First Bot to learn how to implement an autonomous player for Py421 the game.","title":"Getting Started"},{"location":"hello/multi-player/","text":"","title":"Multi-Player"},{"location":"hood/api/","text":"Communication API The idea is to use a client/server architecture to permit any code in any language to seat on a game (i.e., connect to act as a player). For that we choose to use a high-level messaging library. High-Level Message Queuing API We do not require to address directly the TCP protocol while our need for network usage is very classical. A game process acts as a server, accepts and manages client-player processes. Then the game and players exchange game states and action description. For that purpose tree solution was considered: MQTT - Mosquitto , ZeroMQ and ROS2 Mosquitto is a light implementation of the MQTT standard. MQTT defines protocols for process interconnection. a Broker server serves as a central point to connect all the other nodes. Topics are defined as pipes of data flow. Any process can subscribe (i.e. reads) or publish (i.e. write) in the topics. Wikipedia is a good entrance point for more details. ZeroMQ follows its own protocols and propose a high-level interface to interprocess communication via difference communication architecture and different low-level protocols including in-procecus communication. ROS2 is developed for robotic purpose. It is very similar to MQTT in its communication architecture. However, in ROS , messages in topics are typed and several types and algorithms are already developed to exchange, manipulate and visualize geometrical and temporal data. Also in ROS2 the broker process is removed. For HackaGames use, ZeroMQ present the best compromise. It requires more implementation compared to MQTT - Mosquitto to permit processes to communicate but it allows the development of dedicated game/players communication protocols. This way, there is no need for a broker, and the game server can better address specific information to identified players. ROS2 solution was removed from the candidates mostly because of the huge dependencies relying on this solution. Using ROS2 induces to install and use a heavy API for developing and for managing communicating processes. One of the core advantages of ZeroMQ resides in the fact that it is an open-source solution developed in C and so on, easily integrable in any other languages. Appropriation of ZeroMQ The philosophy is to make games and players interoperable whatever the technology used to develop them. That for HackaGames proposes 2 independent implementation of its core features. One in Python ( hackapy ) and one in C ( hackalib - to come). In hackapy , interoperable API is developed in the (interprocess.py)[https://bitbucket.org/imt-mobisyst/hackagames/src/master/hackapy/interprocess.py] file. The implementation includes two main classes: Dealer and Client . The Dealer aims to represent a game. It manages Client connection each one labeled as an identified player. A tweak class Local permits a game and players to run in the same process, if all of them are implemented with Python. This way, implemented games and players are agnostic from the usage or not of interprocess architecture via ZeroMQ . Those elements can be running without any difference with a Dealer and Clients or with Local manager.","title":"Protocol API"},{"location":"hood/api/#communication-api","text":"The idea is to use a client/server architecture to permit any code in any language to seat on a game (i.e., connect to act as a player). For that we choose to use a high-level messaging library.","title":"Communication API"},{"location":"hood/api/#high-level-message-queuing-api","text":"We do not require to address directly the TCP protocol while our need for network usage is very classical. A game process acts as a server, accepts and manages client-player processes. Then the game and players exchange game states and action description. For that purpose tree solution was considered: MQTT - Mosquitto , ZeroMQ and ROS2 Mosquitto is a light implementation of the MQTT standard. MQTT defines protocols for process interconnection. a Broker server serves as a central point to connect all the other nodes. Topics are defined as pipes of data flow. Any process can subscribe (i.e. reads) or publish (i.e. write) in the topics. Wikipedia is a good entrance point for more details. ZeroMQ follows its own protocols and propose a high-level interface to interprocess communication via difference communication architecture and different low-level protocols including in-procecus communication. ROS2 is developed for robotic purpose. It is very similar to MQTT in its communication architecture. However, in ROS , messages in topics are typed and several types and algorithms are already developed to exchange, manipulate and visualize geometrical and temporal data. Also in ROS2 the broker process is removed. For HackaGames use, ZeroMQ present the best compromise. It requires more implementation compared to MQTT - Mosquitto to permit processes to communicate but it allows the development of dedicated game/players communication protocols. This way, there is no need for a broker, and the game server can better address specific information to identified players. ROS2 solution was removed from the candidates mostly because of the huge dependencies relying on this solution. Using ROS2 induces to install and use a heavy API for developing and for managing communicating processes. One of the core advantages of ZeroMQ resides in the fact that it is an open-source solution developed in C and so on, easily integrable in any other languages.","title":"High-Level Message Queuing API"},{"location":"hood/api/#appropriation-of-zeromq","text":"The philosophy is to make games and players interoperable whatever the technology used to develop them. That for HackaGames proposes 2 independent implementation of its core features. One in Python ( hackapy ) and one in C ( hackalib - to come). In hackapy , interoperable API is developed in the (interprocess.py)[https://bitbucket.org/imt-mobisyst/hackagames/src/master/hackapy/interprocess.py] file. The implementation includes two main classes: Dealer and Client . The Dealer aims to represent a game. It manages Client connection each one labeled as an identified player. A tweak class Local permits a game and players to run in the same process, if all of them are implemented with Python. This way, implemented games and players are agnostic from the usage or not of interprocess architecture via ZeroMQ . Those elements can be running without any difference with a Dealer and Clients or with Local manager.","title":"Appropriation of ZeroMQ"},{"location":"hood/newgame/","text":"Game Creation in Python The ideas here is to present step by step the game creation in python with hackapy . Reminder, hackapy is the HackaGames python librairy helping for the game and players to communicate together. Directory structure In your workspace directory create a subdirectory gameXyz where Xyz identify your new game ( gameHello for instance). This new subdirectory (your working directory) will also include another subdirectory gameEngine regrouping the source code making your game working. Directory squeletom: gameHello # your game folder - gameEngine # sourcecode of the game Then we start with 3 files: README.md : a Markdown readme first file presenting the game and the rules. gameEngine/__init__.py : a classical python files marking the entrance of your gameEngine package. start-server : a start script, lauching the game server. Game Engine At minima gameEngine python package include a Game class deriving from hackapy.AbsGame in the __init__.py file. It is suppozed that the new Game class implement the abstract methods of AbsGame : class AbsGame(): # Game interface : def initialize(self): # Initialize a new game # Return the game configuration (as a PodInterface) # the returned Pod is given to player's wake-up method. pass def playerHand( self, iPlayer ): # Return the game elements in the player vision (a PodInterface) # the returned pod feed the player's perception method. pass def applyPlayerAction( self, iPlayer, action ): # Apply the action choosen by the player iPlayer. # Return a boolean at True if the player terminate its actions for the current turn. # False means player need to be activated again before step to another player or another game turn. pass def tic( self ): # called function at turn end, after all player played its actions. pass def isEnded( self ): # must return True when the game end, and False else. pass def playerScore( self, iPlayer ): # return the player score for the current game (usefull at game ending) pass A last abstract method is defined: play . This method is more an inside method re-defined in function of how the players are managed. Actually, two strategies are proposed: AbsSequentialGame: Players are activated at turns. The perception of the game for the \\(i\\) -th player and a call to decide are sent after the \\((i-1)\\) -th player terminates its actions ( applyPlayerAction( (i-1), \"action\" ) returns True ). AbsSimultaneousGame: (Work In Progress) Players are activated in a simultaneous way. All the players receive their perception of the game then all the players are requested for their actions. Generally action resolution in these kinds of games are resolved in the tic method. Example of of the simple hello games: First, initialize python file and import the hackapy package. #!env python3 \"\"\" HackaGame - Game - Hello \"\"\" import sys sys.path.insert( 1, __file__.split('gameHello')[0] ) import hackapy as hg Then, the Game implement an abstract sequential game (each player play at turns). The Game methods to implemented are: initialize , playerHand , applyPlayerAction , isEnded and playerScore . Here the Hello game simply echo the player action in a terminal \\(3\\) times (method applyPlayerAction ). class GameHello( hg.AbsSequentialGame ) : # Game interface : def initialize(self): # initialize the counter and only say hello. self.counter= 0 return hg.Pod( 'hello' ) def playerHand( self, iPlayer ): # ping with the increasing counter return hg.Pod( 'hi', flags=[ self.counter ] ) def applyPlayerAction( self, iPlayer, action ): # print the receive action message. And that all. print( f\"Player-{iPlayer} say < {action} >\" ) return True def tic( self ): # step on the counter. self.counter= min( self.counter+1, 3 ) def isEnded( self ): # if the counter reach it final value return self.counter == 3 def playerScore( self, iPlayer ): # All players are winners. return 1 A counter initialized in initialize method, count \\(3\\) game turn (i.e. after each player play at-turn) in tic method. Then the isEnded method will return True . The method playerHand informs the player about the counter status. Finaly, there is no winner and all player will end with a result at \\(1\\) (method playerScore ). Lets play The start-server script will permit to lauch the game server. It only instancate a Game with a determined number of players then call the AbsGame start method. #!env python3 \"\"\" HackaGame - Game - Hello \"\"\" from gameEngine import GameHello game= GameHello() game.start() That it. You can set your script executable ( chmod +x ./gameHello/start-server ) and play with your new game: # In a first shell: ./gameHello/start-server # In a second shell: ./hackagames/connect-shell Game initialization: AbsGame initialization need a number of players. By default the value is on 1 . However, if you want to fix this number or if you want to add extrat initialization, you need to call the parent initialization in your own. As reminder in Python , initialization method is named __init__ and super() function provides an access to parent methods. For instance, for a 2 player game: class MyGame( hg.AbsSequentialGame ) : # Initialization: def __init__(self) : super().__init__( numberOfPlayers=2 ) self._myAttribut= \"Some initializations\" Going futher: Command Interpreter: from hackapy.command import Command, Option # Define a command interpreter: 2 options: host address and port: cmd= Command( \"start-server\", [ Option( \"port\", \"p\", default=1400 ), Option( \"number\", \"n\", 2, \"number of games\" ) ], ( \"star a server fo gameConnect4 on your machine. \" \"gameConnect4 do not take ARGUMENT.\" )) # Process the command line: cmd.process() if not cmd.ready() : print( cmd.help() ) exit() ... game.start( cmd.option(\"number\"), cmd.option(\"port\") ) Going futher: Test-Driven: You can start with a first : test_01_AbsGame script in a test directory. to verify the call to HackaGames Games methods... \"\"\" Test - Hello Games Class \"\"\" import sys sys.path.insert( 1, __file__.split('gameHello')[0] ) import hackapy as hg import gameHello.gameEngine as ge def test_gameMethod(): game= ge.GameHello() assert( type( game.initialize().asPod() ) is hg.Pod ) assert( type( game.playerHand(1).asPod() ) is hg.Pod ) assert( game.applyPlayerAction( 1, \"sleep\" ) ) game.tic() assert( not game.isEnded() ) assert( game.playerScore(1) == 1 ) ...","title":"New Game"},{"location":"hood/newgame/#game-creation-in-python","text":"The ideas here is to present step by step the game creation in python with hackapy . Reminder, hackapy is the HackaGames python librairy helping for the game and players to communicate together.","title":"Game Creation in Python"},{"location":"hood/newgame/#directory-structure","text":"In your workspace directory create a subdirectory gameXyz where Xyz identify your new game ( gameHello for instance). This new subdirectory (your working directory) will also include another subdirectory gameEngine regrouping the source code making your game working. Directory squeletom: gameHello # your game folder - gameEngine # sourcecode of the game Then we start with 3 files: README.md : a Markdown readme first file presenting the game and the rules. gameEngine/__init__.py : a classical python files marking the entrance of your gameEngine package. start-server : a start script, lauching the game server.","title":"Directory structure"},{"location":"hood/newgame/#game-engine","text":"At minima gameEngine python package include a Game class deriving from hackapy.AbsGame in the __init__.py file. It is suppozed that the new Game class implement the abstract methods of AbsGame : class AbsGame(): # Game interface : def initialize(self): # Initialize a new game # Return the game configuration (as a PodInterface) # the returned Pod is given to player's wake-up method. pass def playerHand( self, iPlayer ): # Return the game elements in the player vision (a PodInterface) # the returned pod feed the player's perception method. pass def applyPlayerAction( self, iPlayer, action ): # Apply the action choosen by the player iPlayer. # Return a boolean at True if the player terminate its actions for the current turn. # False means player need to be activated again before step to another player or another game turn. pass def tic( self ): # called function at turn end, after all player played its actions. pass def isEnded( self ): # must return True when the game end, and False else. pass def playerScore( self, iPlayer ): # return the player score for the current game (usefull at game ending) pass A last abstract method is defined: play . This method is more an inside method re-defined in function of how the players are managed. Actually, two strategies are proposed: AbsSequentialGame: Players are activated at turns. The perception of the game for the \\(i\\) -th player and a call to decide are sent after the \\((i-1)\\) -th player terminates its actions ( applyPlayerAction( (i-1), \"action\" ) returns True ). AbsSimultaneousGame: (Work In Progress) Players are activated in a simultaneous way. All the players receive their perception of the game then all the players are requested for their actions. Generally action resolution in these kinds of games are resolved in the tic method.","title":"Game Engine"},{"location":"hood/newgame/#example-of-of-the-simple-hello-games","text":"First, initialize python file and import the hackapy package. #!env python3 \"\"\" HackaGame - Game - Hello \"\"\" import sys sys.path.insert( 1, __file__.split('gameHello')[0] ) import hackapy as hg Then, the Game implement an abstract sequential game (each player play at turns). The Game methods to implemented are: initialize , playerHand , applyPlayerAction , isEnded and playerScore . Here the Hello game simply echo the player action in a terminal \\(3\\) times (method applyPlayerAction ). class GameHello( hg.AbsSequentialGame ) : # Game interface : def initialize(self): # initialize the counter and only say hello. self.counter= 0 return hg.Pod( 'hello' ) def playerHand( self, iPlayer ): # ping with the increasing counter return hg.Pod( 'hi', flags=[ self.counter ] ) def applyPlayerAction( self, iPlayer, action ): # print the receive action message. And that all. print( f\"Player-{iPlayer} say < {action} >\" ) return True def tic( self ): # step on the counter. self.counter= min( self.counter+1, 3 ) def isEnded( self ): # if the counter reach it final value return self.counter == 3 def playerScore( self, iPlayer ): # All players are winners. return 1 A counter initialized in initialize method, count \\(3\\) game turn (i.e. after each player play at-turn) in tic method. Then the isEnded method will return True . The method playerHand informs the player about the counter status. Finaly, there is no winner and all player will end with a result at \\(1\\) (method playerScore ).","title":"Example of of the simple hello games:"},{"location":"hood/newgame/#lets-play","text":"The start-server script will permit to lauch the game server. It only instancate a Game with a determined number of players then call the AbsGame start method. #!env python3 \"\"\" HackaGame - Game - Hello \"\"\" from gameEngine import GameHello game= GameHello() game.start() That it. You can set your script executable ( chmod +x ./gameHello/start-server ) and play with your new game: # In a first shell: ./gameHello/start-server # In a second shell: ./hackagames/connect-shell","title":"Lets play"},{"location":"hood/newgame/#game-initialization","text":"AbsGame initialization need a number of players. By default the value is on 1 . However, if you want to fix this number or if you want to add extrat initialization, you need to call the parent initialization in your own. As reminder in Python , initialization method is named __init__ and super() function provides an access to parent methods. For instance, for a 2 player game: class MyGame( hg.AbsSequentialGame ) : # Initialization: def __init__(self) : super().__init__( numberOfPlayers=2 ) self._myAttribut= \"Some initializations\"","title":"Game initialization:"},{"location":"hood/newgame/#going-futher-command-interpreter","text":"from hackapy.command import Command, Option # Define a command interpreter: 2 options: host address and port: cmd= Command( \"start-server\", [ Option( \"port\", \"p\", default=1400 ), Option( \"number\", \"n\", 2, \"number of games\" ) ], ( \"star a server fo gameConnect4 on your machine. \" \"gameConnect4 do not take ARGUMENT.\" )) # Process the command line: cmd.process() if not cmd.ready() : print( cmd.help() ) exit() ... game.start( cmd.option(\"number\"), cmd.option(\"port\") )","title":"Going futher: Command Interpreter:"},{"location":"hood/newgame/#going-futher-test-driven","text":"You can start with a first : test_01_AbsGame script in a test directory. to verify the call to HackaGames Games methods... \"\"\" Test - Hello Games Class \"\"\" import sys sys.path.insert( 1, __file__.split('gameHello')[0] ) import hackapy as hg import gameHello.gameEngine as ge def test_gameMethod(): game= ge.GameHello() assert( type( game.initialize().asPod() ) is hg.Pod ) assert( type( game.playerHand(1).asPod() ) is hg.Pod ) assert( game.applyPlayerAction( 1, \"sleep\" ) ) game.tic() assert( not game.isEnded() ) assert( game.playerScore(1) == 1 ) ...","title":"Going futher: Test-Driven:"},{"location":"hood/testdriven/","text":"Test Driven Development Test Driven Developement (TDD) consist of defining test that would validate a desired functionality before to develop the functionality itself. After development, the test allows to validate that the functionality works but also that functionally matches the initial expectation. Wikipedia is a good entrance point for going further in the concepts of TDD. One of the simplest ways to develop tests in Python is to use pytest . Pytest tool: A test script has file-name starting with test_ . Optionally it can be regrouped in a test directory. It is composed of test function starting with def test_ defining assert based test. For instance test_pytest.py : def test_test(): assert( True ) Then, Pytest can be used to run the tests: # install pip install pytest # execute all your tests: pytest Test your players: Test your games: A minimal test bench built considering the gameHello tutorial: \"\"\" Test - hello.Engine \"\"\" import sys sys.path.insert( 1, __file__.split('hackagames')[0] ) import hackagames.hackapy as hg import gameHello.gameEngine as ge def test_gameMethod(): game= ge.GameConnect4() assert( type( game.initialize().asPod() ) is hg.Pod ) assert( type( game.playerHand(1).asPod() ) is hg.Pod ) game.applyPlayerAction( 1, \"test\" ) game.tic() assert( not game.isEnded() ) assert( game.playerScore(1) == 0 ) def test_initialize(): game= ge.GameConnect4() wakeUpPod= game.initialize().asPod() assert( str(wakeUpPod) == \"hello:\" ) assert( wakeUpPod.family() == \"hello\" ) assert( wakeUpPod.status() == \"\" ) assert( wakeUpPod.flags() == [] ) assert( wakeUpPod.values() == [] ) assert( len( wakeUpPod.children() ) == 0 ) def test_playerHand(): game= ge.GameConnect4() game.initialize().asPod() handPod= game.playerHand(1).asPod() assert( str(handPod) == 'hi: [0]' ) assert( handPod.family() == \"hi\" ) assert( handPod.status() == \"\" ) assert( handPod.flags() == [0] ) assert( handPod.values() == [] ) assert( len( handPod.children() ) == 0 ) Then you have to adapt the test while you implement your game.","title":"Test-Driven"},{"location":"hood/testdriven/#test-driven-development","text":"Test Driven Developement (TDD) consist of defining test that would validate a desired functionality before to develop the functionality itself. After development, the test allows to validate that the functionality works but also that functionally matches the initial expectation. Wikipedia is a good entrance point for going further in the concepts of TDD. One of the simplest ways to develop tests in Python is to use pytest .","title":"Test Driven Development"},{"location":"hood/testdriven/#pytest-tool","text":"A test script has file-name starting with test_ . Optionally it can be regrouped in a test directory. It is composed of test function starting with def test_ defining assert based test. For instance test_pytest.py : def test_test(): assert( True ) Then, Pytest can be used to run the tests: # install pip install pytest # execute all your tests: pytest","title":"Pytest tool:"},{"location":"hood/testdriven/#test-your-players","text":"","title":"Test your players:"},{"location":"hood/testdriven/#test-your-games","text":"A minimal test bench built considering the gameHello tutorial: \"\"\" Test - hello.Engine \"\"\" import sys sys.path.insert( 1, __file__.split('hackagames')[0] ) import hackagames.hackapy as hg import gameHello.gameEngine as ge def test_gameMethod(): game= ge.GameConnect4() assert( type( game.initialize().asPod() ) is hg.Pod ) assert( type( game.playerHand(1).asPod() ) is hg.Pod ) game.applyPlayerAction( 1, \"test\" ) game.tic() assert( not game.isEnded() ) assert( game.playerScore(1) == 0 ) def test_initialize(): game= ge.GameConnect4() wakeUpPod= game.initialize().asPod() assert( str(wakeUpPod) == \"hello:\" ) assert( wakeUpPod.family() == \"hello\" ) assert( wakeUpPod.status() == \"\" ) assert( wakeUpPod.flags() == [] ) assert( wakeUpPod.values() == [] ) assert( len( wakeUpPod.children() ) == 0 ) def test_playerHand(): game= ge.GameConnect4() game.initialize().asPod() handPod= game.playerHand(1).asPod() assert( str(handPod) == 'hi: [0]' ) assert( handPod.family() == \"hi\" ) assert( handPod.status() == \"\" ) assert( handPod.flags() == [0] ) assert( handPod.values() == [] ) assert( len( handPod.children() ) == 0 ) Then you have to adapt the test while you implement your game.","title":"Test your games:"},{"location":"learn-ai/learn-model/","text":"Basic Model Learning: Markov-Decision-Process This tutorial aims to compute the policy from logs (transition and rewards) on 421 game. Learn model tutorial is structured as policy tutorials. Generate logs. Process the logs to produce a model. Process the model to produce a policy. The model is based on Markov Decision Process (MDP) framework: On Wikipedia [ [s', p, r], ... ] Generate logs. Modify your recorder Player from policy tutorial in order to log transitions and reward into a log_421_sasr.csv file with : (previous_state, previous_action, current_state, reward) Process logs. This process script will load the log_421_sasr.csv file to produce 2 dictionnaries: transition[previous_state][previous_action][] that return a probability. reward[previous_state][previous_action] that return the average reward of doing previous_action from previous_state . The transition and reward can be saved on 2 diferent json file. Process model. This is the interesting part of the exercice. From the transitions and rewards (load from json ) we want to compute the optimal policy \\(\\pi^*\\) For that purpose, we recommand to apply Value Iteration (cf. On Wikipedia ) Value-Iteration Input: an MDP: \\(\\langle S, A, T, R \\rangle\\) ; precision error: \\(\\epsilon\\) ; discount factor: \\(\\gamma\\) ; initial V(s) Repeat until: maximal delta < \\(\\epsilon\\) For each state \\(s \\in S\\) Search the action \\(a^*\\) maximizing the Bellman Equation on \\(s\\) \\[a^*= \\arg\\max_{a \\in A}\\left( R(s, a) + \\gamma \\sum_{s'\\in S} T(s,a,s') \\times V(s') \\right)\\] Update \\(\\pi(s)\\) and \\(V(s)\\) by considering action \\(a^*\\) Compute the delta value between the previous and the new \\(V(s)\\) Output: an optimal \\(\\pi^*\\) and associated V-values Exploit. Load the policy on your policyBot to test-it. In Reinforcement Model-Learning In fact the real chalenge is to learn the model and use the learned model at the same time.","title":"Learn Model"},{"location":"learn-ai/learn-model/#basic-model-learning-markov-decision-process","text":"This tutorial aims to compute the policy from logs (transition and rewards) on 421 game. Learn model tutorial is structured as policy tutorials. Generate logs. Process the logs to produce a model. Process the model to produce a policy. The model is based on Markov Decision Process (MDP) framework: On Wikipedia [ [s', p, r], ... ]","title":"Basic Model Learning: Markov-Decision-Process"},{"location":"learn-ai/learn-model/#generate-logs","text":"Modify your recorder Player from policy tutorial in order to log transitions and reward into a log_421_sasr.csv file with : (previous_state, previous_action, current_state, reward)","title":"Generate logs."},{"location":"learn-ai/learn-model/#process-logs","text":"This process script will load the log_421_sasr.csv file to produce 2 dictionnaries: transition[previous_state][previous_action][] that return a probability. reward[previous_state][previous_action] that return the average reward of doing previous_action from previous_state . The transition and reward can be saved on 2 diferent json file.","title":"Process logs."},{"location":"learn-ai/learn-model/#process-model","text":"This is the interesting part of the exercice. From the transitions and rewards (load from json ) we want to compute the optimal policy \\(\\pi^*\\) For that purpose, we recommand to apply Value Iteration (cf. On Wikipedia ) Value-Iteration Input: an MDP: \\(\\langle S, A, T, R \\rangle\\) ; precision error: \\(\\epsilon\\) ; discount factor: \\(\\gamma\\) ; initial V(s) Repeat until: maximal delta < \\(\\epsilon\\) For each state \\(s \\in S\\) Search the action \\(a^*\\) maximizing the Bellman Equation on \\(s\\) \\[a^*= \\arg\\max_{a \\in A}\\left( R(s, a) + \\gamma \\sum_{s'\\in S} T(s,a,s') \\times V(s') \\right)\\] Update \\(\\pi(s)\\) and \\(V(s)\\) by considering action \\(a^*\\) Compute the delta value between the previous and the new \\(V(s)\\) Output: an optimal \\(\\pi^*\\) and associated V-values","title":"Process model."},{"location":"learn-ai/learn-model/#exploit","text":"Load the policy on your policyBot to test-it.","title":"Exploit."},{"location":"learn-ai/learn-model/#in-reinforcement-model-learning","text":"In fact the real chalenge is to learn the model and use the learned model at the same time.","title":"In Reinforcement Model-Learning"},{"location":"learn-ai/policy/","text":"Understand the notion of Policy Policy is a function \\(\\pi\\) returning the action to perform regarding a given state. To better understand the notion of policy we propose to learn one for Py421 game. We wan to get a maximum of information about the interest of applying actions in game situation (state) We propose to do that by acting randomly, with a RecorderRandBot based on a py421.firstBot Bot . Record experiments The idea consists in recording information required to learn a policy, in other world, to evaluate efficiency of visited state and action. The evaluation is classically obtained at the end of a game, with the final score. The expected file would look like: state, action, result state, action, result state, action, result state, action, result state, action, result ... For instance with Py421 : 4-2-1, keep-keep-keep, 800 6-3-1, keep-roll-keep, 184 5-5-3, roll-keep-keep, 104 ... It require to trace the visited state and action. The RecorderRandBot will lookalike: class RecorderRandBot( firstBot.Bot ): def wakeUp(self, playerId, numberOfPlayers, gameConf): self._trace= [] def decide(self): state= '-'.join([ str(d) for d in self.dices ]) action= super.decide() self._trace.append( {'state': state, 'action': action} ) return action def sleep(self, result): # open a State.Action.Value file in append mode: logFile= open( \"log-421-SAV.csv\", \"a\" ) # For each recorded experience in trace for xp in self._trace : # add a line in the file logFile.write( f\"{xp['state']}, {xp['action']}, {result}\\n\" ) logFile.close() Important: the recording can only be done at sleep time. The bot need to reach the end of the game to evaluate the succession of actions it performed. Tracing the visited state and actions is performed at decide step, with a trace attribute initialized at wake-up step. You can open your log-sav.csv (state, action, value) files to see it content. Process the data Processing log-sav.csv file consists of generating a structure matching a coherent policy from brute data. Typically, the structure can be simply a dictionnary over possible states. ie: policy= { 'state1': 'actionInState1', 'state2': 'actionInState2', ... } Dictionary: - Python documentation - On w3school But first, it is required to read the log file and group the same experiences together. For instance, in state '4-3-1' random strategy will try several times the action 'keep-roll-keep' with different results. The simplest way to do that is to create a dictionary over state referencing dictionaries over action referencing lists of reached scores (expected result: data['state']['action'] -> a list of value ). In other term in a script 'process-sav.py': data= {} # Load data.. logFile= open(\"log-421-sav.csv\", \"r\") for line in logFile : state, action, value= tuple( line.split(', ') ) value= float(value) if state not in data : data[state]= {action: [value]} elif action not in data[state]: data[state][action]= [value] else : data[state][action].append( value ) logFile.close() We can now process the data: Compute the average score for each tuple (state, action) Select the action with the maximum score in a policy dictionary. In the end, policy[\"4-2-1\"] should return \"keep-keep-keep\" for instance. Notice that, a json package exists with a dump function to record the policy into a policy-421-sav.json file. policyFile= open(\"policy-421-sav.json\", \"w\") json.dump(data, policyFile, sort_keys=True, indent=2 ) policyFile.close() exploit Finally, it is possible to exploit the policy with a policyBot player. First load the policy, in the player constructor for instance: def __init__(self, policyFilePath): super().__init__() policyFile= open(policyFilePath) self.policy= json.load( policyFile ) policyFile.close() Then apply the policy actions: def decide(self): action= self.policy[ '-'.join([ str(d) for d in self.dices ]) ] return action The policy reaches an average score close up to \\(290\\) . Complete Policy: You can apply the same method but over the complete state definition. By adding the horizon in the state definition ( 4-2-1h2 for instance rather than only 4-2-1 ), it possible to reach average score of more than \\(320\\) . It will just require more experiences in the recording phase.","title":"Policy"},{"location":"learn-ai/policy/#understand-the-notion-of-policy","text":"Policy is a function \\(\\pi\\) returning the action to perform regarding a given state. To better understand the notion of policy we propose to learn one for Py421 game. We wan to get a maximum of information about the interest of applying actions in game situation (state) We propose to do that by acting randomly, with a RecorderRandBot based on a py421.firstBot Bot .","title":"Understand the notion of Policy"},{"location":"learn-ai/policy/#record-experiments","text":"The idea consists in recording information required to learn a policy, in other world, to evaluate efficiency of visited state and action. The evaluation is classically obtained at the end of a game, with the final score. The expected file would look like: state, action, result state, action, result state, action, result state, action, result state, action, result ... For instance with Py421 : 4-2-1, keep-keep-keep, 800 6-3-1, keep-roll-keep, 184 5-5-3, roll-keep-keep, 104 ... It require to trace the visited state and action. The RecorderRandBot will lookalike: class RecorderRandBot( firstBot.Bot ): def wakeUp(self, playerId, numberOfPlayers, gameConf): self._trace= [] def decide(self): state= '-'.join([ str(d) for d in self.dices ]) action= super.decide() self._trace.append( {'state': state, 'action': action} ) return action def sleep(self, result): # open a State.Action.Value file in append mode: logFile= open( \"log-421-SAV.csv\", \"a\" ) # For each recorded experience in trace for xp in self._trace : # add a line in the file logFile.write( f\"{xp['state']}, {xp['action']}, {result}\\n\" ) logFile.close() Important: the recording can only be done at sleep time. The bot need to reach the end of the game to evaluate the succession of actions it performed. Tracing the visited state and actions is performed at decide step, with a trace attribute initialized at wake-up step. You can open your log-sav.csv (state, action, value) files to see it content.","title":"Record experiments"},{"location":"learn-ai/policy/#process-the-data","text":"Processing log-sav.csv file consists of generating a structure matching a coherent policy from brute data. Typically, the structure can be simply a dictionnary over possible states. ie: policy= { 'state1': 'actionInState1', 'state2': 'actionInState2', ... } Dictionary: - Python documentation - On w3school But first, it is required to read the log file and group the same experiences together. For instance, in state '4-3-1' random strategy will try several times the action 'keep-roll-keep' with different results. The simplest way to do that is to create a dictionary over state referencing dictionaries over action referencing lists of reached scores (expected result: data['state']['action'] -> a list of value ). In other term in a script 'process-sav.py': data= {} # Load data.. logFile= open(\"log-421-sav.csv\", \"r\") for line in logFile : state, action, value= tuple( line.split(', ') ) value= float(value) if state not in data : data[state]= {action: [value]} elif action not in data[state]: data[state][action]= [value] else : data[state][action].append( value ) logFile.close() We can now process the data: Compute the average score for each tuple (state, action) Select the action with the maximum score in a policy dictionary. In the end, policy[\"4-2-1\"] should return \"keep-keep-keep\" for instance. Notice that, a json package exists with a dump function to record the policy into a policy-421-sav.json file. policyFile= open(\"policy-421-sav.json\", \"w\") json.dump(data, policyFile, sort_keys=True, indent=2 ) policyFile.close()","title":"Process the data"},{"location":"learn-ai/policy/#exploit","text":"Finally, it is possible to exploit the policy with a policyBot player. First load the policy, in the player constructor for instance: def __init__(self, policyFilePath): super().__init__() policyFile= open(policyFilePath) self.policy= json.load( policyFile ) policyFile.close() Then apply the policy actions: def decide(self): action= self.policy[ '-'.join([ str(d) for d in self.dices ]) ] return action The policy reaches an average score close up to \\(290\\) .","title":"exploit"},{"location":"learn-ai/policy/#complete-policy","text":"You can apply the same method but over the complete state definition. By adding the horizon in the state definition ( 4-2-1h2 for instance rather than only 4-2-1 ), it possible to reach average score of more than \\(320\\) . It will just require more experiences in the recording phase.","title":"Complete Policy:"},{"location":"learn-ai/qlearning/","text":"Basic Reinforcement Learning: Q-Learning Reinforcement Learning is a family of approaches and algorithms that enhance an autonomous system (an agent) to learn from it successful tries and failures [Cf. Wikipedia ]. Technically, at the beginning, the agent is capable of acting in it environment (with default pure random action for instance) and by acting it gets feedback. Accumulating feedback, it will be capable of evaluating the interest of actions in a given context. In reinforcement Learning family, Q-Learning is quite a simple and apply pretty well for learning to play 421 . The goal of the tutorial: Implement a new QLearnerBot player from random player. At initialization qvalues is created empty with the other required variables. At decision steps the player updates its qvalues and choose a new action to perform. And do not forget: you ~~can~~ must test your code at each development step by executing the code for a few games and validate that the output is as expected (also a good python tool to make test: pytest ). qvalues equation The Q-Learning algorithm ([Cf. Wikipedia ]) consists in computing qvalues , a dictionary of the interest or value of executing a given action from a given state. \\[ \\mathit{qvalues}(s, a) \\in \\mathbb{R} \\] A qvalue is equal to the immediate reward of doing action \\(a\\) in a state \\(s\\) plus the future rewards modeled as \\(\\mathit{qvalues}(s', a')\\) . \\(s'\\) is the state reached from (s, a) and \\(a'\\) the next action that will be performed. In other terms, \\(\\mathit{qvalues}(s, a)\\) is the cumulative reward from \\(s\\) , knowing that the next action is \\(a\\) . However, the resulting experiment of doing \\(a\\) from \\(s\\) need to be computed as an average. Finally, one of the ways to formalize updates on \\(\\mathit{qvalues}(s, a)\\) is: \\[ \\mathit{qvalues}(s, a) = \\alpha(\\ r(s, a, s') + \\mathit{qvalues}(s', a')\\ ) + (1-\\alpha)(\\ \\mathit{qvalues}(s, a)\\ ) \\] The learning rate \\(\\alpha\\) is the speed that incoming experiences erase the oldest (can be initialized at \\(0.1\\) as a first approximation). Implementing qvalues At some point, our new player will require to update its qvalues from its experiments. So let's start with a method: def updateQvalues(self, previous_state, previous_action, reward, current_state, current_action): ... A simple way to implement qvalues in python language is to implement it as a Dictionnary of dictionaries (expected result: qvalues['state']['action'] -> the interest of doing 'action' in 'state' ). But first: initializing empty qvalues will look like: qvalues= {} Typically in the constructor method __init__ in python (and with Q-Learning attributes): class QPlayer : def __init__(self): self.alpha= 0.1 # learning rate, speed that an incoming experience erases the oldest. self.qvalues= {} # Returning to the updateQvalues method, initializing action values for a given state will look like: state= \"4-2-1\" if state not in self.qvalues.keys() : self.qvalues[state]= { \"keep-keep-keep\":0.0, \"roll-keep-keep\":0.0, \"keep-roll-keep\":0.0, \"roll-roll-keep\":0.0, \"keep-keep-roll\":0.0, \"roll-keep-roll\":0.0, \"keep-roll-roll\":0.0, \"roll-roll-roll\":0.0 } Finally, modifying a value in qvalues will look like: self.qvalues[\"4-2-1\"][\"roll-roll-roll\"]= ... When to update qvalues? The updateQvalues method requires to confront current state and action with memorized previous state and action. It can be performed while a new state is reaches. So a first call to updateQvalues can be implemented into perception method. At this time, we consider that we have no idea about the value of doing this or that action or of reaching a specific state. The reward is set to \\(0\\) . However, a last call to the updateQvalues need to be set-up into the sleep method. At sleep step, the reward matches the game score, and a special state \"end\" with a fix action \"sleep\" need to be added to the qValues. Exploration-Exploitation Dilemma Until now we just compute and record statistical rewards. The idea is to use-it on to take decision at some time (i.e. when we record a good knowledge). However the first difficulty in reinforcement learning result on the definition of this \"at some time\". In other terms, when to stop the computation of statistical kwonledge by exploring actions and to start the exploitation of computed statistics to make decisions. It is known as the exploration versus exploitation trade-off [cf. wikipedia ]. One way to overpass the trade-off is to put it random. The \u03b5-greedy heuristic suppose that you will choose an exploration action (i.e. a random action for instance) a few times in a given time step. More technically, at each time step the AI randomly choose to get a random action or to get the best one accordingly to the current knowledge. The random chose of explore versus exploit is weighted by \u03b5 and 1-\u03b5 with \u03b5 between 1 and 0. Do not forget to initialize the self.epsilon in the constructor method ( self.epsilon= 0.1 is generally a good first value, it states that a random action would be chosen 1 time over 10). Experiments You can now try to answer the question: how many episodes are required to learn a good enough policy. One way to do that is to plot the evolution of the strategy during the learning. The package pyplot provides tools for instance to do that. At the end of a series of games, it would be more interesting to plot the evolution of the average score rather to compute a unique average. Generate a plot with one point every \\(500\\) games for instance. Typically after playing X games: import matplotlib.pyplot as plt scores= [] size= len(results) for i in range(0, size, 500) : s= min(i+500, size) scores.append( sum([ x for x in results[i:s] ]) / (s-i) ) plt.plot( [ 500*i for i in range(len(scores)) ], scores ) plt.show()","title":"Q Learning"},{"location":"learn-ai/qlearning/#basic-reinforcement-learning-q-learning","text":"Reinforcement Learning is a family of approaches and algorithms that enhance an autonomous system (an agent) to learn from it successful tries and failures [Cf. Wikipedia ]. Technically, at the beginning, the agent is capable of acting in it environment (with default pure random action for instance) and by acting it gets feedback. Accumulating feedback, it will be capable of evaluating the interest of actions in a given context. In reinforcement Learning family, Q-Learning is quite a simple and apply pretty well for learning to play 421 . The goal of the tutorial: Implement a new QLearnerBot player from random player. At initialization qvalues is created empty with the other required variables. At decision steps the player updates its qvalues and choose a new action to perform. And do not forget: you ~~can~~ must test your code at each development step by executing the code for a few games and validate that the output is as expected (also a good python tool to make test: pytest ).","title":"Basic Reinforcement Learning: Q-Learning"},{"location":"learn-ai/qlearning/#qvalues-equation","text":"The Q-Learning algorithm ([Cf. Wikipedia ]) consists in computing qvalues , a dictionary of the interest or value of executing a given action from a given state. \\[ \\mathit{qvalues}(s, a) \\in \\mathbb{R} \\] A qvalue is equal to the immediate reward of doing action \\(a\\) in a state \\(s\\) plus the future rewards modeled as \\(\\mathit{qvalues}(s', a')\\) . \\(s'\\) is the state reached from (s, a) and \\(a'\\) the next action that will be performed. In other terms, \\(\\mathit{qvalues}(s, a)\\) is the cumulative reward from \\(s\\) , knowing that the next action is \\(a\\) . However, the resulting experiment of doing \\(a\\) from \\(s\\) need to be computed as an average. Finally, one of the ways to formalize updates on \\(\\mathit{qvalues}(s, a)\\) is: \\[ \\mathit{qvalues}(s, a) = \\alpha(\\ r(s, a, s') + \\mathit{qvalues}(s', a')\\ ) + (1-\\alpha)(\\ \\mathit{qvalues}(s, a)\\ ) \\] The learning rate \\(\\alpha\\) is the speed that incoming experiences erase the oldest (can be initialized at \\(0.1\\) as a first approximation).","title":"qvalues equation"},{"location":"learn-ai/qlearning/#implementing-qvalues","text":"At some point, our new player will require to update its qvalues from its experiments. So let's start with a method: def updateQvalues(self, previous_state, previous_action, reward, current_state, current_action): ... A simple way to implement qvalues in python language is to implement it as a Dictionnary of dictionaries (expected result: qvalues['state']['action'] -> the interest of doing 'action' in 'state' ). But first: initializing empty qvalues will look like: qvalues= {} Typically in the constructor method __init__ in python (and with Q-Learning attributes): class QPlayer : def __init__(self): self.alpha= 0.1 # learning rate, speed that an incoming experience erases the oldest. self.qvalues= {} # Returning to the updateQvalues method, initializing action values for a given state will look like: state= \"4-2-1\" if state not in self.qvalues.keys() : self.qvalues[state]= { \"keep-keep-keep\":0.0, \"roll-keep-keep\":0.0, \"keep-roll-keep\":0.0, \"roll-roll-keep\":0.0, \"keep-keep-roll\":0.0, \"roll-keep-roll\":0.0, \"keep-roll-roll\":0.0, \"roll-roll-roll\":0.0 } Finally, modifying a value in qvalues will look like: self.qvalues[\"4-2-1\"][\"roll-roll-roll\"]= ...","title":"Implementing qvalues"},{"location":"learn-ai/qlearning/#when-to-update-qvalues","text":"The updateQvalues method requires to confront current state and action with memorized previous state and action. It can be performed while a new state is reaches. So a first call to updateQvalues can be implemented into perception method. At this time, we consider that we have no idea about the value of doing this or that action or of reaching a specific state. The reward is set to \\(0\\) . However, a last call to the updateQvalues need to be set-up into the sleep method. At sleep step, the reward matches the game score, and a special state \"end\" with a fix action \"sleep\" need to be added to the qValues.","title":"When to update qvalues?"},{"location":"learn-ai/qlearning/#exploration-exploitation-dilemma","text":"Until now we just compute and record statistical rewards. The idea is to use-it on to take decision at some time (i.e. when we record a good knowledge). However the first difficulty in reinforcement learning result on the definition of this \"at some time\". In other terms, when to stop the computation of statistical kwonledge by exploring actions and to start the exploitation of computed statistics to make decisions. It is known as the exploration versus exploitation trade-off [cf. wikipedia ]. One way to overpass the trade-off is to put it random. The \u03b5-greedy heuristic suppose that you will choose an exploration action (i.e. a random action for instance) a few times in a given time step. More technically, at each time step the AI randomly choose to get a random action or to get the best one accordingly to the current knowledge. The random chose of explore versus exploit is weighted by \u03b5 and 1-\u03b5 with \u03b5 between 1 and 0. Do not forget to initialize the self.epsilon in the constructor method ( self.epsilon= 0.1 is generally a good first value, it states that a random action would be chosen 1 time over 10).","title":"Exploration-Exploitation Dilemma"},{"location":"learn-ai/qlearning/#experiments","text":"You can now try to answer the question: how many episodes are required to learn a good enough policy. One way to do that is to plot the evolution of the strategy during the learning. The package pyplot provides tools for instance to do that. At the end of a series of games, it would be more interesting to plot the evolution of the average score rather to compute a unique average. Generate a plot with one point every \\(500\\) games for instance. Typically after playing X games: import matplotlib.pyplot as plt scores= [] size= len(results) for i in range(0, size, 500) : s= min(i+500, size) scores.append( sum([ x for x in results[i:s] ]) / (s-i) ) plt.plot( [ 500*i for i in range(len(scores)) ], scores ) plt.show()","title":"Experiments"},{"location":"learn-ai/scale-up/","text":"Scale-Up The goal of the tutorial is to understand, feel the complexity of addressing a system with combinatorial explosion over the number of states... This tutorial relies on MoveIt game. Make sure you clearly understand the game and its API. Basic Q-Learning: In its basic configuration MoveIt involves a \\(6 \\times 4\\) grid with randomly generated obstacles. Has a first exercise you can apply basic Q-Learning with a fixed configuration by fixing the random seed at game creation ( game= GameMoveIt( seed=128 ) ). In this case the system state 'only' involve the positions of the robot and of the humans. Something as \\(3\\) variables into \\(6 \\times 4\\) possibilities. We also require the robot target, but will only record the directions ( \\(\\in [0, 6]\\) )... def state(self): robot, human1, human2 = tuple(self._mobiles) # Path to the goal: pathGoal= self._board.path( robot.x(), robot.y(), robot.goalx(), robot.goaly() ) state= f\"{robot.x()}-{robot.y()}-{pathGoal[0]}\" state+= f\"-{human1.x()}-{human1.y()}\" state+= f\"-{human2.x()}-{human2.y()}\" return state At this point the system is modeled with \\((6 \\times 4)^3*6\\) states. Lucky for us, the reward can be easily computed at each time steps (in the perception method). newScore= statePod.value(1) reward= newScore - self._score self._score= newScore You can now apply basic Q-Learning as experimented on 4.2.1 . Expert Heuristic Optimizations: A Basic Q-Learning learning is slow and will be blocked at something like \\(-100\\) meaning that the robot continues to generate collision. A first optimization of the approach consists of including expert knowledge to limit the bad actions and to speed up the learning process. Remove forbidden actions: At decision time, it is possible to remove from the pool of available actions, the actions driving the robot on an obstacle or on a human. Speed-up Learning process Each time a new state is visited and recorded on Q dictionary, it is possible to associate the action identified by the path to the robot target with a positive value. Still negative ? In fact, the state space does not integrate the trajectory of the humans. The humans generally move toward their own goal (with error exception). The human\u2019s goals are hidden to the robot, but the human trajectory into the bot state to better anticipate the next human's moves. Scale-up: Mission on a \\(4 \\times 6\\) grid with \\(2\\) humans remains a very limited scenario. At launch time, it is possible to change the game configuration: game= GameMoveIt( seed=128, # Grid seed for generations... sizeHeight=4, # size of the grid: number of lines sizeLine= 6, # size of the grid: number of cells in a line numberOfObstacles= 6, # number of obstacles numberOfHuman= 3 # number of humans ) The provided solution should be capable of learning rapidly, new behavior on new environment. Local vs Global States The real problem consists in removing the limitation of learning behairio on a unique environment settings, but to be capable of using the learned policy in all situations. By integrating map configuration into the state variables, the state space will grow exponentially. Which variables ? For how many states ? A better solution consists to model the system from the point of view of the actions. We speak about local versus global models. For instance, the goal can be characterized as direction and distance rather than \\((x, y)\\) coordinate. Propose a state definition based from robot location in a less dependent way to the grid dimension and try again Q-Learning algorithm. To notice that local representation would be more suitable with Decision-Tree.","title":"Scale-Up"},{"location":"learn-ai/scale-up/#scale-up","text":"The goal of the tutorial is to understand, feel the complexity of addressing a system with combinatorial explosion over the number of states... This tutorial relies on MoveIt game. Make sure you clearly understand the game and its API.","title":"Scale-Up"},{"location":"learn-ai/scale-up/#basic-q-learning","text":"In its basic configuration MoveIt involves a \\(6 \\times 4\\) grid with randomly generated obstacles. Has a first exercise you can apply basic Q-Learning with a fixed configuration by fixing the random seed at game creation ( game= GameMoveIt( seed=128 ) ). In this case the system state 'only' involve the positions of the robot and of the humans. Something as \\(3\\) variables into \\(6 \\times 4\\) possibilities. We also require the robot target, but will only record the directions ( \\(\\in [0, 6]\\) )... def state(self): robot, human1, human2 = tuple(self._mobiles) # Path to the goal: pathGoal= self._board.path( robot.x(), robot.y(), robot.goalx(), robot.goaly() ) state= f\"{robot.x()}-{robot.y()}-{pathGoal[0]}\" state+= f\"-{human1.x()}-{human1.y()}\" state+= f\"-{human2.x()}-{human2.y()}\" return state At this point the system is modeled with \\((6 \\times 4)^3*6\\) states. Lucky for us, the reward can be easily computed at each time steps (in the perception method). newScore= statePod.value(1) reward= newScore - self._score self._score= newScore You can now apply basic Q-Learning as experimented on 4.2.1 .","title":"Basic Q-Learning:"},{"location":"learn-ai/scale-up/#expert-heuristic-optimizations","text":"A Basic Q-Learning learning is slow and will be blocked at something like \\(-100\\) meaning that the robot continues to generate collision. A first optimization of the approach consists of including expert knowledge to limit the bad actions and to speed up the learning process.","title":"Expert Heuristic Optimizations:"},{"location":"learn-ai/scale-up/#remove-forbidden-actions","text":"At decision time, it is possible to remove from the pool of available actions, the actions driving the robot on an obstacle or on a human.","title":"Remove forbidden actions:"},{"location":"learn-ai/scale-up/#speed-up-learning-process","text":"Each time a new state is visited and recorded on Q dictionary, it is possible to associate the action identified by the path to the robot target with a positive value.","title":"Speed-up Learning process"},{"location":"learn-ai/scale-up/#still-negative","text":"In fact, the state space does not integrate the trajectory of the humans. The humans generally move toward their own goal (with error exception). The human\u2019s goals are hidden to the robot, but the human trajectory into the bot state to better anticipate the next human's moves.","title":"Still negative ?"},{"location":"learn-ai/scale-up/#scale-up_1","text":"Mission on a \\(4 \\times 6\\) grid with \\(2\\) humans remains a very limited scenario. At launch time, it is possible to change the game configuration: game= GameMoveIt( seed=128, # Grid seed for generations... sizeHeight=4, # size of the grid: number of lines sizeLine= 6, # size of the grid: number of cells in a line numberOfObstacles= 6, # number of obstacles numberOfHuman= 3 # number of humans ) The provided solution should be capable of learning rapidly, new behavior on new environment.","title":"Scale-up:"},{"location":"learn-ai/scale-up/#local-vs-global-states","text":"The real problem consists in removing the limitation of learning behairio on a unique environment settings, but to be capable of using the learned policy in all situations. By integrating map configuration into the state variables, the state space will grow exponentially. Which variables ? For how many states ? A better solution consists to model the system from the point of view of the actions. We speak about local versus global models. For instance, the goal can be characterized as direction and distance rather than \\((x, y)\\) coordinate. Propose a state definition based from robot location in a less dependent way to the grid dimension and try again Q-Learning algorithm. To notice that local representation would be more suitable with Decision-Tree.","title":"Local vs Global States"}]}