{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"An Hackable Games' Engine Welcome to the HackaGames documentation. HackaGames is an open game engine dedicated to the development of Artificial Intelligence (AI) based on Combinatorial Optimization (CO) technique. The philosophy of hackagames is to permit developers to easily work in any language of thier choice. For that, the project is based on a communication protocol relying on ZeroMQ and is developed accordingly to KISS (Keep It Stupid Simple) principle. The main feature of this project is to permit the game, players and AIs to work on their own process potentially distributed over different machines. In other terms, HackaGames implements a simple client/server architecture to permit AI to take a seat on a game by agreeing on a communication protocol. HackaGames is seen as an API for game development. Several games are proposed for example: Py421 : A very simple one player dice game to get the concept of AI implementation. The project itself and it source code are available on github . Concurency: HackaGames is not what you are looking for ? Try those solutions: ludii a general game system designed to play, evaluate and design a wide range of games (JAVA) pettingzoo of farama multi-agent learning framework (Python) pommerman a hackable Bomberman game (Python) codingame web-based environment for NPC development (complete solution for one file codes). Roblox an online game platform and game creation system that allows users to program games and play games created by other users. Godot Open Source Game engine (dev in Cpp) (or even more K.I.S with RayLib - dev in C).","title":"Home"},{"location":"#an-hackable-games-engine","text":"Welcome to the HackaGames documentation. HackaGames is an open game engine dedicated to the development of Artificial Intelligence (AI) based on Combinatorial Optimization (CO) technique. The philosophy of hackagames is to permit developers to easily work in any language of thier choice. For that, the project is based on a communication protocol relying on ZeroMQ and is developed accordingly to KISS (Keep It Stupid Simple) principle. The main feature of this project is to permit the game, players and AIs to work on their own process potentially distributed over different machines. In other terms, HackaGames implements a simple client/server architecture to permit AI to take a seat on a game by agreeing on a communication protocol. HackaGames is seen as an API for game development. Several games are proposed for example: Py421 : A very simple one player dice game to get the concept of AI implementation. The project itself and it source code are available on github .","title":"An Hackable Games' Engine"},{"location":"#concurency","text":"HackaGames is not what you are looking for ? Try those solutions: ludii a general game system designed to play, evaluate and design a wide range of games (JAVA) pettingzoo of farama multi-agent learning framework (Python) pommerman a hackable Bomberman game (Python) codingame web-based environment for NPC development (complete solution for one file codes). Roblox an online game platform and game creation system that allows users to program games and play games created by other users. Godot Open Source Game engine (dev in Cpp) (or even more K.I.S with RayLib - dev in C).","title":"Concurency:"},{"location":"f.a.q/","text":"F.A.Q Are we forced to use the client-server architecture ? It is possible to shunt network architecture if the game and all the players' codes are in the same language (in python for instance). In that case, a game can generally be started in a test mode with all its players in one unique process (cf. the start-interactive scripts). For instance, for Py421 solo game, you have to edit a python script launcherPy421.py . The code requires to import the game and the players (only one here) and to instantiate them with the testPlayer method: #!env python3 from hackagames.gamePy421.gameEngine import GameSolo as Game from tutos.myPy421Bot import AutonomousPlayer as Player # Instanciate and start 100 games game= Game() player= Player() results= game.testPlayer( player, 100 ) That it, you can execute your script: python3 ./tutos/launcherPy421.py which calls your player. The second attribute in testPlayer method of game instance ( 100 here) is the number of games the players will play before the process end. Is there some courses ? yes, here: on the repo of the courses","title":"F.A.Q"},{"location":"f.a.q/#faq","text":"","title":"F.A.Q"},{"location":"f.a.q/#are-we-forced-to-use-the-client-server-architecture","text":"It is possible to shunt network architecture if the game and all the players' codes are in the same language (in python for instance). In that case, a game can generally be started in a test mode with all its players in one unique process (cf. the start-interactive scripts). For instance, for Py421 solo game, you have to edit a python script launcherPy421.py . The code requires to import the game and the players (only one here) and to instantiate them with the testPlayer method: #!env python3 from hackagames.gamePy421.gameEngine import GameSolo as Game from tutos.myPy421Bot import AutonomousPlayer as Player # Instanciate and start 100 games game= Game() player= Player() results= game.testPlayer( player, 100 ) That it, you can execute your script: python3 ./tutos/launcherPy421.py which calls your player. The second attribute in testPlayer method of game instance ( 100 here) is the number of games the players will play before the process end.","title":"Are we forced to use the client-server architecture ?"},{"location":"f.a.q/#is-there-some-courses","text":"yes, here: on the repo of the courses","title":"Is there some courses ?"},{"location":"games/connect4/","text":"Connect4 Connect4 is a HackaGames game, A simple two-player game where each player tries to align 4 of their pieces. Try the game: The play/connect4.py script starts the game with an interactive interface in a shell. python3 hackagames/play/connect4.py The player can perform one and only one action at its turn, and the game stops automatically with a winner or when no more pieces can be set on the grid. The actions consist of positioning a player's piece on one of the grid columns. There are 7 possible actions: A , B , C , D , E , F and G for the corresponding column. Example of the game at some point: A B C D E F G | | | | | | | | | | | | | | | | | | | | | | | | | | | | O | | | | | | | X | X | | | | | X | O | O | O | | | X | ----------------------------- you: O First player play 'O' and the second 'X'. The first player aligning 4 of its pieces, in any direction win the game. Initialize an Autonomous Player If you are implementing your first bot, please follow the first bot tutorial on Py421 game. The wakeUp method informs about the mode of the game, the perception of the status of the grid and the possibilities of actions. The hackagames Connect4 package includes a very useful Grid class to manipulate the game state. A minimal random Bot can be implemented in a few lines: from hacka.games.connect4.grid import Grid import random class Bot : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gamePod): assert( gamePod.family() == 'Connect4') self._playerId= playerId self._grid= Grid() def perceive(self, gameState): # update the game state: self._grid.fromPod( gameState ) def decide(self): options = self._grid.possibilities() return random.choice( options ) def sleep(self, result): pass And can be tested thanks to a very simple launcher script: # Setup a game: from hacka.games.connect4 import GameConnect4 as Game from hacka.games.connect4.firstBot import Bot as Opponent # Setup your bot: from myRandomBot import Bot bot= Bot() oppo= Opponent() # Instanciate and start 100 games game= Game() results= game.test2Players( bot, oppo, 1000 ) # Analyze the result print( f\"Average score: {sum(results[0])/len(results[0])} vs {sum(results[1])/len(results[1])}\" ) At this point, the results comparing the 2 Bot would be very close. The 2 bots have the same behavior. To notice that, the list of actions do not filter positions already taken. However in case of wrong actions, the game will ask the player for a second call. Customaize your Bot: You can explore the grid to select the best action possible. Some useful Gird methods: columnSize(self) : The number of columns ( 7 in classical configuration) heightMax(self) : The height of the grid ( 6 position per column in classical configuration) height(self, iColumn) : the actual height of the iColumn column (considered the played piece in this column) column(self, iColumn) : a list of integers modeling the iColumn column pieces ( 0 no player, 1 player 1 and 2 player 2 ) position(self, c, h) : The value at column c height h ( 0 , 1 or 2 ). To test a move you will need: copy(self) : returning a deep copy of the grid (before to make some changes) playerPlay(self, iPlayer, aLetter) : to alter the grid considering that player iPlayer play on the aLetter column. winner(self) : returns the player Id winning the game if 4 of his pieces are aligned. possibilities(self) : returns the list of possible move (letters), if the columns are not full. To notice that you can move from column letter to the column identifier and vice versa with chr (a char from an integer) and ord (the interger code of a char). For instance: iColumn= ord(aLetter)-ord('A') aLetter= chr( ord('A')+iColumn )","title":"Connect4"},{"location":"games/connect4/#connect4","text":"Connect4 is a HackaGames game, A simple two-player game where each player tries to align 4 of their pieces.","title":"Connect4"},{"location":"games/connect4/#try-the-game","text":"The play/connect4.py script starts the game with an interactive interface in a shell. python3 hackagames/play/connect4.py The player can perform one and only one action at its turn, and the game stops automatically with a winner or when no more pieces can be set on the grid. The actions consist of positioning a player's piece on one of the grid columns. There are 7 possible actions: A , B , C , D , E , F and G for the corresponding column. Example of the game at some point: A B C D E F G | | | | | | | | | | | | | | | | | | | | | | | | | | | | O | | | | | | | X | X | | | | | X | O | O | O | | | X | ----------------------------- you: O First player play 'O' and the second 'X'. The first player aligning 4 of its pieces, in any direction win the game.","title":"Try the game:"},{"location":"games/connect4/#initialize-an-autonomous-player","text":"If you are implementing your first bot, please follow the first bot tutorial on Py421 game. The wakeUp method informs about the mode of the game, the perception of the status of the grid and the possibilities of actions. The hackagames Connect4 package includes a very useful Grid class to manipulate the game state. A minimal random Bot can be implemented in a few lines: from hacka.games.connect4.grid import Grid import random class Bot : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gamePod): assert( gamePod.family() == 'Connect4') self._playerId= playerId self._grid= Grid() def perceive(self, gameState): # update the game state: self._grid.fromPod( gameState ) def decide(self): options = self._grid.possibilities() return random.choice( options ) def sleep(self, result): pass And can be tested thanks to a very simple launcher script: # Setup a game: from hacka.games.connect4 import GameConnect4 as Game from hacka.games.connect4.firstBot import Bot as Opponent # Setup your bot: from myRandomBot import Bot bot= Bot() oppo= Opponent() # Instanciate and start 100 games game= Game() results= game.test2Players( bot, oppo, 1000 ) # Analyze the result print( f\"Average score: {sum(results[0])/len(results[0])} vs {sum(results[1])/len(results[1])}\" ) At this point, the results comparing the 2 Bot would be very close. The 2 bots have the same behavior. To notice that, the list of actions do not filter positions already taken. However in case of wrong actions, the game will ask the player for a second call.","title":"Initialize an Autonomous Player"},{"location":"games/connect4/#customaize-your-bot","text":"You can explore the grid to select the best action possible. Some useful Gird methods: columnSize(self) : The number of columns ( 7 in classical configuration) heightMax(self) : The height of the grid ( 6 position per column in classical configuration) height(self, iColumn) : the actual height of the iColumn column (considered the played piece in this column) column(self, iColumn) : a list of integers modeling the iColumn column pieces ( 0 no player, 1 player 1 and 2 player 2 ) position(self, c, h) : The value at column c height h ( 0 , 1 or 2 ). To test a move you will need: copy(self) : returning a deep copy of the grid (before to make some changes) playerPlay(self, iPlayer, aLetter) : to alter the grid considering that player iPlayer play on the aLetter column. winner(self) : returns the player Id winning the game if 4 of his pieces are aligned. possibilities(self) : returns the list of possible move (letters), if the columns are not full. To notice that you can move from column letter to the column identifier and vice versa with chr (a char from an integer) and ord (the interger code of a char). For instance: iColumn= ord(aLetter)-ord('A') aLetter= chr( ord('A')+iColumn )","title":"Customaize your Bot:"},{"location":"games/py421/","text":"Py421 Py421 is an HackaGames game, a simple dice game where players try to optimize their dices combination after a maximum of 2 roll dice steps. Try the game: The play/py421.py script starts the game with an interactive interface in a shell. python3 hackagames/play/py421.py Py421 is a 3-dice game. The player can roll several times to get a combination. The player can perform one and only one action on his turn, and the game stops automatically after 2 turns. The actions consist in keeping or rolling each of the 3 dices. So there are 8 possible actions: keep-keep-keep , keep-keep-roll , keep-roll-keep , keep-roll-roll , roll-keep-keep , roll-keep-roll , roll-roll-keep and roll-roll-roll The goal is to optimize the combination of dices before the end of the 2 turns. The best combination ever is 4-2-1 for 800 points. But you can explore other combinations. Initialize a Bot If you are implementing your first bot, please follow the first bot tutorial. As for any Hackagames bots, a Py421 bot implements the the 4 player/bot methods wakeUp , perceive , decide and sleep . Theire is no information to get from The gameConf . The gameState parameter of the perceive includes 2 elements (2 gameState childs). First child model the roll-again horizon (one integer value), the second child the dices ( \\(3\\) intergers and \\(1\\) floating point value for the score of the combinaison). import random class Bot : def actions(self): return [ 'keep-keep-keep', 'keep-keep-roll', 'keep-roll-keep', 'keep-roll-roll', 'roll-keep-keep', 'roll-keep-roll', 'roll-roll-keep', 'roll-roll-roll' ] # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): self._horizon= 3 self._dices= [0, 0, 0] self._score= 0.0 def perceive(self, gameState): self._horizon= gameState.child(1).integer(1) self._dices= gameState.child(2).integers() self._score= gameState.child(2).value(1) def decide(self): return random.choice( self.actions() ) def sleep(self, result): print( \"> result: \"+ str(result) ) You can try your Bot AI by importing GameSolo into a launch script: #!python3 # Setup a game: from hacka.games.py421 import GameSolo as Game from bot421 import Bot bot= Bot() # Instanciate and start 100 games game= Game() results= game.testPlayer( bot, 10 ) # Analyze the result print( f\"Average score: {sum(results)/len(results)}\" ) The testPlayer method returns the results of bot player. Multi-player version You can confront 2 Bot-AIs by importing GameDuo rather than GameSolo into your launch script: #!python3 # Setup a game: from hacka.games.py421 import GameDuo as Game from py421Bot import Bot as Bot1 from duo421Bot import Bot as Bot2 # Instanciate and start 100 games game= Game() bot1= Bot1() bot2= Bot2() results= game.launch( [bot1, bot2], 10 ) # Analyze the result print( f\"Average scores: \" ) for i in range(1, 3) : pres= results[i-1] print( f\" - Player-{i}: {sum(pres)/len(pres)}\" ) The launch method returns the results for all bots players. It supose you have a second Bot class in a new file duo421Bot.py . Optimize a 2-player Bot : A Bot designed for solo mode of 421 can also play to duo mode. However, the wakeUp methods informative and the perception method is incresed in duo mode. The player identifier of the wakeUp inform if the bot is the first or the second player. The gameState also include the conbinaison of your opponent: def perceive(self, gameState): self._horizon= gameState.child(1).integer(1) self._dices= gameState.child(2).integers() self._score= gameState.child(2).value(1) self._oppo_dices= gameState.child(3).integers() self._oppo_score= gameState.child(3).value(1) print( f'\\t> H: {self._horizon}, DICES: {self._dices}, SCORE: {self._score} (VS: {self._oppo_dices} {self._oppo_score})' ) Use this increased state definition to optimize the bot's strategies. Server Mode It is possible to start 421 in duo mode. To try this version executes the following commad: python3 hackagames/play/py421 duo -n 2 That start a 2 games 4.2.1 where you have to confront the random player. The first game you play as the first player role. You control the number of roll-again. The opponent will not be capable to roll its dice more time than you. The second game, you play the second player role. You know the combinaision to beat.","title":"Py4.2.1"},{"location":"games/py421/#py421","text":"Py421 is an HackaGames game, a simple dice game where players try to optimize their dices combination after a maximum of 2 roll dice steps.","title":"Py421"},{"location":"games/py421/#try-the-game","text":"The play/py421.py script starts the game with an interactive interface in a shell. python3 hackagames/play/py421.py Py421 is a 3-dice game. The player can roll several times to get a combination. The player can perform one and only one action on his turn, and the game stops automatically after 2 turns. The actions consist in keeping or rolling each of the 3 dices. So there are 8 possible actions: keep-keep-keep , keep-keep-roll , keep-roll-keep , keep-roll-roll , roll-keep-keep , roll-keep-roll , roll-roll-keep and roll-roll-roll The goal is to optimize the combination of dices before the end of the 2 turns. The best combination ever is 4-2-1 for 800 points. But you can explore other combinations.","title":"Try the game:"},{"location":"games/py421/#initialize-a-bot","text":"If you are implementing your first bot, please follow the first bot tutorial. As for any Hackagames bots, a Py421 bot implements the the 4 player/bot methods wakeUp , perceive , decide and sleep . Theire is no information to get from The gameConf . The gameState parameter of the perceive includes 2 elements (2 gameState childs). First child model the roll-again horizon (one integer value), the second child the dices ( \\(3\\) intergers and \\(1\\) floating point value for the score of the combinaison). import random class Bot : def actions(self): return [ 'keep-keep-keep', 'keep-keep-roll', 'keep-roll-keep', 'keep-roll-roll', 'roll-keep-keep', 'roll-keep-roll', 'roll-roll-keep', 'roll-roll-roll' ] # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): self._horizon= 3 self._dices= [0, 0, 0] self._score= 0.0 def perceive(self, gameState): self._horizon= gameState.child(1).integer(1) self._dices= gameState.child(2).integers() self._score= gameState.child(2).value(1) def decide(self): return random.choice( self.actions() ) def sleep(self, result): print( \"> result: \"+ str(result) ) You can try your Bot AI by importing GameSolo into a launch script: #!python3 # Setup a game: from hacka.games.py421 import GameSolo as Game from bot421 import Bot bot= Bot() # Instanciate and start 100 games game= Game() results= game.testPlayer( bot, 10 ) # Analyze the result print( f\"Average score: {sum(results)/len(results)}\" ) The testPlayer method returns the results of bot player.","title":"Initialize a Bot"},{"location":"games/py421/#multi-player-version","text":"You can confront 2 Bot-AIs by importing GameDuo rather than GameSolo into your launch script: #!python3 # Setup a game: from hacka.games.py421 import GameDuo as Game from py421Bot import Bot as Bot1 from duo421Bot import Bot as Bot2 # Instanciate and start 100 games game= Game() bot1= Bot1() bot2= Bot2() results= game.launch( [bot1, bot2], 10 ) # Analyze the result print( f\"Average scores: \" ) for i in range(1, 3) : pres= results[i-1] print( f\" - Player-{i}: {sum(pres)/len(pres)}\" ) The launch method returns the results for all bots players. It supose you have a second Bot class in a new file duo421Bot.py .","title":"Multi-player version"},{"location":"games/py421/#optimize-a-2-player-bot","text":"A Bot designed for solo mode of 421 can also play to duo mode. However, the wakeUp methods informative and the perception method is incresed in duo mode. The player identifier of the wakeUp inform if the bot is the first or the second player. The gameState also include the conbinaison of your opponent: def perceive(self, gameState): self._horizon= gameState.child(1).integer(1) self._dices= gameState.child(2).integers() self._score= gameState.child(2).value(1) self._oppo_dices= gameState.child(3).integers() self._oppo_score= gameState.child(3).value(1) print( f'\\t> H: {self._horizon}, DICES: {self._dices}, SCORE: {self._score} (VS: {self._oppo_dices} {self._oppo_score})' ) Use this increased state definition to optimize the bot's strategies.","title":"Optimize a 2-player Bot :"},{"location":"games/py421/#server-mode","text":"It is possible to start 421 in duo mode. To try this version executes the following commad: python3 hackagames/play/py421 duo -n 2 That start a 2 games 4.2.1 where you have to confront the random player. The first game you play as the first player role. You control the number of roll-again. The opponent will not be capable to roll its dice more time than you. The second game, you play the second player role. You know the combinaision to beat.","title":"Server Mode"},{"location":"games/risky/","text":"Risky Risky is a HackaGames strategic turn-based game where two armies (or more) fights for a territory. Try the game: The play/risky.py script starts the game with an interactive interface in a shell playing against an artificial player ( firstBot.py ) playing randomly. python3 hackagames/play/risky.py The world is composed by interconnected nodes forming a tabletop as, for instance .' '. | | '. .3 / \\ .' '. .' '. | |-----| | '. .1 '. .2 \\ / .' '. | | '. .4 The 2 players are referenced as a player A and player B ; starting respectively in positions 1 and 2. When an army is on a node, the information is presented as below: .'A'. # Player ID |1- 12| # army action and force '. .4 # node ID In this example, an army of player A is on node 4 . This army has 1 action-point and is composed by 12 soldiers. Each army has 2 main attributes: its action counter (the number of action it can perform - max 2) its force (the size of the army - max 24) At its turn the player can make several actions (in the limit of action counters): Moving: move X Y FORCE to move FORCE units from cell X to cell Y Growing: grow X to grow the army on nodes X . The increase of the army is depending on the initial army size and the neighbors (the number of connected owned nodes). Sleeping: sleep to increase the action counter by one for all the armies. To notice that a moving action that will move an army toward an adversarial node trigger a fight. Iteratively, each force point of the attack and the defense has a chance to deal one damage. Defenses have an increased chance than attackers. However if the attack is greater than the defense than each extra point count double. The fight is running until one of the army is destroyed. For instance, with a move 1 2 10 with a defense of 8 on the node 2 , the fight will start by considering an attack force of 12 - \\(2 \\times (10-8)\\) - times 1 chance over 2 against a defense of 8 times 2 chances over 3. The exact amount of damages at the end of the fight remains uncertain. Futhermore, the game accepts also meta-actions: Defending: defends - this action triggers grow X into any available X positions, then sleep Fighting: fight X - this action searches for the maximal army to move on an adverserial X position. Exepending: expend X - this action distributes with move an army on X to all the empty connected cells. Initialize a Bot If you are implementing your first bot, please follow the first bot tutorial on Py421 game. The wakeUp method informs about the mode of the game and more importantly the tabletop structure, the perception of the status of each army on the tabletop. It is possible to manage a copy of Riscky game locally for helping decision-making. A minimal random Bot can be implemented in a few lines: from hacka.games.risky import GameRisky import random class Bot : # Player interface : def wakeUp(self, iPlayer, numberOfPlayers, gameConf): self._playerId= chr( ord(\"A\")+iPlayer-1 ) self._game= GameRisky().fromPod( gameConf ) def perceive(self, gameState): self._game.fromPod( gameState ) def decide(self): actions= self._game.searchMetaActions( self._playerId ) return random.choice( actions ) def sleep(self, result): pass And can be tested thanks to a very simple launcher script: #!python3 # Setup a game: from hacka.games.risky import GameRisky as Game from hacka.games.risky.firstBot import Bot as Opponent # Setup your bot: from randomBot import Bot bot= Bot() oppo= Opponent() # Instanciate and start 100 games game= Game() results= game.test2Players( bot, oppo, 100 ) # Analyze the result print( f\"Average score: {sum(results[0])/len(results[0])} vs {sum(results[1])/len(results[1])}\" ) At this point, the results comparing the 2 Bots would be very close. Customaize your AI: To customize your AI you can use the game engine copy (cf. risky.py ) Somme of the available methods: def fromPod( self, gameState ): # Update the board from the perception. def buildActionDescritors(self, playerId): # List all the current possible action descriptors from the configuration of the armies. A descriptor is a list ['actionName', nodeId, targetNodeId, maxForce]. targetNodeId and maxForce exist only for 'move' actions... def searchReadyActions(self, playerId): # List a limited set of ready to use actions. def searchMetaActions(self, playerId): # List the set of ready to use meta-actions. def cellIds(self): # return the list of cell identifiers def edgesFrom(self, iCell): # return the list of connected cell identifiers from the iCell cell. def armyOn(self, iCell) : # return an army as a Pod object, if an army is on the iCell cell (and False otherwise). def playerLetter(self, iPlayer): # return the player letter (A, B, C ...) of the ith player (1, 2, ...) def playerNum(self, playerId): # return the player number (1, 2, ...) from its letter (A, B, C ...). An army is a Pod object where the owner is recorded in the status and the 2 attributes is for action counter and force : for iCell in self._game.cellIds() : army= self._game.armyOn(iCell) # The army on the cell 1 if army : owner= army.status() action= army.integer(1) force= army.integer(2) print( f\"Army-{owner} ({action}, {force}) on {iCells}\" ) It is also possible to print the game state thanks to ViewerTerminal tool. from hacka.games.risky import ViewerTerminal ... view= ViewerTerminal( self._game ) view.print(self._playerId) The goal now is to compute that information in order to propose an AI winning your first random Bot. Differents maps: Actually Risky proposes severals maps, increasing gradually the number of cells, and so the complexity. The maps are: board-4 , board-6 , board-10 , board-12 , board-26 .","title":"Risky"},{"location":"games/risky/#risky","text":"Risky is a HackaGames strategic turn-based game where two armies (or more) fights for a territory.","title":"Risky"},{"location":"games/risky/#try-the-game","text":"The play/risky.py script starts the game with an interactive interface in a shell playing against an artificial player ( firstBot.py ) playing randomly. python3 hackagames/play/risky.py The world is composed by interconnected nodes forming a tabletop as, for instance .' '. | | '. .3 / \\ .' '. .' '. | |-----| | '. .1 '. .2 \\ / .' '. | | '. .4 The 2 players are referenced as a player A and player B ; starting respectively in positions 1 and 2. When an army is on a node, the information is presented as below: .'A'. # Player ID |1- 12| # army action and force '. .4 # node ID In this example, an army of player A is on node 4 . This army has 1 action-point and is composed by 12 soldiers. Each army has 2 main attributes: its action counter (the number of action it can perform - max 2) its force (the size of the army - max 24) At its turn the player can make several actions (in the limit of action counters): Moving: move X Y FORCE to move FORCE units from cell X to cell Y Growing: grow X to grow the army on nodes X . The increase of the army is depending on the initial army size and the neighbors (the number of connected owned nodes). Sleeping: sleep to increase the action counter by one for all the armies. To notice that a moving action that will move an army toward an adversarial node trigger a fight. Iteratively, each force point of the attack and the defense has a chance to deal one damage. Defenses have an increased chance than attackers. However if the attack is greater than the defense than each extra point count double. The fight is running until one of the army is destroyed. For instance, with a move 1 2 10 with a defense of 8 on the node 2 , the fight will start by considering an attack force of 12 - \\(2 \\times (10-8)\\) - times 1 chance over 2 against a defense of 8 times 2 chances over 3. The exact amount of damages at the end of the fight remains uncertain. Futhermore, the game accepts also meta-actions: Defending: defends - this action triggers grow X into any available X positions, then sleep Fighting: fight X - this action searches for the maximal army to move on an adverserial X position. Exepending: expend X - this action distributes with move an army on X to all the empty connected cells.","title":"Try the game:"},{"location":"games/risky/#initialize-a-bot","text":"If you are implementing your first bot, please follow the first bot tutorial on Py421 game. The wakeUp method informs about the mode of the game and more importantly the tabletop structure, the perception of the status of each army on the tabletop. It is possible to manage a copy of Riscky game locally for helping decision-making. A minimal random Bot can be implemented in a few lines: from hacka.games.risky import GameRisky import random class Bot : # Player interface : def wakeUp(self, iPlayer, numberOfPlayers, gameConf): self._playerId= chr( ord(\"A\")+iPlayer-1 ) self._game= GameRisky().fromPod( gameConf ) def perceive(self, gameState): self._game.fromPod( gameState ) def decide(self): actions= self._game.searchMetaActions( self._playerId ) return random.choice( actions ) def sleep(self, result): pass And can be tested thanks to a very simple launcher script: #!python3 # Setup a game: from hacka.games.risky import GameRisky as Game from hacka.games.risky.firstBot import Bot as Opponent # Setup your bot: from randomBot import Bot bot= Bot() oppo= Opponent() # Instanciate and start 100 games game= Game() results= game.test2Players( bot, oppo, 100 ) # Analyze the result print( f\"Average score: {sum(results[0])/len(results[0])} vs {sum(results[1])/len(results[1])}\" ) At this point, the results comparing the 2 Bots would be very close.","title":"Initialize a Bot"},{"location":"games/risky/#customaize-your-ai","text":"To customize your AI you can use the game engine copy (cf. risky.py ) Somme of the available methods: def fromPod( self, gameState ): # Update the board from the perception. def buildActionDescritors(self, playerId): # List all the current possible action descriptors from the configuration of the armies. A descriptor is a list ['actionName', nodeId, targetNodeId, maxForce]. targetNodeId and maxForce exist only for 'move' actions... def searchReadyActions(self, playerId): # List a limited set of ready to use actions. def searchMetaActions(self, playerId): # List the set of ready to use meta-actions. def cellIds(self): # return the list of cell identifiers def edgesFrom(self, iCell): # return the list of connected cell identifiers from the iCell cell. def armyOn(self, iCell) : # return an army as a Pod object, if an army is on the iCell cell (and False otherwise). def playerLetter(self, iPlayer): # return the player letter (A, B, C ...) of the ith player (1, 2, ...) def playerNum(self, playerId): # return the player number (1, 2, ...) from its letter (A, B, C ...). An army is a Pod object where the owner is recorded in the status and the 2 attributes is for action counter and force : for iCell in self._game.cellIds() : army= self._game.armyOn(iCell) # The army on the cell 1 if army : owner= army.status() action= army.integer(1) force= army.integer(2) print( f\"Army-{owner} ({action}, {force}) on {iCells}\" ) It is also possible to print the game state thanks to ViewerTerminal tool. from hacka.games.risky import ViewerTerminal ... view= ViewerTerminal( self._game ) view.print(self._playerId) The goal now is to compute that information in order to propose an AI winning your first random Bot.","title":"Customaize your AI:"},{"location":"games/risky/#differents-maps","text":"Actually Risky proposes severals maps, increasing gradually the number of cells, and so the complexity. The maps are: board-4 , board-6 , board-10 , board-12 , board-26 .","title":"Differents maps:"},{"location":"games/tictactoe/","text":"TicTacToe TicTacToe is a HackaGames game, a simple two-player game where each player tries to align trees of their pieces. It comes with two modes: classic and ultimate . Try the game: The play/tictactoe.py script starts the game with an interactive interface in a shell. python3 hackagames/play/tictactoe.py In ticTacToe , the players can perform one and only one action at its turn, and the game stops automatically with a winner or when no more pieces can be set on the tabletop. The actions consist of positioning a player's piece on the grid of the form: coordinateLetter-coordinateNumber . There are 3 times 3 actions in classic mode: A-1 , A-2 , A-3 , B-1 , B-2 , B-3 , C-1 , C-2 and C-3 . Example of grid at some point: x: A B C 1 x 2 o 3 o x The letter at the up-left corner informs about the player pieces ('x' or 'o'). The first player aligning 3 of its pieces win the game. Ultimate TicTacToe The ultimate mode is a hierarchical 2-levels TicTacToe . It is possible to activate the mode with a command parameter: python3 hackagames/play/tictactoe.py ultimate The grid is composed of 9 times 9 cells, so potentially 9 times 9 actions for the players: A-1 , A-2 , ... , A-9 , B-1 , ... , `I-9. In practice, most of the time, only a sub-number of actions are available. The particularity in hierarchical TicTacToe is that, the piece position taken by a player in the local grid indicates the next grid to play for the next player turn. Example of grid at some point: x: A B C D E F G H I 1 x | | 2 | | 3 o | | -------|-------|------- 4 | | 5 | | 6 | o | -------|-------|------- 7 | | 8 | x | 9 | | actions: D:F-7:9 The line actions: D:F-7:9 indicate that it is possible to play in any free position between D and F and 7 and 9 , so : D-7 , D-8 , D-9 , E-7 , E-9 , F-7 , F-8 or F-9 . At the beginning of the game, it is possible to play in a corner grid, a side grid or the center grid. That for the action line indicates: actions: A:C-1:3, A:C-4:6, D:F-4:6 The players have to win 3 aligned classic grids to win a Ultimate TicTacToe . Initialize a Bot If you are implementing your first bot, please follow the first bot tutorial on Py421 game. The wakeUp method informs about the mode of the game, the perception of the status of the grid and the possibilities of actions. The hackagames Tictactoe package includes a very useful Grid class to manipulate the game state. A minimal random Bot can be implemented in a few lines: from hacka.games.tictactoe.grid import Grid import random class Bot : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gamePod ): assert( gamePod.family() == 'TicTacToe') assert( gamePod.status() in ['Classic', 'Ultimate'] ) self._playerId= playerId self._grid= Grid( gamePod.status() ) self._possibilities= [1] def perceive(self, gameState): # Update the grid: self._grid.update( gameState.children()[:-1] ) self._possibilities= gameState.children()[-1].integers() def decide(self): # Get all actions actions= self._grid.possibleActions( self._possibilities ) # Select one return random.choice( actions ) def sleep(self, result): pass And can be tested thanks to a very simple launcher script: # Setup a game: from hacka.games.tictactoe import GameTTT as Game from hacka.games.tictactoe.firstBot import Bot as Opponent # Setup your bot: from myRandomBot import Bot bot= Bot() oppo= Opponent() # Instanciate and start 100 games game= Game(\"classic\") # can be set to \"ultimate\" results= game.test2Players( bot, oppo, 1000 ) # Analyze the result print( f\"Average score: {sum(results[0])/len(results[0])} vs {sum(results[1])/len(results[1])}\" ) At this point, the results comparing the 2 Bot would be very close. The 2 bots have the same behavior. To notice that, the list of actions do not filter positions already taken. However in case of wrong actions, the game will ask the player for a second call. Customize your Bot: The Grid object provide for two main methods: at(abs, ord) , returning a cell value (for instance grid.at('A', 2) \\(\\rightarrow\\) 0 if the cell is empty) and at_set(abs, ord, aValue) to set a specific value on a specific cell. From that point you can crate your own evaluation of the grid and which action is interesting. You can also import the tictactoe/engine class for a more complete toolbox (test actions, etc.)","title":"TicTacToe"},{"location":"games/tictactoe/#tictactoe","text":"TicTacToe is a HackaGames game, a simple two-player game where each player tries to align trees of their pieces. It comes with two modes: classic and ultimate .","title":"TicTacToe"},{"location":"games/tictactoe/#try-the-game","text":"The play/tictactoe.py script starts the game with an interactive interface in a shell. python3 hackagames/play/tictactoe.py In ticTacToe , the players can perform one and only one action at its turn, and the game stops automatically with a winner or when no more pieces can be set on the tabletop. The actions consist of positioning a player's piece on the grid of the form: coordinateLetter-coordinateNumber . There are 3 times 3 actions in classic mode: A-1 , A-2 , A-3 , B-1 , B-2 , B-3 , C-1 , C-2 and C-3 . Example of grid at some point: x: A B C 1 x 2 o 3 o x The letter at the up-left corner informs about the player pieces ('x' or 'o'). The first player aligning 3 of its pieces win the game.","title":"Try the game:"},{"location":"games/tictactoe/#ultimate-tictactoe","text":"The ultimate mode is a hierarchical 2-levels TicTacToe . It is possible to activate the mode with a command parameter: python3 hackagames/play/tictactoe.py ultimate The grid is composed of 9 times 9 cells, so potentially 9 times 9 actions for the players: A-1 , A-2 , ... , A-9 , B-1 , ... , `I-9. In practice, most of the time, only a sub-number of actions are available. The particularity in hierarchical TicTacToe is that, the piece position taken by a player in the local grid indicates the next grid to play for the next player turn. Example of grid at some point: x: A B C D E F G H I 1 x | | 2 | | 3 o | | -------|-------|------- 4 | | 5 | | 6 | o | -------|-------|------- 7 | | 8 | x | 9 | | actions: D:F-7:9 The line actions: D:F-7:9 indicate that it is possible to play in any free position between D and F and 7 and 9 , so : D-7 , D-8 , D-9 , E-7 , E-9 , F-7 , F-8 or F-9 . At the beginning of the game, it is possible to play in a corner grid, a side grid or the center grid. That for the action line indicates: actions: A:C-1:3, A:C-4:6, D:F-4:6 The players have to win 3 aligned classic grids to win a Ultimate TicTacToe .","title":"Ultimate TicTacToe"},{"location":"games/tictactoe/#initialize-a-bot","text":"If you are implementing your first bot, please follow the first bot tutorial on Py421 game. The wakeUp method informs about the mode of the game, the perception of the status of the grid and the possibilities of actions. The hackagames Tictactoe package includes a very useful Grid class to manipulate the game state. A minimal random Bot can be implemented in a few lines: from hacka.games.tictactoe.grid import Grid import random class Bot : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gamePod ): assert( gamePod.family() == 'TicTacToe') assert( gamePod.status() in ['Classic', 'Ultimate'] ) self._playerId= playerId self._grid= Grid( gamePod.status() ) self._possibilities= [1] def perceive(self, gameState): # Update the grid: self._grid.update( gameState.children()[:-1] ) self._possibilities= gameState.children()[-1].integers() def decide(self): # Get all actions actions= self._grid.possibleActions( self._possibilities ) # Select one return random.choice( actions ) def sleep(self, result): pass And can be tested thanks to a very simple launcher script: # Setup a game: from hacka.games.tictactoe import GameTTT as Game from hacka.games.tictactoe.firstBot import Bot as Opponent # Setup your bot: from myRandomBot import Bot bot= Bot() oppo= Opponent() # Instanciate and start 100 games game= Game(\"classic\") # can be set to \"ultimate\" results= game.test2Players( bot, oppo, 1000 ) # Analyze the result print( f\"Average score: {sum(results[0])/len(results[0])} vs {sum(results[1])/len(results[1])}\" ) At this point, the results comparing the 2 Bot would be very close. The 2 bots have the same behavior. To notice that, the list of actions do not filter positions already taken. However in case of wrong actions, the game will ask the player for a second call.","title":"Initialize a Bot"},{"location":"games/tictactoe/#customize-your-bot","text":"The Grid object provide for two main methods: at(abs, ord) , returning a cell value (for instance grid.at('A', 2) \\(\\rightarrow\\) 0 if the cell is empty) and at_set(abs, ord, aValue) to set a specific value on a specific cell. From that point you can crate your own evaluation of the grid and which action is interesting. You can also import the tictactoe/engine class for a more complete toolbox (test actions, etc.)","title":"Customize your Bot:"},{"location":"hello/04-protocol/","text":"","title":"04 protocol"},{"location":"hello/first-bot/","text":"First Bot In this tutorial, we will learn about how the games and the players/bots communicate together. In Hackagames several players can play simultaneously to games. That for, the loop control of the program is handled by the game. Then, the player bots have to generate the appropriate responses to situations. Initialize a Bot environment First, let implement a simple script, instantiating a game and a player. Put the next code into a script file py421-launch.py . from hackagames.py421 import GameMaster from py421Bot import FirstBot as Bot # Instanciate a 421 Game-Master (handle the game and the players): gameMaster= GameMaster(\"Solo\") # Instanciate your first bot: bot= Bot() # Start 1000 games with your bot: results= gameMaster.launchLocal( [bot], 100 ) # And analyze the result: botResults= results[0] print( f\"Average score: {sum(botResults)/len(botResults)}\" ) This script imports the py421 game and a bot , then it starts \\(100\\) games with that bot as main player. It returns the average endgame results of bot . At this point your script should not work. You need to create your FirstBot bot class in a py421Bot.py file aside of your py421-launch.py . Initialize your First Bot, i.e. an Autonomous Player A hackagames -formatted Bot is a class, implementing \\(4\\) specific methods: wakeUp , perceive , decide and sleep . wakeUp takes as parameters the initial configuration of a game, perceive the evolutive game state, and sleep the final result of the game. The method decide does not take any parameter, but it should return the bot action. The informations transiting between games and bots are formated as Pod (PieaceOfData), an hackaGames feature. So, the minimal Bot implementation in your py421Bot.py file, created aside of py421-launch.py should be: from hacka import Pod class FirstBot : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): pass def perceive(self, gameState): pass def decide(self): return Pod('keep-keep-keep') def sleep(self, result): pass Now the py421-launch.py should works. In a terminal: python3 ./py421-launch.py Understand Bot methods : Games and Bots rely on the hacka library, to exchange information about the game and the players' intentions (game configuration, game states and decisions). The players can be human interfaces or bots. Game Loop: A game mainly relies on perceive and decide capability for players and bots to update the information about the game and to ask for player action. The perceive method has \\(1\\) argument: the gameState , the current configuration of the game from the point of view of the player. In fact, gameState can be different from one player to another in multiplayer games with partial observation (typically card games). In our example (421), there is only one player with the full information about the game. So the basic configuration of Py421 ( Solo mode), the gameState informs about the $3$ dice values and the horizon counter (the number of remaining roll-again possibilities). The gameState read and associate to the bot instance self` is as follows: def perceive(self, gameState): self._horizon= gameState.child(1).integer(1) self._dices= gameState.child(2).integers() self._score= gameState.child(2).value(1) The decide method has no argument, but should return the player action. The action is formatted as a string. decide is always triggered after at least one perceive call. So, the bot instance self as all the required informations to process a decision. For instance, in Py421 , we can keep the \\(4.2.1\\) combination and roll the others. def decide( self ): stateString= f\"{self._horizon} - {self._dices} ({self._score})\" action= 'roll-roll-roll' if self._dices == [4, 2, 1] : action= \"keep-keep-keep\" print( stateString + \": \"+ action ) return Pod(action) As a result, this new decide method should statistically increase the obtained scores with py421-launch.py . Multiple Games : Furthermore, A GameMaster can start several games sequentially. Therefore, the methods wakeUp and sleep are triggered respectively when a new game is started and stopped. They are triggered 100 times in our script. During a game, players are activated at turn with the perceive and the decide methods. The wakeUp has \\(3\\) arguments: playerId , numberOfPlayers , gameConf informing in the place of the player in the game, the number of players and the initial configuration. At this stage, there is no information to get from playerId , numberOfPlayers and gameConf in Py421 , into Solo mode. Finally, the sleep informs about the obtained score with its argument result . The score is computed for the player regarding the ending game. In Py421 into Solo mode, the result matches the final combination value. You have to try to increase decisions to increase average scores after \\(1000\\) games. Pod - Hacka's Piece-Of-Data : The method arguments gameConf and gameState as the expected return of decide method are formatted as hacka.Pod (Piece Of Data), a tree data structure packing integers, values, and strings data. It is not required to understand Pod . Examples are always provided to get the game information from Pod . However, you can go to the Under the hood - Pod page for more detail on this core element of HackaGames .","title":"First Bot"},{"location":"hello/first-bot/#first-bot","text":"In this tutorial, we will learn about how the games and the players/bots communicate together. In Hackagames several players can play simultaneously to games. That for, the loop control of the program is handled by the game. Then, the player bots have to generate the appropriate responses to situations.","title":"First Bot"},{"location":"hello/first-bot/#initialize-a-bot-environment","text":"First, let implement a simple script, instantiating a game and a player. Put the next code into a script file py421-launch.py . from hackagames.py421 import GameMaster from py421Bot import FirstBot as Bot # Instanciate a 421 Game-Master (handle the game and the players): gameMaster= GameMaster(\"Solo\") # Instanciate your first bot: bot= Bot() # Start 1000 games with your bot: results= gameMaster.launchLocal( [bot], 100 ) # And analyze the result: botResults= results[0] print( f\"Average score: {sum(botResults)/len(botResults)}\" ) This script imports the py421 game and a bot , then it starts \\(100\\) games with that bot as main player. It returns the average endgame results of bot . At this point your script should not work. You need to create your FirstBot bot class in a py421Bot.py file aside of your py421-launch.py .","title":"Initialize a Bot environment"},{"location":"hello/first-bot/#initialize-your-first-bot-ie-an-autonomous-player","text":"A hackagames -formatted Bot is a class, implementing \\(4\\) specific methods: wakeUp , perceive , decide and sleep . wakeUp takes as parameters the initial configuration of a game, perceive the evolutive game state, and sleep the final result of the game. The method decide does not take any parameter, but it should return the bot action. The informations transiting between games and bots are formated as Pod (PieaceOfData), an hackaGames feature. So, the minimal Bot implementation in your py421Bot.py file, created aside of py421-launch.py should be: from hacka import Pod class FirstBot : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): pass def perceive(self, gameState): pass def decide(self): return Pod('keep-keep-keep') def sleep(self, result): pass Now the py421-launch.py should works. In a terminal: python3 ./py421-launch.py","title":"Initialize your First Bot, i.e. an Autonomous Player"},{"location":"hello/first-bot/#understand-bot-methods","text":"Games and Bots rely on the hacka library, to exchange information about the game and the players' intentions (game configuration, game states and decisions). The players can be human interfaces or bots.","title":"Understand Bot methods :"},{"location":"hello/first-bot/#game-loop","text":"A game mainly relies on perceive and decide capability for players and bots to update the information about the game and to ask for player action. The perceive method has \\(1\\) argument: the gameState , the current configuration of the game from the point of view of the player. In fact, gameState can be different from one player to another in multiplayer games with partial observation (typically card games). In our example (421), there is only one player with the full information about the game. So the basic configuration of Py421 ( Solo mode), the gameState informs about the $3$ dice values and the horizon counter (the number of remaining roll-again possibilities). The gameState read and associate to the bot instance self` is as follows: def perceive(self, gameState): self._horizon= gameState.child(1).integer(1) self._dices= gameState.child(2).integers() self._score= gameState.child(2).value(1) The decide method has no argument, but should return the player action. The action is formatted as a string. decide is always triggered after at least one perceive call. So, the bot instance self as all the required informations to process a decision. For instance, in Py421 , we can keep the \\(4.2.1\\) combination and roll the others. def decide( self ): stateString= f\"{self._horizon} - {self._dices} ({self._score})\" action= 'roll-roll-roll' if self._dices == [4, 2, 1] : action= \"keep-keep-keep\" print( stateString + \": \"+ action ) return Pod(action) As a result, this new decide method should statistically increase the obtained scores with py421-launch.py .","title":"Game Loop:"},{"location":"hello/first-bot/#multiple-games","text":"Furthermore, A GameMaster can start several games sequentially. Therefore, the methods wakeUp and sleep are triggered respectively when a new game is started and stopped. They are triggered 100 times in our script. During a game, players are activated at turn with the perceive and the decide methods. The wakeUp has \\(3\\) arguments: playerId , numberOfPlayers , gameConf informing in the place of the player in the game, the number of players and the initial configuration. At this stage, there is no information to get from playerId , numberOfPlayers and gameConf in Py421 , into Solo mode. Finally, the sleep informs about the obtained score with its argument result . The score is computed for the player regarding the ending game. In Py421 into Solo mode, the result matches the final combination value. You have to try to increase decisions to increase average scores after \\(1000\\) games.","title":"Multiple Games :"},{"location":"hello/first-bot/#pod-hackas-piece-of-data","text":"The method arguments gameConf and gameState as the expected return of decide method are formatted as hacka.Pod (Piece Of Data), a tree data structure packing integers, values, and strings data. It is not required to understand Pod . Examples are always provided to get the game information from Pod . However, you can go to the Under the hood - Pod page for more detail on this core element of HackaGames .","title":"Pod - Hacka's Piece-Of-Data :"},{"location":"hello/install/","text":"Install Process A quick tutorial to make HackaGames run on your computer. Nota bene : HackaGames is natively developed on Linux systems. Documentation is built regarding Ubuntu-like distributions. Commands are given in bash syntax. But most of the commands work pretty well on Windows PowerShell IDE (Integrated Development Environment), it is encouraged to use VisualStudio Code code.visualstudio.com and to create a project dedicated to HackaGames tutorials. Command should be work on a terminal open in the IDE. Getting Started (Python) A version restricted to Python resources can be easily installed thanks to the pip package manager: pip install hackagames From that point, for a first contact, you can play with the simplest proposed game: Py421 , from a shortcut: hacka-py421-play Alternatively - it is possible to edit and execute a short script launching a game: from hackagames.py421 import GameMaster from hacka.player import PlayerShell gameMaster= GameMaster(\"Solo\") gameMaster.launchLocal( [PlayerShell()], 1 ) The Py421 is a three-dice game. The player can roll several times to get the best combination. The actions consist of keeping or rolling each of the 3 dice. So there are 8 possible actions: keep-keep-keep , keep-keep-roll , keep-roll-keep , keep-roll-roll , roll-keep-keep , roll-keep-roll , roll-roll-keep and roll-roll-roll The goal is to optimize the combination of dice before the end of the second turn. The best combination ever is 4-2-1 . But you can explore other combinations. You can then follow the tutorial of First Bot to learn how to implement an autonomous player for Py421 the game. Installation from sources HackaGames is shared on github under an open-source license. It is possible to install it from sources. Simply clone the Git repository, then use Python pip on the local project directory. git clone https://github.com/ktorz-net/hackagames pip install ./hackagames or python -m pip install ./hackagames That's it; you have the last in-development version. Dependencies The project relies on several libs: hackapy itself is based on zmq for the network protocol of HackaGames tqdm offers nice but simple process-bars","title":"Install"},{"location":"hello/install/#install-process","text":"A quick tutorial to make HackaGames run on your computer. Nota bene : HackaGames is natively developed on Linux systems. Documentation is built regarding Ubuntu-like distributions. Commands are given in bash syntax. But most of the commands work pretty well on Windows PowerShell IDE (Integrated Development Environment), it is encouraged to use VisualStudio Code code.visualstudio.com and to create a project dedicated to HackaGames tutorials. Command should be work on a terminal open in the IDE.","title":"Install Process"},{"location":"hello/install/#getting-started-python","text":"A version restricted to Python resources can be easily installed thanks to the pip package manager: pip install hackagames From that point, for a first contact, you can play with the simplest proposed game: Py421 , from a shortcut: hacka-py421-play Alternatively - it is possible to edit and execute a short script launching a game: from hackagames.py421 import GameMaster from hacka.player import PlayerShell gameMaster= GameMaster(\"Solo\") gameMaster.launchLocal( [PlayerShell()], 1 ) The Py421 is a three-dice game. The player can roll several times to get the best combination. The actions consist of keeping or rolling each of the 3 dice. So there are 8 possible actions: keep-keep-keep , keep-keep-roll , keep-roll-keep , keep-roll-roll , roll-keep-keep , roll-keep-roll , roll-roll-keep and roll-roll-roll The goal is to optimize the combination of dice before the end of the second turn. The best combination ever is 4-2-1 . But you can explore other combinations. You can then follow the tutorial of First Bot to learn how to implement an autonomous player for Py421 the game.","title":"Getting Started (Python)"},{"location":"hello/install/#installation-from-sources","text":"HackaGames is shared on github under an open-source license. It is possible to install it from sources. Simply clone the Git repository, then use Python pip on the local project directory. git clone https://github.com/ktorz-net/hackagames pip install ./hackagames or python -m pip install ./hackagames That's it; you have the last in-development version.","title":"Installation from sources"},{"location":"hello/install/#dependencies","text":"The project relies on several libs: hackapy itself is based on zmq for the network protocol of HackaGames tqdm offers nice but simple process-bars","title":"Dependencies"},{"location":"hello/multi-player/","text":"Multi-Player in HackaGames Most of the games are multi-players games where Artificial Intelligences, embodied into Bot , fight. ATTENTION HACKAGAMES CHANGES MAKE THIS TUTORIAL BROKEN AS IT IS With a simple script One simple way to start a Python HackaGames game with several players is to instantiate a game and the players/Bots in a launcher script: # Setup a game: from hacka.games.aGame import Game from hacka.games.aGame.firstBot import Bot as Opponent from myBot import Bot bot, opponent1, opponent2= Bot(), Opponent(), Opponent() # Instanciate and start 100 games number= 100 game= Game() results=game.launch( [bot, opponent1, opponent2], number ) # Analyze the results for name, individualResults in zip( [\"bob\", \"natacha\", \"jad\"], results ) : print( f\"- {name},s average score: {sum(individualResults)/number}\" ) Human Interface The hacka.pylib package provides a very basic shell interface capable of playing any hackagames. from hacka.pylib.player import PlayerIHM as Interface Typically it is the one used by py421 game. However, systematically more tailored shell interface is provided. from hacka.games.aGames.shell import Interface Those interfaces can replace bot players into the launcher script. Example with Connect4: # Setup a game: from hacka.games.connect4 import GameConnect4 as Game from hacka.games.connect4.firstBot import Bot from hacka.games.connect4.shell import Interface # Instanciate and start 100 games number= 4 game= Game() results=game.launch( [Bot(), Interface()], number ) # Analyze the results for name, individualResults in zip( [\"bob\", \"me\"], results ) : print( f\"- {name},s average score: {sum(individualResults)/number}\" ) Client-Server Architecture HackaGames is designed to run as a client-server architecture. The game run as a server, and players (Bots or Human Interfaces) as clients. The game wait for the appropriate number of players before to start. For instance, on three different shells: # shell-1 python3 -m hacka.games.tictactoe.serve ultimate -n 2 # shell-2 python3 -m hacka.games.tictactoe.shell # shell-3 python3 -m hacka.games.tictactoe.firstBot Each shell process one entity of the game, coordination is provided with hackagame protocol. The shells can be distributed over different machine. To connect a player to a specific hackagames server, the player need to inherite from hacka.pylib abstract player ( AbsPlayer ) and take a seat on a game.... from ... import pylib as hk def main() : player= MyBot() player.takeASeat( \"xxx.local\" ) class MyBot( hk.AbsPlayer ) : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gamePod ): ... def perceive(self, gameState): ... def decide(self): ... #def sleep(self, result): ... # Script : if __name__ == '__main__' : main() ...","title":"Multiplayer"},{"location":"hello/multi-player/#multi-player-in-hackagames","text":"Most of the games are multi-players games where Artificial Intelligences, embodied into Bot , fight. ATTENTION HACKAGAMES CHANGES MAKE THIS TUTORIAL BROKEN AS IT IS","title":"Multi-Player in HackaGames"},{"location":"hello/multi-player/#with-a-simple-script","text":"One simple way to start a Python HackaGames game with several players is to instantiate a game and the players/Bots in a launcher script: # Setup a game: from hacka.games.aGame import Game from hacka.games.aGame.firstBot import Bot as Opponent from myBot import Bot bot, opponent1, opponent2= Bot(), Opponent(), Opponent() # Instanciate and start 100 games number= 100 game= Game() results=game.launch( [bot, opponent1, opponent2], number ) # Analyze the results for name, individualResults in zip( [\"bob\", \"natacha\", \"jad\"], results ) : print( f\"- {name},s average score: {sum(individualResults)/number}\" )","title":"With a simple script"},{"location":"hello/multi-player/#human-interface","text":"The hacka.pylib package provides a very basic shell interface capable of playing any hackagames. from hacka.pylib.player import PlayerIHM as Interface Typically it is the one used by py421 game. However, systematically more tailored shell interface is provided. from hacka.games.aGames.shell import Interface Those interfaces can replace bot players into the launcher script. Example with Connect4: # Setup a game: from hacka.games.connect4 import GameConnect4 as Game from hacka.games.connect4.firstBot import Bot from hacka.games.connect4.shell import Interface # Instanciate and start 100 games number= 4 game= Game() results=game.launch( [Bot(), Interface()], number ) # Analyze the results for name, individualResults in zip( [\"bob\", \"me\"], results ) : print( f\"- {name},s average score: {sum(individualResults)/number}\" )","title":"Human Interface"},{"location":"hello/multi-player/#client-server-architecture","text":"HackaGames is designed to run as a client-server architecture. The game run as a server, and players (Bots or Human Interfaces) as clients. The game wait for the appropriate number of players before to start. For instance, on three different shells: # shell-1 python3 -m hacka.games.tictactoe.serve ultimate -n 2 # shell-2 python3 -m hacka.games.tictactoe.shell # shell-3 python3 -m hacka.games.tictactoe.firstBot Each shell process one entity of the game, coordination is provided with hackagame protocol. The shells can be distributed over different machine. To connect a player to a specific hackagames server, the player need to inherite from hacka.pylib abstract player ( AbsPlayer ) and take a seat on a game.... from ... import pylib as hk def main() : player= MyBot() player.takeASeat( \"xxx.local\" ) class MyBot( hk.AbsPlayer ) : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gamePod ): ... def perceive(self, gameState): ... def decide(self): ... #def sleep(self, result): ... # Script : if __name__ == '__main__' : main() ...","title":"Client-Server Architecture"},{"location":"hood/api/","text":"Communication API The idea is to use a client/server architecture to permit any code in any language to seat on a game (i.e., connect to act as a player). For that we choose to use a high-level messaging library. High-Level Message Queuing API We do not require to address directly the TCP protocol while our need for network usage is very classical. A game process acts as a server, accepts and manages client-player processes. Then the game and players exchange game states and action description. For that purpose tree solution was considered: MQTT - Mosquitto , ZeroMQ and ROS2 Mosquitto is a light implementation of the MQTT standard. MQTT defines protocols for process interconnection. a Broker server serves as a central point to connect all the other nodes. Topics are defined as pipes of data flow. Any process can subscribe (i.e. reads) or publish (i.e. write) in the topics. Wikipedia is a good entrance point for more details. ZeroMQ follows its own protocols and propose a high-level interface to interprocess communication via difference communication architecture and different low-level protocols including in-procecus communication. ROS2 is developed for robotic purpose. It is very similar to MQTT in its communication architecture. However, in ROS , messages in topics are typed and several types and algorithms are already developed to exchange, manipulate and visualize geometrical and temporal data. Also in ROS2 the broker process is removed. For HackaGames use, ZeroMQ present the best compromise. It requires more implementation compared to MQTT - Mosquitto to permit processes to communicate but it allows the development of dedicated game/players communication protocols. This way, there is no need for a broker, and the game server can better address specific information to identified players. ROS2 solution was removed from the candidates mostly because of the huge dependencies relying on this solution. Using ROS2 induces to install and use a heavy API for developing and for managing communicating processes. One of the core advantages of ZeroMQ resides in the fact that it is an open-source solution developed in C and so on, easily integrable in any other languages. Appropriation of ZeroMQ The philosophy is to make games and players interoperable whatever the technology used to develop them. That for HackaGames proposes 2 independent implementation of its core features. One in Python ( hackapy ) and one in C ( hackalib - to come). In hackapy , interoperable API is developed in the (interprocess.py)[https://bitbucket.org/imt-mobisyst/hackagames/src/master/hackapy/interprocess.py] file. The implementation includes two main classes: Dealer and Client . The Dealer aims to represent a game. It manages Client connection each one labeled as an identified player. A tweak class Local permits a game and players to run in the same process, if all of them are implemented with Python. This way, implemented games and players are agnostic from the usage or not of interprocess architecture via ZeroMQ . Those elements can be running without any difference with a Dealer and Clients or with Local manager.","title":"Protocol API"},{"location":"hood/api/#communication-api","text":"The idea is to use a client/server architecture to permit any code in any language to seat on a game (i.e., connect to act as a player). For that we choose to use a high-level messaging library.","title":"Communication API"},{"location":"hood/api/#high-level-message-queuing-api","text":"We do not require to address directly the TCP protocol while our need for network usage is very classical. A game process acts as a server, accepts and manages client-player processes. Then the game and players exchange game states and action description. For that purpose tree solution was considered: MQTT - Mosquitto , ZeroMQ and ROS2 Mosquitto is a light implementation of the MQTT standard. MQTT defines protocols for process interconnection. a Broker server serves as a central point to connect all the other nodes. Topics are defined as pipes of data flow. Any process can subscribe (i.e. reads) or publish (i.e. write) in the topics. Wikipedia is a good entrance point for more details. ZeroMQ follows its own protocols and propose a high-level interface to interprocess communication via difference communication architecture and different low-level protocols including in-procecus communication. ROS2 is developed for robotic purpose. It is very similar to MQTT in its communication architecture. However, in ROS , messages in topics are typed and several types and algorithms are already developed to exchange, manipulate and visualize geometrical and temporal data. Also in ROS2 the broker process is removed. For HackaGames use, ZeroMQ present the best compromise. It requires more implementation compared to MQTT - Mosquitto to permit processes to communicate but it allows the development of dedicated game/players communication protocols. This way, there is no need for a broker, and the game server can better address specific information to identified players. ROS2 solution was removed from the candidates mostly because of the huge dependencies relying on this solution. Using ROS2 induces to install and use a heavy API for developing and for managing communicating processes. One of the core advantages of ZeroMQ resides in the fact that it is an open-source solution developed in C and so on, easily integrable in any other languages.","title":"High-Level Message Queuing API"},{"location":"hood/api/#appropriation-of-zeromq","text":"The philosophy is to make games and players interoperable whatever the technology used to develop them. That for HackaGames proposes 2 independent implementation of its core features. One in Python ( hackapy ) and one in C ( hackalib - to come). In hackapy , interoperable API is developed in the (interprocess.py)[https://bitbucket.org/imt-mobisyst/hackagames/src/master/hackapy/interprocess.py] file. The implementation includes two main classes: Dealer and Client . The Dealer aims to represent a game. It manages Client connection each one labeled as an identified player. A tweak class Local permits a game and players to run in the same process, if all of them are implemented with Python. This way, implemented games and players are agnostic from the usage or not of interprocess architecture via ZeroMQ . Those elements can be running without any difference with a Dealer and Clients or with Local manager.","title":"Appropriation of ZeroMQ"},{"location":"hood/newgame/","text":"Game Creation in Python The ideas here is to present step by step the game creation in python with hackapy . Reminder, hackapy is the HackaGames python librairy helping for the game and players to communicate together. Directory structure In your workspace directory create a subdirectory gameXyz where Xyz identify your new game ( gameHello for instance). This new subdirectory (your working directory) will also include another subdirectory gameEngine regrouping the source code making your game working. Directory squeletom: gameHello # your game folder - gameEngine # sourcecode of the game Then we start with 3 files: README.md : a Markdown readme first file presenting the game and the rules. gameEngine/__init__.py : a classical python files marking the entrance of your gameEngine package. start-server : a start script, lauching the game server. Game Engine At minima gameEngine python package include a Game class deriving from hackapy.AbsGame in the __init__.py file. It is suppozed that the new Game class implement the abstract methods of AbsGame : class AbsGame(): # Game interface : def initialize(self): # Initialize a new game # Return the game configuration (as a PodInterface) # the returned Pod is given to player's wake-up method. pass def playerHand( self, iPlayer ): # Return the game elements in the player vision (a PodInterface) # the returned pod feed the player's perception method. pass def applyPlayerAction( self, iPlayer, action ): # Apply the action choosen by the player iPlayer. # Return a boolean at True if the player terminate its actions for the current turn. # False means player need to be activated again before step to another player or another game turn. pass def tic( self ): # called function at turn end, after all player played its actions. pass def isEnded( self ): # must return True when the game end, and False else. pass def playerScore( self, iPlayer ): # return the player score for the current game (usefull at game ending) pass A last abstract method is defined: play . This method is more an inside method re-defined in function of how the players are managed. Actually, two strategies are proposed: AbsSequentialGame: Players are activated at turns. The perception of the game for the \\(i\\) -th player and a call to decide are sent after the \\((i-1)\\) -th player terminates its actions ( applyPlayerAction( (i-1), \"action\" ) returns True ). AbsSimultaneousGame: (Work In Progress) Players are activated in a simultaneous way. All the players receive their perception of the game then all the players are requested for their actions. Generally action resolution in these kinds of games are resolved in the tic method. Example of of the simple hello games: First, initialize python file and import the hackapy package. #!env python3 \"\"\" HackaGame - Game - Hello \"\"\" import sys sys.path.insert( 1, __file__.split('gameHello')[0] ) import hackapy as hg Then, the Game implement an abstract sequential game (each player play at turns). The Game methods to implemented are: initialize , playerHand , applyPlayerAction , isEnded and playerScore . Here the Hello game simply echo the player action in a terminal \\(3\\) times (method applyPlayerAction ). class GameHello( hg.AbsSequentialGame ) : # Game interface : def initialize(self): # initialize the counter and only say hello. self.counter= 0 return hg.Pod( 'hello' ) def playerHand( self, iPlayer ): # ping with the increasing counter return hg.Pod( 'hi', flags=[ self.counter ] ) def applyPlayerAction( self, iPlayer, action ): # print the receive action message. And that all. print( f\"Player-{iPlayer} say < {action} >\" ) return True def tic( self ): # step on the counter. self.counter= min( self.counter+1, 3 ) def isEnded( self ): # if the counter reach it final value return self.counter == 3 def playerScore( self, iPlayer ): # All players are winners. return 1 A counter initialized in initialize method, count \\(3\\) game turn (i.e. after each player play at-turn) in tic method. Then the isEnded method will return True . The method playerHand informs the player about the counter status. Finaly, there is no winner and all player will end with a result at \\(1\\) (method playerScore ). Lets play The start-server script will permit to lauch the game server. It only instancate a Game with a determined number of players then call the AbsGame start method. #!env python3 \"\"\" HackaGame - Game - Hello \"\"\" from gameEngine import GameHello game= GameHello() game.start() That it. You can set your script executable ( chmod +x ./gameHello/start-server ) and play with your new game: # In a first shell: ./gameHello/start-server # In a second shell: ./hackagames/connect-shell Game initialization: AbsGame initialization need a number of players. By default the value is on 1 . However, if you want to fix this number or if you want to add extrat initialization, you need to call the parent initialization in your own. As reminder in Python , initialization method is named __init__ and super() function provides an access to parent methods. For instance, for a 2 player game: class MyGame( hg.AbsSequentialGame ) : # Initialization: def __init__(self) : super().__init__( numberOfPlayers=2 ) self._myAttribut= \"Some initializations\" Going futher: Command Interpreter: from hackapy.command import Command, Option # Define a command interpreter: 2 options: host address and port: cmd= Command( \"start-server\", [ Option( \"port\", \"p\", default=1400 ), Option( \"number\", \"n\", 2, \"number of games\" ) ], ( \"star a server fo gameConnect4 on your machine. \" \"gameConnect4 do not take ARGUMENT.\" )) # Process the command line: cmd.process() if not cmd.ready() : print( cmd.help() ) exit() ... game.start( cmd.option(\"number\"), cmd.option(\"port\") ) Going futher: Test-Driven: You can start with a first : test_01_AbsGame script in a test directory. to verify the call to HackaGames Games methods... \"\"\" Test - Hello Games Class \"\"\" import sys sys.path.insert( 1, __file__.split('gameHello')[0] ) import hackapy as hg import gameHello.gameEngine as ge def test_gameMethod(): game= ge.GameHello() assert( type( game.initialize().asPod() ) is hg.Pod ) assert( type( game.playerHand(1).asPod() ) is hg.Pod ) assert( game.applyPlayerAction( 1, \"sleep\" ) ) game.tic() assert( not game.isEnded() ) assert( game.playerScore(1) == 1 ) ...","title":"New Game"},{"location":"hood/newgame/#game-creation-in-python","text":"The ideas here is to present step by step the game creation in python with hackapy . Reminder, hackapy is the HackaGames python librairy helping for the game and players to communicate together.","title":"Game Creation in Python"},{"location":"hood/newgame/#directory-structure","text":"In your workspace directory create a subdirectory gameXyz where Xyz identify your new game ( gameHello for instance). This new subdirectory (your working directory) will also include another subdirectory gameEngine regrouping the source code making your game working. Directory squeletom: gameHello # your game folder - gameEngine # sourcecode of the game Then we start with 3 files: README.md : a Markdown readme first file presenting the game and the rules. gameEngine/__init__.py : a classical python files marking the entrance of your gameEngine package. start-server : a start script, lauching the game server.","title":"Directory structure"},{"location":"hood/newgame/#game-engine","text":"At minima gameEngine python package include a Game class deriving from hackapy.AbsGame in the __init__.py file. It is suppozed that the new Game class implement the abstract methods of AbsGame : class AbsGame(): # Game interface : def initialize(self): # Initialize a new game # Return the game configuration (as a PodInterface) # the returned Pod is given to player's wake-up method. pass def playerHand( self, iPlayer ): # Return the game elements in the player vision (a PodInterface) # the returned pod feed the player's perception method. pass def applyPlayerAction( self, iPlayer, action ): # Apply the action choosen by the player iPlayer. # Return a boolean at True if the player terminate its actions for the current turn. # False means player need to be activated again before step to another player or another game turn. pass def tic( self ): # called function at turn end, after all player played its actions. pass def isEnded( self ): # must return True when the game end, and False else. pass def playerScore( self, iPlayer ): # return the player score for the current game (usefull at game ending) pass A last abstract method is defined: play . This method is more an inside method re-defined in function of how the players are managed. Actually, two strategies are proposed: AbsSequentialGame: Players are activated at turns. The perception of the game for the \\(i\\) -th player and a call to decide are sent after the \\((i-1)\\) -th player terminates its actions ( applyPlayerAction( (i-1), \"action\" ) returns True ). AbsSimultaneousGame: (Work In Progress) Players are activated in a simultaneous way. All the players receive their perception of the game then all the players are requested for their actions. Generally action resolution in these kinds of games are resolved in the tic method.","title":"Game Engine"},{"location":"hood/newgame/#example-of-of-the-simple-hello-games","text":"First, initialize python file and import the hackapy package. #!env python3 \"\"\" HackaGame - Game - Hello \"\"\" import sys sys.path.insert( 1, __file__.split('gameHello')[0] ) import hackapy as hg Then, the Game implement an abstract sequential game (each player play at turns). The Game methods to implemented are: initialize , playerHand , applyPlayerAction , isEnded and playerScore . Here the Hello game simply echo the player action in a terminal \\(3\\) times (method applyPlayerAction ). class GameHello( hg.AbsSequentialGame ) : # Game interface : def initialize(self): # initialize the counter and only say hello. self.counter= 0 return hg.Pod( 'hello' ) def playerHand( self, iPlayer ): # ping with the increasing counter return hg.Pod( 'hi', flags=[ self.counter ] ) def applyPlayerAction( self, iPlayer, action ): # print the receive action message. And that all. print( f\"Player-{iPlayer} say < {action} >\" ) return True def tic( self ): # step on the counter. self.counter= min( self.counter+1, 3 ) def isEnded( self ): # if the counter reach it final value return self.counter == 3 def playerScore( self, iPlayer ): # All players are winners. return 1 A counter initialized in initialize method, count \\(3\\) game turn (i.e. after each player play at-turn) in tic method. Then the isEnded method will return True . The method playerHand informs the player about the counter status. Finaly, there is no winner and all player will end with a result at \\(1\\) (method playerScore ).","title":"Example of of the simple hello games:"},{"location":"hood/newgame/#lets-play","text":"The start-server script will permit to lauch the game server. It only instancate a Game with a determined number of players then call the AbsGame start method. #!env python3 \"\"\" HackaGame - Game - Hello \"\"\" from gameEngine import GameHello game= GameHello() game.start() That it. You can set your script executable ( chmod +x ./gameHello/start-server ) and play with your new game: # In a first shell: ./gameHello/start-server # In a second shell: ./hackagames/connect-shell","title":"Lets play"},{"location":"hood/newgame/#game-initialization","text":"AbsGame initialization need a number of players. By default the value is on 1 . However, if you want to fix this number or if you want to add extrat initialization, you need to call the parent initialization in your own. As reminder in Python , initialization method is named __init__ and super() function provides an access to parent methods. For instance, for a 2 player game: class MyGame( hg.AbsSequentialGame ) : # Initialization: def __init__(self) : super().__init__( numberOfPlayers=2 ) self._myAttribut= \"Some initializations\"","title":"Game initialization:"},{"location":"hood/newgame/#going-futher-command-interpreter","text":"from hackapy.command import Command, Option # Define a command interpreter: 2 options: host address and port: cmd= Command( \"start-server\", [ Option( \"port\", \"p\", default=1400 ), Option( \"number\", \"n\", 2, \"number of games\" ) ], ( \"star a server fo gameConnect4 on your machine. \" \"gameConnect4 do not take ARGUMENT.\" )) # Process the command line: cmd.process() if not cmd.ready() : print( cmd.help() ) exit() ... game.start( cmd.option(\"number\"), cmd.option(\"port\") )","title":"Going futher: Command Interpreter:"},{"location":"hood/newgame/#going-futher-test-driven","text":"You can start with a first : test_01_AbsGame script in a test directory. to verify the call to HackaGames Games methods... \"\"\" Test - Hello Games Class \"\"\" import sys sys.path.insert( 1, __file__.split('gameHello')[0] ) import hackapy as hg import gameHello.gameEngine as ge def test_gameMethod(): game= ge.GameHello() assert( type( game.initialize().asPod() ) is hg.Pod ) assert( type( game.playerHand(1).asPod() ) is hg.Pod ) assert( game.applyPlayerAction( 1, \"sleep\" ) ) game.tic() assert( not game.isEnded() ) assert( game.playerScore(1) == 1 ) ...","title":"Going futher: Test-Driven:"},{"location":"hood/pod/","text":"Piece-Of-Data - the core Class In the interoperable process between a game and players, information transits. Games send what players can perceive and players send action description in return. That information is structured as Pod (Piece-Of-Data) to be serialized, transmitted and rebuilt. a Pod includes raw data (sequence of characters, intergers, floating points values) and cand be a part of a tree structure of information. Implementation: hackapy: pod.py The example here are presented accordingly to the Python implementation (hackapy). Pod Components Pod is composed of 5 elements of fixed type: family: an string permitting to identifiate the type Pod . It is generally used as a key to make the other elements interpretable. status: an string describing the individual in the family, its current state. flags: a vector of integer values. values: a vector of floating points values. children: a vector of other Pod elements, sub-part of the Pod. Accessor methods permit to get string elements as list : aPod.family() , aPod.status() and list elements: aPod.integers() , aPod.values() , aPod.children() . It is also possible to get a specific element is the lists with: aPod.integer(anInteger) , aPod.value(anInteger) , aPod.child(anInteger) . To notice that, in HackaGames conventions element in list are indexed starting from 1 . The first element is accessed with index 1 : ( aPod.integers(1) for instance). Tree Structure Children componnents of a Pod permits to defines complexe information structured as trees. For more information about tree data structure, Wikipedia is a good entrance point. Trees are very famous in Web technoligies for instance through HTML , XML , Json , Yaml etc formats. In HackaGames most of the game states perceived by players (one Pod ) are composed by several elements each one modeled by child Pod. For instance, a Board would be composed with Cells and Cells would welcome Pieces . The pieces would be the childrens of a Cell , Cells the children of a Board etc. Interface versus Inheritance HackaGames conventions prefer to separate game elements in the implementations from Pod. A Pod imposes specific structuration of elements (flags, values, ...). This structuration is not always efficient in the game mechanisms and not even readable in its implementations. That for, we discourage developers from using inheritance of Pod in game and player implementations and we propose interface mechanisms to generate pods from the game elements and vice versa, when it is pertinent to do it. Game elements should include Pod interface methods: # Pod interface: def asPod(self, family=\"Pod\"): # Should return a Pod describing self. # ... pass def fromPod(self, aPod): # Should regenerate self form a pod description # HackaGames conventions aim to return self at the end of the method. # ... pass","title":"Pod Class"},{"location":"hood/pod/#piece-of-data-the-core-class","text":"In the interoperable process between a game and players, information transits. Games send what players can perceive and players send action description in return. That information is structured as Pod (Piece-Of-Data) to be serialized, transmitted and rebuilt. a Pod includes raw data (sequence of characters, intergers, floating points values) and cand be a part of a tree structure of information. Implementation: hackapy: pod.py The example here are presented accordingly to the Python implementation (hackapy).","title":"Piece-Of-Data - the core Class"},{"location":"hood/pod/#pod-components","text":"Pod is composed of 5 elements of fixed type: family: an string permitting to identifiate the type Pod . It is generally used as a key to make the other elements interpretable. status: an string describing the individual in the family, its current state. flags: a vector of integer values. values: a vector of floating points values. children: a vector of other Pod elements, sub-part of the Pod. Accessor methods permit to get string elements as list : aPod.family() , aPod.status() and list elements: aPod.integers() , aPod.values() , aPod.children() . It is also possible to get a specific element is the lists with: aPod.integer(anInteger) , aPod.value(anInteger) , aPod.child(anInteger) . To notice that, in HackaGames conventions element in list are indexed starting from 1 . The first element is accessed with index 1 : ( aPod.integers(1) for instance).","title":"Pod Components"},{"location":"hood/pod/#tree-structure","text":"Children componnents of a Pod permits to defines complexe information structured as trees. For more information about tree data structure, Wikipedia is a good entrance point. Trees are very famous in Web technoligies for instance through HTML , XML , Json , Yaml etc formats. In HackaGames most of the game states perceived by players (one Pod ) are composed by several elements each one modeled by child Pod. For instance, a Board would be composed with Cells and Cells would welcome Pieces . The pieces would be the childrens of a Cell , Cells the children of a Board etc.","title":"Tree Structure"},{"location":"hood/pod/#interface-versus-inheritance","text":"HackaGames conventions prefer to separate game elements in the implementations from Pod. A Pod imposes specific structuration of elements (flags, values, ...). This structuration is not always efficient in the game mechanisms and not even readable in its implementations. That for, we discourage developers from using inheritance of Pod in game and player implementations and we propose interface mechanisms to generate pods from the game elements and vice versa, when it is pertinent to do it. Game elements should include Pod interface methods: # Pod interface: def asPod(self, family=\"Pod\"): # Should return a Pod describing self. # ... pass def fromPod(self, aPod): # Should regenerate self form a pod description # HackaGames conventions aim to return self at the end of the method. # ... pass","title":"Interface versus Inheritance"},{"location":"hood/testdriven/","text":"Test Driven Development Test Driven Developement (TDD) consist of defining test that would validate a desired functionality before to develop the functionality itself. After development, the test allows to validate that the functionality works but also that functionally matches the initial expectation. Wikipedia is a good entrance point for going further in the concepts of TDD. One of the simplest ways to develop tests in Python is to use pytest . Pytest tool: A test script has file-name starting with test_ . Optionally it can be regrouped in a test directory. It is composed of test function starting with def test_ defining assert based test. For instance test_pytest.py : def test_test(): assert( True ) Then, Pytest can be used to run the tests: # install pip install pytest # execute all your tests: pytest Test your players: Test your games: A minimal test bench built considering the gameHello tutorial: \"\"\" Test - hello.Engine \"\"\" import sys sys.path.insert( 1, __file__.split('hackagames')[0] ) import hackagames.hackapy as hg import gameHello.gameEngine as ge def test_gameMethod(): game= ge.GameConnect4() assert( type( game.initialize().asPod() ) is hg.Pod ) assert( type( game.playerHand(1).asPod() ) is hg.Pod ) game.applyPlayerAction( 1, \"test\" ) game.tic() assert( not game.isEnded() ) assert( game.playerScore(1) == 0 ) def test_initialize(): game= ge.GameConnect4() wakeUpPod= game.initialize().asPod() assert( str(wakeUpPod) == \"hello:\" ) assert( wakeUpPod.family() == \"hello\" ) assert( wakeUpPod.status() == \"\" ) assert( wakeUpPod.integers() == [] ) assert( wakeUpPod.values() == [] ) assert( len( wakeUpPod.children() ) == 0 ) def test_playerHand(): game= ge.GameConnect4() game.initialize().asPod() handPod= game.playerHand(1).asPod() assert( str(handPod) == 'hi: [0]' ) assert( handPod.family() == \"hi\" ) assert( handPod.status() == \"\" ) assert( handPod.integers() == [0] ) assert( handPod.values() == [] ) assert( len( handPod.children() ) == 0 ) Then you have to adapt the test while you implement your game.","title":"Test-Driven"},{"location":"hood/testdriven/#test-driven-development","text":"Test Driven Developement (TDD) consist of defining test that would validate a desired functionality before to develop the functionality itself. After development, the test allows to validate that the functionality works but also that functionally matches the initial expectation. Wikipedia is a good entrance point for going further in the concepts of TDD. One of the simplest ways to develop tests in Python is to use pytest .","title":"Test Driven Development"},{"location":"hood/testdriven/#pytest-tool","text":"A test script has file-name starting with test_ . Optionally it can be regrouped in a test directory. It is composed of test function starting with def test_ defining assert based test. For instance test_pytest.py : def test_test(): assert( True ) Then, Pytest can be used to run the tests: # install pip install pytest # execute all your tests: pytest","title":"Pytest tool:"},{"location":"hood/testdriven/#test-your-players","text":"","title":"Test your players:"},{"location":"hood/testdriven/#test-your-games","text":"A minimal test bench built considering the gameHello tutorial: \"\"\" Test - hello.Engine \"\"\" import sys sys.path.insert( 1, __file__.split('hackagames')[0] ) import hackagames.hackapy as hg import gameHello.gameEngine as ge def test_gameMethod(): game= ge.GameConnect4() assert( type( game.initialize().asPod() ) is hg.Pod ) assert( type( game.playerHand(1).asPod() ) is hg.Pod ) game.applyPlayerAction( 1, \"test\" ) game.tic() assert( not game.isEnded() ) assert( game.playerScore(1) == 0 ) def test_initialize(): game= ge.GameConnect4() wakeUpPod= game.initialize().asPod() assert( str(wakeUpPod) == \"hello:\" ) assert( wakeUpPod.family() == \"hello\" ) assert( wakeUpPod.status() == \"\" ) assert( wakeUpPod.integers() == [] ) assert( wakeUpPod.values() == [] ) assert( len( wakeUpPod.children() ) == 0 ) def test_playerHand(): game= ge.GameConnect4() game.initialize().asPod() handPod= game.playerHand(1).asPod() assert( str(handPod) == 'hi: [0]' ) assert( handPod.family() == \"hi\" ) assert( handPod.status() == \"\" ) assert( handPod.integers() == [0] ) assert( handPod.values() == [] ) assert( len( handPod.children() ) == 0 ) Then you have to adapt the test while you implement your game.","title":"Test your games:"},{"location":"learning/decision-tree/","text":"Using Decision Tree \"A decision tree is a decision support recursive partitioning structure\" Wikipedia . It can be used to partitioning states, i.e. to groups similar states together into super-states. Grouped states should be close in terms of behavior: expected action, value and transition. If such a structure can be built over the state space, then it is possible to exploit it into reinforcement learning. Simple version With Py421 Py421 advantages the combinations 4-2-1 , 1-1-1 or X-1-1 . A first idea could be to focus state definition to target those combinations. This idea modeled as a tree will lookalike: This tree can be implemented with a \"if-then-else\" procedure into a QLearner Bot, to learn a good behavior on Py421 game only over 7 states. As a result, learning should be very fast but with a final behavior less performant than learning applied over all the 168 game states. Expert state definition for Risky The goal is to apply state factorization/clustering to a game where the number of states and actions prevents from using QLearning over all the states. That for, the exercise consists of applying QLearning to risky . 1. As a first, action you should get familiar with the game mechanisms: tuto risky 2. Then, you have to trace (ie. print) a complete definition of crossed states. QUESTION: How many states describe risky configuration (considering the default BOARD-4) ? 3. Finally, try to learn a policy using QLearning over meta actions. QUESTION: Which action has the worst branching number ? A simple score over the game can be computed as the difference between armies total size. This score feeds appropriately the reward computation. def totalArmies(self) : myArmy= 0 opponentArmy= 0 for ic in self._game.cellIds() : army= self._game.armyOn(ic) if army : if self._playerId == army.status() : myArmy+= army.integer(2) else : opponentArmy+= army.integer(2) return myArmy, opponentArmy 4. Speed-up the learning process by reducing the state definition (for instance by considering only player armies (ignoring opponent details). 5. Its working well ? try your solution on board-6 then board-10 and board-12 . The board can be set on the GameRisky constructor, in launcher script: from hacka.games.risky import GameRisky as Game game= Game( map=\"board-6\" ) Generate Decision-Tree in Risky In this next step, the goal is to learn a decision tree from data. A good course introducing decision tree learning is proposed on the w3schools . In our case the data can be generated from our first QLearner Bot. The CSV (ComaSeparetedValue) file is built at sleep step to trace visited states and actions associated with the final game status. states variables, action, winner states variables, action, winner states variables, action, winner states variables, action, winner states variables, action, winner ... states variables : coma separated values of all state variables. action : selected action (defend, expend-1, expend-2, ... fight-1, ...). winner : a boolean ( positive \\(1\\) if the player win, negative \\(0\\) else). The model should be trained to predict if the player is on a winning trajectory. In other terms, if the current state and the applied action can be labeled as good or bad considering a predicted end of the game. The generated Decision-Tree model can be analyzed regarding \\(2\\) aspects: Prediction accuracy: The capability the model has to predict good and bad situation and responses. Model readability: The size of the decision tree (the smallest the best) Naturally, the \\(2\\) aspects are in contradiction, but generally the reduction of the decision tree is welcomed while the consequences of prediction accuracy is low. Tricks: It is generally possible to fix an expected size for the Decision-Trees. While we are not dependent anymore to the number of state and action variables (i.e. fixed decision-tree size), it is possible to generate as many new descriptive variables we want (for instance total size of the army, number occupied territories...). (An alternative to the solution proposed by the w3schools exits with scikit-learn ). Going further The solution should perform on a more complex board (from board-6 to board-12 ). You can apply a classical reinforcement learning approach consisting of a loop: Use the learned Decition-Tree into QLearner Bot. Generate data from learned new QValues (the behavior should be increased, so different than previously used data set). Regenerate a new Decition-Tree. Return to step \\(1.\\)","title":"Using Decision Tree"},{"location":"learning/decision-tree/#using-decision-tree","text":"\"A decision tree is a decision support recursive partitioning structure\" Wikipedia . It can be used to partitioning states, i.e. to groups similar states together into super-states. Grouped states should be close in terms of behavior: expected action, value and transition. If such a structure can be built over the state space, then it is possible to exploit it into reinforcement learning.","title":"Using Decision Tree"},{"location":"learning/decision-tree/#simple-version-with-py421","text":"Py421 advantages the combinations 4-2-1 , 1-1-1 or X-1-1 . A first idea could be to focus state definition to target those combinations. This idea modeled as a tree will lookalike: This tree can be implemented with a \"if-then-else\" procedure into a QLearner Bot, to learn a good behavior on Py421 game only over 7 states. As a result, learning should be very fast but with a final behavior less performant than learning applied over all the 168 game states.","title":"Simple version With Py421"},{"location":"learning/decision-tree/#expert-state-definition-for-risky","text":"The goal is to apply state factorization/clustering to a game where the number of states and actions prevents from using QLearning over all the states. That for, the exercise consists of applying QLearning to risky . 1. As a first, action you should get familiar with the game mechanisms: tuto risky 2. Then, you have to trace (ie. print) a complete definition of crossed states. QUESTION: How many states describe risky configuration (considering the default BOARD-4) ? 3. Finally, try to learn a policy using QLearning over meta actions. QUESTION: Which action has the worst branching number ? A simple score over the game can be computed as the difference between armies total size. This score feeds appropriately the reward computation. def totalArmies(self) : myArmy= 0 opponentArmy= 0 for ic in self._game.cellIds() : army= self._game.armyOn(ic) if army : if self._playerId == army.status() : myArmy+= army.integer(2) else : opponentArmy+= army.integer(2) return myArmy, opponentArmy 4. Speed-up the learning process by reducing the state definition (for instance by considering only player armies (ignoring opponent details). 5. Its working well ? try your solution on board-6 then board-10 and board-12 . The board can be set on the GameRisky constructor, in launcher script: from hacka.games.risky import GameRisky as Game game= Game( map=\"board-6\" )","title":"Expert state definition for Risky"},{"location":"learning/decision-tree/#generate-decision-tree-in-risky","text":"In this next step, the goal is to learn a decision tree from data. A good course introducing decision tree learning is proposed on the w3schools . In our case the data can be generated from our first QLearner Bot. The CSV (ComaSeparetedValue) file is built at sleep step to trace visited states and actions associated with the final game status. states variables, action, winner states variables, action, winner states variables, action, winner states variables, action, winner states variables, action, winner ... states variables : coma separated values of all state variables. action : selected action (defend, expend-1, expend-2, ... fight-1, ...). winner : a boolean ( positive \\(1\\) if the player win, negative \\(0\\) else). The model should be trained to predict if the player is on a winning trajectory. In other terms, if the current state and the applied action can be labeled as good or bad considering a predicted end of the game. The generated Decision-Tree model can be analyzed regarding \\(2\\) aspects: Prediction accuracy: The capability the model has to predict good and bad situation and responses. Model readability: The size of the decision tree (the smallest the best) Naturally, the \\(2\\) aspects are in contradiction, but generally the reduction of the decision tree is welcomed while the consequences of prediction accuracy is low. Tricks: It is generally possible to fix an expected size for the Decision-Trees. While we are not dependent anymore to the number of state and action variables (i.e. fixed decision-tree size), it is possible to generate as many new descriptive variables we want (for instance total size of the army, number occupied territories...). (An alternative to the solution proposed by the w3schools exits with scikit-learn ).","title":"Generate Decision-Tree in Risky"},{"location":"learning/decision-tree/#going-further","text":"The solution should perform on a more complex board (from board-6 to board-12 ). You can apply a classical reinforcement learning approach consisting of a loop: Use the learned Decition-Tree into QLearner Bot. Generate data from learned new QValues (the behavior should be increased, so different than previously used data set). Regenerate a new Decition-Tree. Return to step \\(1.\\)","title":"Going further"},{"location":"learning/model-based/","text":"Model Based Learning: Markov-Decision-Process This tutorial aims to compute a policy from learned transitions and rewards on py421 game. To do that we need to record a transition function and a average reward function. With dictionary, the transition function would be a dictionary over states returning dictionaries over actions returning dictionaries over reached states' returning the number of times the transition is experienced. In case of Py421 , an object about \\(128 \\times 8 \\times 128\\) counters, but the reward function would be simplest. The model is based on Markov Decision Process (MDP) framework: On Wikipedia After enough experiments, it would be possible to process this MDP model function to compute a policy. Initialize your MDP Bot: First we require specific model methods: state() to transform game variable affectations into a dictionary entry, updateModel(state_past, action, state_present, reward) to use to increase knowledge about transition and reward. # Model: def state(self) : return f\"{self._horizon}-{self._dices[0]}-{self._dices[1]}-{self._dices[2]}\" def updateModel( self, past_state, action, present_state, reward ): print( f\"transition: {past_state}, {action}, {present_state}\") print( f\"reward: {reward}\\n\") Then it is possible to prepare perception s method with model update: # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): self._state= \"3-4-2-1\" self._action= \"r-r-r\" self._score= 0 def perceive(self, gameState): self._horizon= gameState.child(1).integer(1) self._dices= gameState.child(2).integers() newScore= gameState.child(2).value(1) # Learn: newState= self.state() self.updateModel( self._state, self._action, newState, newScore-self._score ) # Switch: self._state= newState self._score= newScore Record transitions and reward: The updateModel method updates counters and average rewards. My advice is to keep also a counter experiments over the amount of time the Bot tests the same action into the same state. At some point, the updateModel method, the update should be something like: # Roling reward average : total= self._counterExperiences[past_state][action] oldReward= self._rewards[past_state][action] self._rewards[past_state][action]= (oldReward * total + reward) / (total+1) # Update counters self._counterExperiences[past_state][action]+= 1 self._counterTransitions[past_state][action][present_state]+= 1 Naturally, it supposes that you already create the dictionaries and the entrances... Use json package to save your model (sometimes) into your sleep method. Process model. This is the interesting part of the exercice. From the transitions and rewards we want to compute the optimal policy \\(\\pi^*\\) For that purpose, we recommand to apply Value Iteration (cf. On Wikipedia ) valueIteration: Input: an MDP: \\(\\langle S, A, T, R \\rangle\\) ; precision error: \\(\\epsilon\\) ; discount factor: \\(\\gamma\\) ; initial values(s) Repeat until: maximal delta < \\(\\epsilon\\) For each state \\(s \\in S\\) \\(\\qquad \\qquad -\\) Search the action \\(a^*\\) maximizing the Bellman Equation on \\(s\\) \\[a^*= \\arg\\max_{a \\in A}\\left( \\mathit{value}(s, a) \\right)\\] \\[\\mathit{value}(s, a)= R(s, a) + \\gamma \\sum_{s'\\in S} T(s,a,s') \\times \\mathit{values}(s')\\] \\(\\qquad \\qquad -\\) Set \\(\\pi(s)= a^*\\) and _ \\(\\mathit{values}(s)= \\mathit{computeValue}(s, a^*)\\) \\(\\qquad \\qquad -\\) Compute the delta value between the previous and the new \\(\\mathit{values}(s)\\) Output: an optimal \\(\\pi^*\\) and the associated V-values The policy can be updated every \\(500\\) sleep with a call to your new valueIteration . To notice that self._transition is not directly usable. Values need to be transformed to probabilities ( self._transition[s][a][s'] / self._experiements[s][a] ). Furthermore, any state not defined into self._transition can be considered as a terminal state ( len(V) >= len(self._transition) ). If necessary, terminal state has only the keep-keep-keep action and the transition falls into the same state (itself) with probability of \\(1\\) and a reward of \\(0\\) . Exploit. Use the policy at decision step. However, until a sufisant number of experiments are performed, the model and so the policy cannot be completelly trusted. A random exporation ratio need to be design as for Q-Learning .","title":"Model Based"},{"location":"learning/model-based/#model-based-learning-markov-decision-process","text":"This tutorial aims to compute a policy from learned transitions and rewards on py421 game. To do that we need to record a transition function and a average reward function. With dictionary, the transition function would be a dictionary over states returning dictionaries over actions returning dictionaries over reached states' returning the number of times the transition is experienced. In case of Py421 , an object about \\(128 \\times 8 \\times 128\\) counters, but the reward function would be simplest. The model is based on Markov Decision Process (MDP) framework: On Wikipedia After enough experiments, it would be possible to process this MDP model function to compute a policy.","title":"Model Based Learning: Markov-Decision-Process"},{"location":"learning/model-based/#initialize-your-mdp-bot","text":"First we require specific model methods: state() to transform game variable affectations into a dictionary entry, updateModel(state_past, action, state_present, reward) to use to increase knowledge about transition and reward. # Model: def state(self) : return f\"{self._horizon}-{self._dices[0]}-{self._dices[1]}-{self._dices[2]}\" def updateModel( self, past_state, action, present_state, reward ): print( f\"transition: {past_state}, {action}, {present_state}\") print( f\"reward: {reward}\\n\") Then it is possible to prepare perception s method with model update: # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): self._state= \"3-4-2-1\" self._action= \"r-r-r\" self._score= 0 def perceive(self, gameState): self._horizon= gameState.child(1).integer(1) self._dices= gameState.child(2).integers() newScore= gameState.child(2).value(1) # Learn: newState= self.state() self.updateModel( self._state, self._action, newState, newScore-self._score ) # Switch: self._state= newState self._score= newScore","title":"Initialize your MDP Bot:"},{"location":"learning/model-based/#record-transitions-and-reward","text":"The updateModel method updates counters and average rewards. My advice is to keep also a counter experiments over the amount of time the Bot tests the same action into the same state. At some point, the updateModel method, the update should be something like: # Roling reward average : total= self._counterExperiences[past_state][action] oldReward= self._rewards[past_state][action] self._rewards[past_state][action]= (oldReward * total + reward) / (total+1) # Update counters self._counterExperiences[past_state][action]+= 1 self._counterTransitions[past_state][action][present_state]+= 1 Naturally, it supposes that you already create the dictionaries and the entrances... Use json package to save your model (sometimes) into your sleep method.","title":"Record transitions and reward:"},{"location":"learning/model-based/#process-model","text":"This is the interesting part of the exercice. From the transitions and rewards we want to compute the optimal policy \\(\\pi^*\\) For that purpose, we recommand to apply Value Iteration (cf. On Wikipedia ) valueIteration: Input: an MDP: \\(\\langle S, A, T, R \\rangle\\) ; precision error: \\(\\epsilon\\) ; discount factor: \\(\\gamma\\) ; initial values(s) Repeat until: maximal delta < \\(\\epsilon\\) For each state \\(s \\in S\\) \\(\\qquad \\qquad -\\) Search the action \\(a^*\\) maximizing the Bellman Equation on \\(s\\) \\[a^*= \\arg\\max_{a \\in A}\\left( \\mathit{value}(s, a) \\right)\\] \\[\\mathit{value}(s, a)= R(s, a) + \\gamma \\sum_{s'\\in S} T(s,a,s') \\times \\mathit{values}(s')\\] \\(\\qquad \\qquad -\\) Set \\(\\pi(s)= a^*\\) and _ \\(\\mathit{values}(s)= \\mathit{computeValue}(s, a^*)\\) \\(\\qquad \\qquad -\\) Compute the delta value between the previous and the new \\(\\mathit{values}(s)\\) Output: an optimal \\(\\pi^*\\) and the associated V-values The policy can be updated every \\(500\\) sleep with a call to your new valueIteration . To notice that self._transition is not directly usable. Values need to be transformed to probabilities ( self._transition[s][a][s'] / self._experiements[s][a] ). Furthermore, any state not defined into self._transition can be considered as a terminal state ( len(V) >= len(self._transition) ). If necessary, terminal state has only the keep-keep-keep action and the transition falls into the same state (itself) with probability of \\(1\\) and a reward of \\(0\\) .","title":"Process model."},{"location":"learning/model-based/#exploit","text":"Use the policy at decision step. However, until a sufisant number of experiments are performed, the model and so the policy cannot be completelly trusted. A random exporation ratio need to be design as for Q-Learning .","title":"Exploit."},{"location":"learning/policy/","text":"Understand the notion of Policy Policy is a function \\(\\pi\\) returning the action to perform regarding a given state. To better understand the notion of policy we propose to learn one for Py421 game. We wan to get a maximum of information about the interest of applying actions in game situation (state) We propose to do that by acting randomly, with a RecorderRandBot based on a py421.firstBot Bot . Record experiments The idea consists in recording information required to learn a policy, in other world, to evaluate efficiency of visited state and action. The evaluation is classically obtained at the end of a game, with the final score. The expected file would look like: state, action, result state, action, result state, action, result state, action, result state, action, result ... For instance with Py421 : 4-2-1, keep-keep-keep, 800 6-3-1, keep-roll-keep, 184 5-5-3, roll-keep-keep, 104 ... It require to trace the visited state and action. The recorderRandBot will lookalike: class Pi421Random(): def process(self, state): return random.choice( [ \"keep-keep-keep\", \"roll-keep-keep\", \"keep-roll-keep\", \"roll-roll-keep\", \"keep-keep-roll\", \"roll-keep-roll\", \"keep-roll-roll\", \"roll-roll-roll\" ] ) class RecoderBot(): def __init__(self, policy= Pi421Random() ): self._pi= policy def wakeUp(self, playerId, numberOfPlayers, gameConf): # Recorder buffer: self._trace= [] def perceive(self, gameState): # Game variables: self._horizon = gameState.child(1).integer(1) self._dices = gameState.child(2).integers() self._score = gameState.child(2).value(1) def decide(self): state= '-'.join([ str(d) for d in self._dices ]) action= self._pi.process(state) self._trace.append( {'state': state, 'action': action} ) return action def sleep(self, result): # open a State.Action.Value file in append mode: logFile= open( \"log-SAV.csv\", \"a\" ) # For each recorded experience in trace for xp in self._trace : # add a line in the file logFile.write( f\"{xp['state']}, {xp['action']}, {result}\\n\" ) logFile.close() Important: the recording can only be done at sleep time. The bot need to reach the end of the game to evaluate the succession of actions it performed. Tracing the visited state and actions is performed at decide step, with a trace attribute initialized at wake-up step. You can open your log-sav.csv (state, action, value) files to see it content. Process the data Processing log-sav.csv file consists of generating a structure matching a coherent policy from brute data. Typically, the structure can be simply a dictionnary over possible states. ie: policy= { 'state1': 'actionInState1', 'state2': 'actionInState2', ... } Dictionary: - Python documentation - On w3school But first, it is required to read the log file and group the same experiences together. For instance, in state '4-3-1' random strategy will try several times the action 'keep-roll-keep' with different results. The simplest way to do that is to create a dictionary over state referencing dictionaries over action referencing lists of reached scores (expected result: data['state']['action'] -> a list of value ). Here, an example of the load section for the script process-sav.py : data= {} # Load data.. logFile= open(\"log-SAV.csv\", \"r\") for line in logFile : state, action, value= tuple( line.split(', ') ) value= float(value) if state not in data : data[state]= {action: [value]} elif action not in data[state]: data[state][action]= [value] else : data[state][action].append( value ) logFile.close() print( data ) We can now process the data: Compute the average score for each tuple (state, action) Select the action with the maximum score in a policy dictionary. In the end, policy[\"4-2-1\"] should return \"keep-keep-keep\" for instance. Notice that, a json package exists with a dump function to record the policy into a policy-421-sav.json file. policyFile= open(\"py421PolicyBot.json\", \"w\") json.dump( computedPolicy, policyFile, sort_keys=True, indent=2 ) policyFile.close() exploit Finally, it is possible to exploit the policy with a policyBot player. First load the policy, in the player constructor for instance: def __init__(self, policyFilePath): policyFile= open(policyFilePath) self.policy= json.load( policyFile ) policyFile.close() Then apply the policy actions: def decide(self): action= self.policy[ '-'.join([ str(d) for d in self._dices ]) ] return action The policy reaches an average score close up to \\(290\\) . Complete Policy: You can apply the same method but over the complete state definition. By adding the horizon in the state definition ( 4-2-1h2 for instance rather than only 4-2-1 ), it possible to reach average score of more than \\(320\\) . It will just require more experiences in the recording phase.","title":"Policy"},{"location":"learning/policy/#understand-the-notion-of-policy","text":"Policy is a function \\(\\pi\\) returning the action to perform regarding a given state. To better understand the notion of policy we propose to learn one for Py421 game. We wan to get a maximum of information about the interest of applying actions in game situation (state) We propose to do that by acting randomly, with a RecorderRandBot based on a py421.firstBot Bot .","title":"Understand the notion of Policy"},{"location":"learning/policy/#record-experiments","text":"The idea consists in recording information required to learn a policy, in other world, to evaluate efficiency of visited state and action. The evaluation is classically obtained at the end of a game, with the final score. The expected file would look like: state, action, result state, action, result state, action, result state, action, result state, action, result ... For instance with Py421 : 4-2-1, keep-keep-keep, 800 6-3-1, keep-roll-keep, 184 5-5-3, roll-keep-keep, 104 ... It require to trace the visited state and action. The recorderRandBot will lookalike: class Pi421Random(): def process(self, state): return random.choice( [ \"keep-keep-keep\", \"roll-keep-keep\", \"keep-roll-keep\", \"roll-roll-keep\", \"keep-keep-roll\", \"roll-keep-roll\", \"keep-roll-roll\", \"roll-roll-roll\" ] ) class RecoderBot(): def __init__(self, policy= Pi421Random() ): self._pi= policy def wakeUp(self, playerId, numberOfPlayers, gameConf): # Recorder buffer: self._trace= [] def perceive(self, gameState): # Game variables: self._horizon = gameState.child(1).integer(1) self._dices = gameState.child(2).integers() self._score = gameState.child(2).value(1) def decide(self): state= '-'.join([ str(d) for d in self._dices ]) action= self._pi.process(state) self._trace.append( {'state': state, 'action': action} ) return action def sleep(self, result): # open a State.Action.Value file in append mode: logFile= open( \"log-SAV.csv\", \"a\" ) # For each recorded experience in trace for xp in self._trace : # add a line in the file logFile.write( f\"{xp['state']}, {xp['action']}, {result}\\n\" ) logFile.close() Important: the recording can only be done at sleep time. The bot need to reach the end of the game to evaluate the succession of actions it performed. Tracing the visited state and actions is performed at decide step, with a trace attribute initialized at wake-up step. You can open your log-sav.csv (state, action, value) files to see it content.","title":"Record experiments"},{"location":"learning/policy/#process-the-data","text":"Processing log-sav.csv file consists of generating a structure matching a coherent policy from brute data. Typically, the structure can be simply a dictionnary over possible states. ie: policy= { 'state1': 'actionInState1', 'state2': 'actionInState2', ... } Dictionary: - Python documentation - On w3school But first, it is required to read the log file and group the same experiences together. For instance, in state '4-3-1' random strategy will try several times the action 'keep-roll-keep' with different results. The simplest way to do that is to create a dictionary over state referencing dictionaries over action referencing lists of reached scores (expected result: data['state']['action'] -> a list of value ). Here, an example of the load section for the script process-sav.py : data= {} # Load data.. logFile= open(\"log-SAV.csv\", \"r\") for line in logFile : state, action, value= tuple( line.split(', ') ) value= float(value) if state not in data : data[state]= {action: [value]} elif action not in data[state]: data[state][action]= [value] else : data[state][action].append( value ) logFile.close() print( data ) We can now process the data: Compute the average score for each tuple (state, action) Select the action with the maximum score in a policy dictionary. In the end, policy[\"4-2-1\"] should return \"keep-keep-keep\" for instance. Notice that, a json package exists with a dump function to record the policy into a policy-421-sav.json file. policyFile= open(\"py421PolicyBot.json\", \"w\") json.dump( computedPolicy, policyFile, sort_keys=True, indent=2 ) policyFile.close()","title":"Process the data"},{"location":"learning/policy/#exploit","text":"Finally, it is possible to exploit the policy with a policyBot player. First load the policy, in the player constructor for instance: def __init__(self, policyFilePath): policyFile= open(policyFilePath) self.policy= json.load( policyFile ) policyFile.close() Then apply the policy actions: def decide(self): action= self.policy[ '-'.join([ str(d) for d in self._dices ]) ] return action The policy reaches an average score close up to \\(290\\) .","title":"exploit"},{"location":"learning/policy/#complete-policy","text":"You can apply the same method but over the complete state definition. By adding the horizon in the state definition ( 4-2-1h2 for instance rather than only 4-2-1 ), it possible to reach average score of more than \\(320\\) . It will just require more experiences in the recording phase.","title":"Complete Policy:"},{"location":"learning/qlearning/","text":"Basic Reinforcement Learning: Q-Learning Reinforcement Learning is a family of approaches and algorithms that enhance an autonomous system (an agent) to learn from it successful tries and failures [Cf. Wikipedia ]. Technically, at the beginning, the agent is capable of acting in it environment (with default pure random action for instance) and by acting it gets feedback. Accumulating feedback, it will be capable of evaluating the interest of actions in a given context. In reinforcement Learning family, Q-Learning is quite a simple and apply pretty well for learning to play 421 . The goal of the tutorial: Implement a new qlearnerBot player from random Bot . At initialization qvalues is created empty with the other required variables. At perception steps the bot updates its qvalues At decision steps it chooses a new action to perform. And do not forget: you must test your code at each development step by executing the code for a few games and validate that the output is as expected (also a good python tool to make test: pytest ). Q-Values equation The Q-Learning algorithm ([Cf. Wikipedia ]) consists of computing qvalues , a dictionary of interests/values of executing a given action from a given state. \\[ \\mathit{qvalues}(s, a) \\in \\mathbb{R} \\] A Q-Value is equal to the immediate reward of doing action \\(a\\) in a state \\(s\\) plus the future rewards modeled as \\(\\mathit{qvalues}(s', a')\\) . \\(s'\\) is the state reached from (s, a) and \\(a'\\) the next action that will be performed. In other terms, \\(\\mathit{qvalues}(s, a)\\) is the cumulative reward from \\(s\\) , knowing that the next action is \\(a\\) . However, the resulting experiment of doing \\(a\\) from \\(s\\) need to be computed as an average. Finally, one of the ways to formalize updates on \\(\\mathit{qvalues}(s, a)\\) is: \\[ \\mathit{qvalues}(s, a) = \\alpha \\times \\mathit{experience} + (1-\\alpha)\\times \\mathit{qvalues}(s, a)\\] \\[ \\text{with:}\\ \\mathit{experience}= r(s, a, s') + \\max_{a'\\in A}(\\ \\mathit{qvalues}(s', a')\\ ) \\] The learning rate \\(\\alpha\\) is the speed that incoming experiences erase the oldest (can be initialized at \\(0.1\\) as a first approximation). Implementing Q-Values At some point, our new Bot (defined in qlearnerBot.py ) will require to update its qvalues from its experiments. So let's start with a method: def updateQvalues(self, previous_state, action, current_state, reward): ... A simple way to implement qvalues in python language is to implement it as a Dictionnary of dictionaries (expected result: qvalues['state']['action'] -> the interest of doing 'action' in 'state' ). But first: initializing empty qvalues will look like: qvalues= {} Typically in the constructor method __init__ in python (and with Q-Learning attributes): class Bot : def __init__(self): self._alpha= 0.1 # learning rate, speed that an incoming experience erases the oldest. self._qvalues= { \"wakeUp\": {\"roll-roll-roll\": 0.0} } Returning to the updateQvalues method, initializing action values for a given state will lookalike # state= \"2|6-3-1\" for instance (2 re-rolls from dice: 6, 3 and 1) if state not in self.qvalues.keys() : self._qvalues[state]= { \"keep-keep-keep\":0.0, \"roll-keep-keep\":0.0, \"keep-roll-keep\":0.0, \"roll-roll-keep\":0.0, \"keep-keep-roll\":0.0, \"roll-keep-roll\":0.0, \"keep-roll-roll\":0.0, \"roll-roll-roll\":0.0 } Finally, modifying a value in qvalues will look lookalike self._qvalues[\"2|6-3-1\"][\"roll-roll-roll\"]= ... When to update Q-Values ? The updateQvalues method requires to confront previous and current states It can be performed while a new state is reaches. So, call to updateQvalues can be implemented into perception method. The reward can be computed as the difference between previous and current score. Do not forget to initialize state and action in wakeUp method. Your Bot player interface methods should be: def wakeUp(self, playerId, numberOfPlayers, gameConf): self._state= \"wakeUp\" self._action= \"roll-roll-roll\" self._score= 0 def perceive(self, gameState): self._horizon= gameState.child(1).integer(1) self._dices= gameState.child(2).integers() newScore= gameState.child(2).value(1) # Learn: newState= f\"{self._horizon}|{self._dices[0]}-{self._dices[1]}-{self._dices[2]}\" self.updateQvalues( self._state, self._action, newState, newScore-self._score ) # Switch: self._state= newState self._score= newScore Exploration-Exploitation Dilemma Until now we just compute and record statistical rewards. The idea is to use-it on to take decision at some time (i.e. when we record a good knowledge). However the first difficulty in reinforcement learning result in the definition of this \"at some time\". In other terms, when to stop the computation of statistical kwonledge by exploring actions and to start the exploitation of computed statistics to make decisions. It is known as the exploration versus exploitation trade-off [cf. wikipedia ]. One way to overpass the trade-off is to put it random. The \u03b5-greedy heuristic suppose that you will choose an exploration action (i.e. a random action for instance) a few times in a given time step. More technically, at each time step the AI randomly choose to get a random action or to get the best one accordingly to the current knowledge. The random chose to explore versus to exploit is weighted by \u03b5 and 1-\u03b5 with \u03b5 between 1 and 0. Do not forget to initialize the self._epsilon in the constructor method ( self._epsilon= 0.1 is generally a good first value, it states that a random action would be chosen 1 time over 10). Experiments You can now try to answer the question: how many episodes are required to learn a good enough policy. One way to do that is to plot the evolution of the strategy during the learning. To do that we want to compute the average gain for benches of \\(500\\) games. This way we should be capable of visualizing a progression. Py421 game is a very chaotic game, \\(500\\) games are a minimum to get almost coherent averages. To do that, we will first create a new attribute to remember all final results at Bot initialization. Then, we will compute and print the average, every \\(500\\) games. import matplotlib.pyplot as plt ... Class Bot: def __init__(self): # Progress: self._i= 0 self._results= [] ... def sleep(self, result): self._results.append(result) if len(self._results) == 500 : self._i+= 1 print( f\"- results ({self._i*500}): {sum(self._results)/500.0}\" ) self._results= [] The goal is to find the best learning rate and exploration/exploitation ratio to get a fast and efficient Q-learning process.","title":"Q-Learning"},{"location":"learning/qlearning/#basic-reinforcement-learning-q-learning","text":"Reinforcement Learning is a family of approaches and algorithms that enhance an autonomous system (an agent) to learn from it successful tries and failures [Cf. Wikipedia ]. Technically, at the beginning, the agent is capable of acting in it environment (with default pure random action for instance) and by acting it gets feedback. Accumulating feedback, it will be capable of evaluating the interest of actions in a given context. In reinforcement Learning family, Q-Learning is quite a simple and apply pretty well for learning to play 421 . The goal of the tutorial: Implement a new qlearnerBot player from random Bot . At initialization qvalues is created empty with the other required variables. At perception steps the bot updates its qvalues At decision steps it chooses a new action to perform. And do not forget: you must test your code at each development step by executing the code for a few games and validate that the output is as expected (also a good python tool to make test: pytest ).","title":"Basic Reinforcement Learning: Q-Learning"},{"location":"learning/qlearning/#q-values-equation","text":"The Q-Learning algorithm ([Cf. Wikipedia ]) consists of computing qvalues , a dictionary of interests/values of executing a given action from a given state. \\[ \\mathit{qvalues}(s, a) \\in \\mathbb{R} \\] A Q-Value is equal to the immediate reward of doing action \\(a\\) in a state \\(s\\) plus the future rewards modeled as \\(\\mathit{qvalues}(s', a')\\) . \\(s'\\) is the state reached from (s, a) and \\(a'\\) the next action that will be performed. In other terms, \\(\\mathit{qvalues}(s, a)\\) is the cumulative reward from \\(s\\) , knowing that the next action is \\(a\\) . However, the resulting experiment of doing \\(a\\) from \\(s\\) need to be computed as an average. Finally, one of the ways to formalize updates on \\(\\mathit{qvalues}(s, a)\\) is: \\[ \\mathit{qvalues}(s, a) = \\alpha \\times \\mathit{experience} + (1-\\alpha)\\times \\mathit{qvalues}(s, a)\\] \\[ \\text{with:}\\ \\mathit{experience}= r(s, a, s') + \\max_{a'\\in A}(\\ \\mathit{qvalues}(s', a')\\ ) \\] The learning rate \\(\\alpha\\) is the speed that incoming experiences erase the oldest (can be initialized at \\(0.1\\) as a first approximation).","title":"Q-Values equation"},{"location":"learning/qlearning/#implementing-q-values","text":"At some point, our new Bot (defined in qlearnerBot.py ) will require to update its qvalues from its experiments. So let's start with a method: def updateQvalues(self, previous_state, action, current_state, reward): ... A simple way to implement qvalues in python language is to implement it as a Dictionnary of dictionaries (expected result: qvalues['state']['action'] -> the interest of doing 'action' in 'state' ). But first: initializing empty qvalues will look like: qvalues= {} Typically in the constructor method __init__ in python (and with Q-Learning attributes): class Bot : def __init__(self): self._alpha= 0.1 # learning rate, speed that an incoming experience erases the oldest. self._qvalues= { \"wakeUp\": {\"roll-roll-roll\": 0.0} } Returning to the updateQvalues method, initializing action values for a given state will lookalike # state= \"2|6-3-1\" for instance (2 re-rolls from dice: 6, 3 and 1) if state not in self.qvalues.keys() : self._qvalues[state]= { \"keep-keep-keep\":0.0, \"roll-keep-keep\":0.0, \"keep-roll-keep\":0.0, \"roll-roll-keep\":0.0, \"keep-keep-roll\":0.0, \"roll-keep-roll\":0.0, \"keep-roll-roll\":0.0, \"roll-roll-roll\":0.0 } Finally, modifying a value in qvalues will look lookalike self._qvalues[\"2|6-3-1\"][\"roll-roll-roll\"]= ...","title":"Implementing Q-Values"},{"location":"learning/qlearning/#when-to-update-q-values","text":"The updateQvalues method requires to confront previous and current states It can be performed while a new state is reaches. So, call to updateQvalues can be implemented into perception method. The reward can be computed as the difference between previous and current score. Do not forget to initialize state and action in wakeUp method. Your Bot player interface methods should be: def wakeUp(self, playerId, numberOfPlayers, gameConf): self._state= \"wakeUp\" self._action= \"roll-roll-roll\" self._score= 0 def perceive(self, gameState): self._horizon= gameState.child(1).integer(1) self._dices= gameState.child(2).integers() newScore= gameState.child(2).value(1) # Learn: newState= f\"{self._horizon}|{self._dices[0]}-{self._dices[1]}-{self._dices[2]}\" self.updateQvalues( self._state, self._action, newState, newScore-self._score ) # Switch: self._state= newState self._score= newScore","title":"When to update Q-Values ?"},{"location":"learning/qlearning/#exploration-exploitation-dilemma","text":"Until now we just compute and record statistical rewards. The idea is to use-it on to take decision at some time (i.e. when we record a good knowledge). However the first difficulty in reinforcement learning result in the definition of this \"at some time\". In other terms, when to stop the computation of statistical kwonledge by exploring actions and to start the exploitation of computed statistics to make decisions. It is known as the exploration versus exploitation trade-off [cf. wikipedia ]. One way to overpass the trade-off is to put it random. The \u03b5-greedy heuristic suppose that you will choose an exploration action (i.e. a random action for instance) a few times in a given time step. More technically, at each time step the AI randomly choose to get a random action or to get the best one accordingly to the current knowledge. The random chose to explore versus to exploit is weighted by \u03b5 and 1-\u03b5 with \u03b5 between 1 and 0. Do not forget to initialize the self._epsilon in the constructor method ( self._epsilon= 0.1 is generally a good first value, it states that a random action would be chosen 1 time over 10).","title":"Exploration-Exploitation Dilemma"},{"location":"learning/qlearning/#experiments","text":"You can now try to answer the question: how many episodes are required to learn a good enough policy. One way to do that is to plot the evolution of the strategy during the learning. To do that we want to compute the average gain for benches of \\(500\\) games. This way we should be capable of visualizing a progression. Py421 game is a very chaotic game, \\(500\\) games are a minimum to get almost coherent averages. To do that, we will first create a new attribute to remember all final results at Bot initialization. Then, we will compute and print the average, every \\(500\\) games. import matplotlib.pyplot as plt ... Class Bot: def __init__(self): # Progress: self._i= 0 self._results= [] ... def sleep(self, result): self._results.append(result) if len(self._results) == 500 : self._i+= 1 print( f\"- results ({self._i*500}): {sum(self._results)/500.0}\" ) self._results= [] The goal is to find the best learning rate and exploration/exploitation ratio to get a fast and efficient Q-learning process.","title":"Experiments"},{"location":"learning/scale-up/","text":"Scale-Up The goal of the tutorial is to understand, feel the complexity of addressing a system with combinatorial explosion over the number of states... This tutorial relies on MoveIt game. Make sure you clearly understand the game and its API. Basic Q-Learning: In its basic configuration MoveIt involves a \\(6 \\times 4\\) grid with randomly generated obstacles. Has a first exercise you can apply basic Q-Learning with a fixed configuration by fixing the random seed at game creation ( game= GameMoveIt( seed=128 ) ). In this case the system state 'only' involve the positions of the robot and of the humans. Something as \\(3\\) variables into \\(6 \\times 4\\) possibilities. We also require the robot target, but will only record the directions ( \\(\\in [0, 6]\\) )... def state(self): robot, human1, human2 = tuple(self._mobiles) # Path to the goal: pathGoal= self._board.path( robot.x(), robot.y(), robot.goalx(), robot.goaly() ) state= f\"{robot.x()}-{robot.y()}-{pathGoal[0]}\" state+= f\"-{human1.x()}-{human1.y()}\" state+= f\"-{human2.x()}-{human2.y()}\" return state At this point the system is modeled with \\((6 \\times 4)^3*6\\) states. Lucky for us, the reward can be easily computed at each time steps (in the perception method). newScore= statePod.value(1) reward= newScore - self._score self._score= newScore You can now apply basic Q-Learning as experimented on 4.2.1 . Expert Heuristic Optimizations: A Basic Q-Learning learning is slow and will be blocked at something like \\(-100\\) meaning that the robot continues to generate collision. A first optimization of the approach consists of including expert knowledge to limit the bad actions and to speed up the learning process. Remove forbidden actions: At decision time, it is possible to remove from the pool of available actions, the actions driving the robot on an obstacle or on a human. Speed-up Learning process Each time a new state is visited and recorded on Q dictionary, it is possible to associate the action identified by the path to the robot target with a positive value. Still negative ? In fact, the state space does not integrate the trajectory of the humans. The humans generally move toward their own goal (with error exception). The human\u2019s goals are hidden to the robot, but the human trajectory into the bot state to better anticipate the next human's moves. Scale-up: Mission on a \\(4 \\times 6\\) grid with \\(2\\) humans remains a very limited scenario. At launch time, it is possible to change the game configuration: game= GameMoveIt( seed=128, # Grid seed for generations... sizeHeight=4, # size of the grid: number of lines sizeLine= 6, # size of the grid: number of cells in a line numberOfObstacles= 6, # number of obstacles numberOfHuman= 3 # number of humans ) The provided solution should be capable of learning rapidly, new behavior on new environment. Local vs Global States The real problem consists in removing the limitation of learning behairio on a unique environment settings, but to be capable of using the learned policy in all situations. By integrating map configuration into the state variables, the state space will grow exponentially. Which variables ? For how many states ? A better solution consists to model the system from the point of view of the actions. We speak about local versus global models. For instance, the goal can be characterized as direction and distance rather than \\((x, y)\\) coordinate. Propose a state definition based from robot location in a less dependent way to the grid dimension and try again Q-Learning algorithm. To notice that local representation would be more suitable with Decision-Tree.","title":"Scale-Up"},{"location":"learning/scale-up/#scale-up","text":"The goal of the tutorial is to understand, feel the complexity of addressing a system with combinatorial explosion over the number of states... This tutorial relies on MoveIt game. Make sure you clearly understand the game and its API.","title":"Scale-Up"},{"location":"learning/scale-up/#basic-q-learning","text":"In its basic configuration MoveIt involves a \\(6 \\times 4\\) grid with randomly generated obstacles. Has a first exercise you can apply basic Q-Learning with a fixed configuration by fixing the random seed at game creation ( game= GameMoveIt( seed=128 ) ). In this case the system state 'only' involve the positions of the robot and of the humans. Something as \\(3\\) variables into \\(6 \\times 4\\) possibilities. We also require the robot target, but will only record the directions ( \\(\\in [0, 6]\\) )... def state(self): robot, human1, human2 = tuple(self._mobiles) # Path to the goal: pathGoal= self._board.path( robot.x(), robot.y(), robot.goalx(), robot.goaly() ) state= f\"{robot.x()}-{robot.y()}-{pathGoal[0]}\" state+= f\"-{human1.x()}-{human1.y()}\" state+= f\"-{human2.x()}-{human2.y()}\" return state At this point the system is modeled with \\((6 \\times 4)^3*6\\) states. Lucky for us, the reward can be easily computed at each time steps (in the perception method). newScore= statePod.value(1) reward= newScore - self._score self._score= newScore You can now apply basic Q-Learning as experimented on 4.2.1 .","title":"Basic Q-Learning:"},{"location":"learning/scale-up/#expert-heuristic-optimizations","text":"A Basic Q-Learning learning is slow and will be blocked at something like \\(-100\\) meaning that the robot continues to generate collision. A first optimization of the approach consists of including expert knowledge to limit the bad actions and to speed up the learning process.","title":"Expert Heuristic Optimizations:"},{"location":"learning/scale-up/#remove-forbidden-actions","text":"At decision time, it is possible to remove from the pool of available actions, the actions driving the robot on an obstacle or on a human.","title":"Remove forbidden actions:"},{"location":"learning/scale-up/#speed-up-learning-process","text":"Each time a new state is visited and recorded on Q dictionary, it is possible to associate the action identified by the path to the robot target with a positive value.","title":"Speed-up Learning process"},{"location":"learning/scale-up/#still-negative","text":"In fact, the state space does not integrate the trajectory of the humans. The humans generally move toward their own goal (with error exception). The human\u2019s goals are hidden to the robot, but the human trajectory into the bot state to better anticipate the next human's moves.","title":"Still negative ?"},{"location":"learning/scale-up/#scale-up_1","text":"Mission on a \\(4 \\times 6\\) grid with \\(2\\) humans remains a very limited scenario. At launch time, it is possible to change the game configuration: game= GameMoveIt( seed=128, # Grid seed for generations... sizeHeight=4, # size of the grid: number of lines sizeLine= 6, # size of the grid: number of cells in a line numberOfObstacles= 6, # number of obstacles numberOfHuman= 3 # number of humans ) The provided solution should be capable of learning rapidly, new behavior on new environment.","title":"Scale-up:"},{"location":"learning/scale-up/#local-vs-global-states","text":"The real problem consists in removing the limitation of learning behairio on a unique environment settings, but to be capable of using the learned policy in all situations. By integrating map configuration into the state variables, the state space will grow exponentially. Which variables ? For how many states ? A better solution consists to model the system from the point of view of the actions. We speak about local versus global models. For instance, the goal can be characterized as direction and distance rather than \\((x, y)\\) coordinate. Propose a state definition based from robot location in a less dependent way to the grid dimension and try again Q-Learning algorithm. To notice that local representation would be more suitable with Decision-Tree.","title":"Local vs Global States"},{"location":"learning/speed-up/","text":"Speed-Up The advantage of Model-Based learning compared to solutions like Q-Learning is to provide some guarantees on the computed policy. However, the main goal in machine learning is to build coherent policies as fast as possible (and by doing so, to be capable of scaling up). That for, learning directly the policy is generally preferred in Reinforcement Learning. However, solutions like Q-Learning remains inapplicable in real-life problems, and solutions need to be set up to bypass combinatorial explosions. Experiments First we need to set up an environment to save and monitor our progress in learning policies (or models). One way to do that is to plot the evolution of the strategy during the learning. For instance, the package pyplot provides simple tools in this way. Let generate a plot with one points every \\(500\\) games for instance at sleep time. import matplotlib.pyplot as plt Class Bot: def __init__(self): # Progress self._results= [] self._evals= [] # Q-Learning attributes ... def sleep(self, result): self._results.append(result) if len(self._results) == 500 : self._evals.append( sum(self._results)/500.0 ) self._results= [] self.drawEvaluations() def drawEvaluations(self) : plt.plot( [ i*500 for i in range(len(self._evals)) ], self._evals ) plt.savefig( \"output.png\" ) plt.clf() It\u2019s also possible to save the QValues dictionary and the average score in a file for later use, for instance with json package. fileContent= open(\"my-file.json\", \"w\") json.dump( aDictionary, fileContent, sort_keys=True, indent=2 ) fileContent.close() Then the load function can rebuild a .json dictionary. if os.path.isfile( \"my-file.json\" ) : fileContent= open(\"my-file.json\") aDictionary = json.load(fileContent) fileContent.close() Dynamic Learning Parameters Greedy exploit/Explore ratio is limited, notably when the Q-values become stables. A first solution is to change the ratio dynamically while the systems record experiments. Typically, QLearning can be configured with a high exploration probability during the first benches of games, and decrease this probability with experience (i.e. each time the bot is wape-up again). A similar mechanism can be applied to the learning rate. The parameter of the Qlearning is not directly the Exploration/Explotation ratio and the learning rate, but the initial and final values, and it decreases speed. Factorized Policy The main idea is that a same action could be defined for several 'similar' states. So is it possible to group 'similar' states together, to learn faster. Py421 advantages the combinations 4-2-1 , 1-1-1 or X-1-1 . A first idea could be to focus state definition to target those combinations. This idea modeled as a tree will lookalike: This tree can be implemented with a \"if-then-else\" procedure into a QLearner Bot, to learn a good behavior on Py421 game only over 7 states. As a result, learning should be very fast but with a final behavior less performant than learning applied over all the 168 game states. In fact, such a solution is close to Decision Tree approaches, a decision support based on a recursive partitioning structure Wikipedia .","title":"Speed-up"},{"location":"learning/speed-up/#speed-up","text":"The advantage of Model-Based learning compared to solutions like Q-Learning is to provide some guarantees on the computed policy. However, the main goal in machine learning is to build coherent policies as fast as possible (and by doing so, to be capable of scaling up). That for, learning directly the policy is generally preferred in Reinforcement Learning. However, solutions like Q-Learning remains inapplicable in real-life problems, and solutions need to be set up to bypass combinatorial explosions.","title":"Speed-Up"},{"location":"learning/speed-up/#experiments","text":"First we need to set up an environment to save and monitor our progress in learning policies (or models). One way to do that is to plot the evolution of the strategy during the learning. For instance, the package pyplot provides simple tools in this way. Let generate a plot with one points every \\(500\\) games for instance at sleep time. import matplotlib.pyplot as plt Class Bot: def __init__(self): # Progress self._results= [] self._evals= [] # Q-Learning attributes ... def sleep(self, result): self._results.append(result) if len(self._results) == 500 : self._evals.append( sum(self._results)/500.0 ) self._results= [] self.drawEvaluations() def drawEvaluations(self) : plt.plot( [ i*500 for i in range(len(self._evals)) ], self._evals ) plt.savefig( \"output.png\" ) plt.clf() It\u2019s also possible to save the QValues dictionary and the average score in a file for later use, for instance with json package. fileContent= open(\"my-file.json\", \"w\") json.dump( aDictionary, fileContent, sort_keys=True, indent=2 ) fileContent.close() Then the load function can rebuild a .json dictionary. if os.path.isfile( \"my-file.json\" ) : fileContent= open(\"my-file.json\") aDictionary = json.load(fileContent) fileContent.close()","title":"Experiments"},{"location":"learning/speed-up/#dynamic-learning-parameters","text":"Greedy exploit/Explore ratio is limited, notably when the Q-values become stables. A first solution is to change the ratio dynamically while the systems record experiments. Typically, QLearning can be configured with a high exploration probability during the first benches of games, and decrease this probability with experience (i.e. each time the bot is wape-up again). A similar mechanism can be applied to the learning rate. The parameter of the Qlearning is not directly the Exploration/Explotation ratio and the learning rate, but the initial and final values, and it decreases speed.","title":"Dynamic Learning Parameters"},{"location":"learning/speed-up/#factorized-policy","text":"The main idea is that a same action could be defined for several 'similar' states. So is it possible to group 'similar' states together, to learn faster. Py421 advantages the combinations 4-2-1 , 1-1-1 or X-1-1 . A first idea could be to focus state definition to target those combinations. This idea modeled as a tree will lookalike: This tree can be implemented with a \"if-then-else\" procedure into a QLearner Bot, to learn a good behavior on Py421 game only over 7 states. As a result, learning should be very fast but with a final behavior less performant than learning applied over all the 168 game states. In fact, such a solution is close to Decision Tree approaches, a decision support based on a recursive partitioning structure Wikipedia .","title":"Factorized Policy"},{"location":"tiled-land/pickndel/","text":"Pick'n Del A HackableGame based on tiled-land. Install In your directory we advise to clone side by side hackagames and tiled-land . git clone https://github.com/iktorz-net/hackagames git clone https://github.com/imt-mobisyst/tiled-land Make sure you have the last version, and install them with python3 pip : git -C hackagames pull pip install ./hackagames git -C tiled-land pull pip install ./tiled-land get-started: Everything starts with a launch file. It will define a world and test a bot. As a first try, we recommend playing directly with the human interface (the player.ShellPlayer ). import tiledland.game.pickndel as pnd world= pnd.World().initializeGrid([ [00, 00, 00, 00], # 1 2 3 4 [-1, 00, -1, -1], # 5 [00, 00, 00, 00], # 6 7 8 9 [00, -1, -1, 00] #10 13 ]) master= pnd.GameMaster( world, tic=12 ) bot= pnd.player.ShellPlayer() master.testPlayer( bot, 1 ) The goal is to realize a maximum of pick and delivery missions (cf. market place area). then you can replace the ShellPlayer with your own Bot: import tiledland.game.pickndel as pnd class FirstBot(): # Constructor: def __init__(self): super().__init__() self._model= pnd.World() self._id= 0 self._tic= 0 # Player interface : def wakeUp(self, playerId, numberOfPlayers, gamePod): self._id= playerId self._model.fromPod(gamePod) def perceive(self, podState): self._tic= self._model.setOnPodState(podState) def decide(self): return \"pass\" def sleep(self, result): print( f\"End on: {result}\" ) We recommend using a _model attributes to model the Pick'n Del world (the map, the robot and the missions). Somme useful methods: carrierTile() returning the current tile position of the carrier. carrierMission() returning the current mission identifier. carrierGoal() returning the next tile to reach. More Complexity: The first increase in complexity consists of growing the size of the world. To this effect, it is possible to download a complete launch file that will start a game with different game configuration, each recorded in a json format. Archives to download: .zip The pickndel.py launch script will start a game accordingly to the configuration given as command argument: For instance python3 pickndel.py conf-pnd-medium-02.json As a first exercise, the goal is to learn, using reinforcement learning how to move in a world knowing that some of the tiles are encumbered. There is a probability to be stuck inside a tile when the carrier try a move from that tile. The encumbered tile and the associated probabilities are not known, and the smallest path is not necessarily the faster.","title":"Pick'n Del"},{"location":"tiled-land/pickndel/#pickn-del","text":"A HackableGame based on tiled-land.","title":"Pick'n Del"},{"location":"tiled-land/pickndel/#install","text":"In your directory we advise to clone side by side hackagames and tiled-land . git clone https://github.com/iktorz-net/hackagames git clone https://github.com/imt-mobisyst/tiled-land Make sure you have the last version, and install them with python3 pip : git -C hackagames pull pip install ./hackagames git -C tiled-land pull pip install ./tiled-land","title":"Install"},{"location":"tiled-land/pickndel/#get-started","text":"Everything starts with a launch file. It will define a world and test a bot. As a first try, we recommend playing directly with the human interface (the player.ShellPlayer ). import tiledland.game.pickndel as pnd world= pnd.World().initializeGrid([ [00, 00, 00, 00], # 1 2 3 4 [-1, 00, -1, -1], # 5 [00, 00, 00, 00], # 6 7 8 9 [00, -1, -1, 00] #10 13 ]) master= pnd.GameMaster( world, tic=12 ) bot= pnd.player.ShellPlayer() master.testPlayer( bot, 1 ) The goal is to realize a maximum of pick and delivery missions (cf. market place area). then you can replace the ShellPlayer with your own Bot: import tiledland.game.pickndel as pnd class FirstBot(): # Constructor: def __init__(self): super().__init__() self._model= pnd.World() self._id= 0 self._tic= 0 # Player interface : def wakeUp(self, playerId, numberOfPlayers, gamePod): self._id= playerId self._model.fromPod(gamePod) def perceive(self, podState): self._tic= self._model.setOnPodState(podState) def decide(self): return \"pass\" def sleep(self, result): print( f\"End on: {result}\" ) We recommend using a _model attributes to model the Pick'n Del world (the map, the robot and the missions). Somme useful methods: carrierTile() returning the current tile position of the carrier. carrierMission() returning the current mission identifier. carrierGoal() returning the next tile to reach.","title":"get-started:"},{"location":"tiled-land/pickndel/#more-complexity","text":"The first increase in complexity consists of growing the size of the world. To this effect, it is possible to download a complete launch file that will start a game with different game configuration, each recorded in a json format. Archives to download: .zip The pickndel.py launch script will start a game accordingly to the configuration given as command argument: For instance python3 pickndel.py conf-pnd-medium-02.json As a first exercise, the goal is to learn, using reinforcement learning how to move in a world knowing that some of the tiles are encumbered. There is a probability to be stuck inside a tile when the carrier try a move from that tile. The encumbered tile and the associated probabilities are not known, and the smallest path is not necessarily the faster.","title":"More Complexity:"}]}