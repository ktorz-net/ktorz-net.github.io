{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"An Hackable Games' Engine Welcome to the HackaGames documentation. HackaGames is an open game engine dedicated to the development of Artificial Intelligence (AI) based on Combinatorial Optimization (CO) technique. The philosophy of hackagames is to permit developers to easily work in any language of thier choice. For that, the project is based on a communication protocol relying on ZeroMQ and is developed accordingly to KISS (Keep It Stupid Simple) principle. The main feature of this project is to permit the game, players and AIs to work on their own process potentially distributed over different machines. In other terms, HackaGames implements a simple client/server architecture to permit AI to take a seat on a game by agreeing on a communication protocol. HackaGames is seen as an API for game development. Several games are proposed for example: Py421 : A very simple one player dice game to get the concept of AI implementation (not a core HackaGames client/server game). TicTacToe : classic and Ultimate TicTacToe game. Connect4 : whell, classic Connect 4 game. MoveIt : pick-up delivery optimisation game. Risky : simple turn-based strategic game. The project itself and it source code are available on github . Install Simplely clone the git repository and use python pip . git clone https://github.com/ktorz-net/hackagames pip install ./hackagames For a more detail see the install page of this tutorial. Getting Started The easiest way is to play with one of the proposed python3 games, Py421 for instance. Concurency: HackaGame is not what you are looking for ? Try those solutions: ludii a general game system designed to play, evaluate and design a wide range of games\" (JAVA) pettingzoo of farama multi-agent learning framework (Python) pommerman a hackable Bomberman game (Python) codingame web-based environment for NPC development (complete solution for one file codes). Roblox an online game platform and game creation system that allows users to program games and play games created by other users. Godot Open Source Game engine (dev in Cpp) (or even more K.I.S with RayLib - dev in C).","title":"Home"},{"location":"#an-hackable-games-engine","text":"Welcome to the HackaGames documentation. HackaGames is an open game engine dedicated to the development of Artificial Intelligence (AI) based on Combinatorial Optimization (CO) technique. The philosophy of hackagames is to permit developers to easily work in any language of thier choice. For that, the project is based on a communication protocol relying on ZeroMQ and is developed accordingly to KISS (Keep It Stupid Simple) principle. The main feature of this project is to permit the game, players and AIs to work on their own process potentially distributed over different machines. In other terms, HackaGames implements a simple client/server architecture to permit AI to take a seat on a game by agreeing on a communication protocol. HackaGames is seen as an API for game development. Several games are proposed for example: Py421 : A very simple one player dice game to get the concept of AI implementation (not a core HackaGames client/server game). TicTacToe : classic and Ultimate TicTacToe game. Connect4 : whell, classic Connect 4 game. MoveIt : pick-up delivery optimisation game. Risky : simple turn-based strategic game. The project itself and it source code are available on github .","title":"An Hackable Games' Engine"},{"location":"#install","text":"Simplely clone the git repository and use python pip . git clone https://github.com/ktorz-net/hackagames pip install ./hackagames For a more detail see the install page of this tutorial.","title":"Install"},{"location":"#getting-started","text":"The easiest way is to play with one of the proposed python3 games, Py421 for instance.","title":"Getting Started"},{"location":"#concurency","text":"HackaGame is not what you are looking for ? Try those solutions: ludii a general game system designed to play, evaluate and design a wide range of games\" (JAVA) pettingzoo of farama multi-agent learning framework (Python) pommerman a hackable Bomberman game (Python) codingame web-based environment for NPC development (complete solution for one file codes). Roblox an online game platform and game creation system that allows users to program games and play games created by other users. Godot Open Source Game engine (dev in Cpp) (or even more K.I.S with RayLib - dev in C).","title":"Concurency:"},{"location":"f.a.q/","text":"F.A.Q Are we forced to use the client-server architecture ? It is possible to shunt network architecture if the game and all the players' codes are in the same language (in python for instance). In that case, a game can generally be started in a test mode with all its players in one unique process (cf. the start-interactive scripts). For instance, for Py421 solo game, you have to edit a python script launcherPy421.py . The code requires to import the game and the players (only one here) and to instantiate them with the testPlayer method: #!env python3 from hackagames.gamePy421.gameEngine import GameSolo as Game from tutos.myPy421Bot import AutonomousPlayer as Player # Instanciate and start 100 games game= Game() player= Player() results= game.testPlayer( player, 100 ) That it, you can execute your script: python3 ./tutos/launcherPy421.py which calls your player. The second attribute in testPlayer method of game instance ( 100 here) is the number of games the players will play before the process end. Is there some courses ? yes, here: on the repo of the courses","title":"F.A.Q"},{"location":"f.a.q/#faq","text":"","title":"F.A.Q"},{"location":"f.a.q/#are-we-forced-to-use-the-client-server-architecture","text":"It is possible to shunt network architecture if the game and all the players' codes are in the same language (in python for instance). In that case, a game can generally be started in a test mode with all its players in one unique process (cf. the start-interactive scripts). For instance, for Py421 solo game, you have to edit a python script launcherPy421.py . The code requires to import the game and the players (only one here) and to instantiate them with the testPlayer method: #!env python3 from hackagames.gamePy421.gameEngine import GameSolo as Game from tutos.myPy421Bot import AutonomousPlayer as Player # Instanciate and start 100 games game= Game() player= Player() results= game.testPlayer( player, 100 ) That it, you can execute your script: python3 ./tutos/launcherPy421.py which calls your player. The second attribute in testPlayer method of game instance ( 100 here) is the number of games the players will play before the process end.","title":"Are we forced to use the client-server architecture ?"},{"location":"f.a.q/#is-there-some-courses","text":"yes, here: on the repo of the courses","title":"Is there some courses ?"},{"location":"games/connect4/","text":"Connect4 Connect4 is a HackaGames game, A simple two-player game where each player tries to align 4 of their pieces. Try the game: The play/connect4.py script starts the game with an interactive interface in a shell. python3 hackagames/play/connect4.py The player can perform one and only one action at its turn, and the game stops automatically with a winner or when no more pieces can be set on the grid. The actions consist of positioning a player's piece on one of the grid columns. There are 7 possible actions: A , B , C , D , E , F and G for the corresponding column. Example of the game at some point: A B C D E F G | | | | | | | | | | | | | | | | | | | | | | | | | | | | O | | | | | | | X | X | | | | | X | O | O | O | | | X | ----------------------------- you: O First player play 'O' and the second 'X'. The first player aligning 4 of its pieces, in any direction win the game. Initialize an Autonomous Player If you are implementing your first bot, please follow the first bot tutorial on Py421 game. The wakeUp method informs about the mode of the game, the perception of the status of the grid and the possibilities of actions. The hackagames Connect4 package includes a very useful Grid class to manipulate the game state. A minimal random Bot can be implemented in a few lines: from hacka.games.connect4.grid import Grid import random class Bot : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gamePod): assert( gamePod.family() == 'Connect4') self._playerId= playerId self._grid= Grid() def perceive(self, gameState): # update the game state: self._grid.fromPod( gameState ) def decide(self): options = self._grid.possibilities() return random.choice( options ) def sleep(self, result): pass And can be tested thanks to a very simple launcher script: # Setup a game: from hacka.games.connect4 import GameConnect4 as Game from hacka.games.connect4.firstBot import Bot as Opponent # Setup your bot: from myRandomBot import Bot bot= Bot() oppo= Opponent() # Instanciate and start 100 games game= Game() results= game.test2Players( bot, oppo, 1000 ) # Analyze the result print( f\"Average score: {sum(results[0])/len(results[0])} vs {sum(results[1])/len(results[1])}\" ) At this point, the results comparing the 2 Bot would be very close. The 2 bots have the same behavior. To notice that, the list of actions do not filter positions already taken. However in case of wrong actions, the game will ask the player for a second call. Customaize your Bot: You can explore the grid to select the best action possible. Some useful Gird methods: columnSize(self) : The number of columns ( 7 in classical configuration) heightMax(self) : The height of the grid ( 6 position per column in classical configuration) height(self, iColumn) : the actual height of the iColumn column (considered the played piece in this column) column(self, iColumn) : a list of integers modeling the iColumn column pieces ( 0 no player, 1 player 1 and 2 player 2 ) position(self, c, h) : The value at column c height h ( 0 , 1 or 2 ). To test a move you will need: copy(self) : returning a deep copy of the grid (before to make some changes) playerPlay(self, iPlayer, aLetter) : to alter the grid considering that player iPlayer play on the aLetter column. winner(self) : returns the player Id winning the game if 4 of his pieces are aligned. possibilities(self) : returns the list of possible move (letters), if the columns are not full. To notice that you can move from column letter to the column identifier and vice versa with chr (a char from an integer) and ord (the interger code of a char). For instance: iColumn= ord(aLetter)-ord('A') aLetter= chr( ord('A')+iColumn )","title":"Connect4"},{"location":"games/connect4/#connect4","text":"Connect4 is a HackaGames game, A simple two-player game where each player tries to align 4 of their pieces.","title":"Connect4"},{"location":"games/connect4/#try-the-game","text":"The play/connect4.py script starts the game with an interactive interface in a shell. python3 hackagames/play/connect4.py The player can perform one and only one action at its turn, and the game stops automatically with a winner or when no more pieces can be set on the grid. The actions consist of positioning a player's piece on one of the grid columns. There are 7 possible actions: A , B , C , D , E , F and G for the corresponding column. Example of the game at some point: A B C D E F G | | | | | | | | | | | | | | | | | | | | | | | | | | | | O | | | | | | | X | X | | | | | X | O | O | O | | | X | ----------------------------- you: O First player play 'O' and the second 'X'. The first player aligning 4 of its pieces, in any direction win the game.","title":"Try the game:"},{"location":"games/connect4/#initialize-an-autonomous-player","text":"If you are implementing your first bot, please follow the first bot tutorial on Py421 game. The wakeUp method informs about the mode of the game, the perception of the status of the grid and the possibilities of actions. The hackagames Connect4 package includes a very useful Grid class to manipulate the game state. A minimal random Bot can be implemented in a few lines: from hacka.games.connect4.grid import Grid import random class Bot : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gamePod): assert( gamePod.family() == 'Connect4') self._playerId= playerId self._grid= Grid() def perceive(self, gameState): # update the game state: self._grid.fromPod( gameState ) def decide(self): options = self._grid.possibilities() return random.choice( options ) def sleep(self, result): pass And can be tested thanks to a very simple launcher script: # Setup a game: from hacka.games.connect4 import GameConnect4 as Game from hacka.games.connect4.firstBot import Bot as Opponent # Setup your bot: from myRandomBot import Bot bot= Bot() oppo= Opponent() # Instanciate and start 100 games game= Game() results= game.test2Players( bot, oppo, 1000 ) # Analyze the result print( f\"Average score: {sum(results[0])/len(results[0])} vs {sum(results[1])/len(results[1])}\" ) At this point, the results comparing the 2 Bot would be very close. The 2 bots have the same behavior. To notice that, the list of actions do not filter positions already taken. However in case of wrong actions, the game will ask the player for a second call.","title":"Initialize an Autonomous Player"},{"location":"games/connect4/#customaize-your-bot","text":"You can explore the grid to select the best action possible. Some useful Gird methods: columnSize(self) : The number of columns ( 7 in classical configuration) heightMax(self) : The height of the grid ( 6 position per column in classical configuration) height(self, iColumn) : the actual height of the iColumn column (considered the played piece in this column) column(self, iColumn) : a list of integers modeling the iColumn column pieces ( 0 no player, 1 player 1 and 2 player 2 ) position(self, c, h) : The value at column c height h ( 0 , 1 or 2 ). To test a move you will need: copy(self) : returning a deep copy of the grid (before to make some changes) playerPlay(self, iPlayer, aLetter) : to alter the grid considering that player iPlayer play on the aLetter column. winner(self) : returns the player Id winning the game if 4 of his pieces are aligned. possibilities(self) : returns the list of possible move (letters), if the columns are not full. To notice that you can move from column letter to the column identifier and vice versa with chr (a char from an integer) and ord (the interger code of a char). For instance: iColumn= ord(aLetter)-ord('A') aLetter= chr( ord('A')+iColumn )","title":"Customaize your Bot:"},{"location":"games/moveIt/","text":"MoveIt MoveIt is a HackaGames game. MoveIt is a based on collision-free path planning problems, where several mobiles has each a position reach in a clustered environment. In the simplest form, you control a robot in an environment with humans. This tutorial is based only on Python3 . Try the game: The start-interactive script starts the game with an interactive interface in a shell. python3 hackagames/gameMoveIt/start-interactive Example of game configuration: \u2581 \u2581 \u2581 \u2581 \u2581 \u2581 \u2596\u259d\u2581\u2581\u2581\u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2584\u259f\u2588\u2588\u2588\u2599\u2584 \u2588 \u239bH \u239e \u2588 \u2588 \u2588 \u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588 \u239d 2\u23a0 \u2588 \u2588 \u2588 \u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2584\u259f\u2588\u2588\u2588\u2599\u2584\u2594\u2594\u2594\u2596\u259d\u2581\u2581\u2581\u2598\u2597 \u2584\u259f\u2588\u2588\u2588\u2599\u2584 \u2584\u259f\u2588\u2588\u2588\u2599\u2584 \u2584\u259f\u2588\u2588\u2588\u2599\u2584 \u2596\u259d \u2580\u259c\u2588\u2588\u2588\u259b\u2580 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u239bH \u239e \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u23a1 \u23a4 \u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u239d 3\u23a0 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u23a3 \u23a61\u2588 \u2580\u259c\u2588\u2588\u2588\u259b\u2580 \u2598\u2597\u2594\u2594\u2594\u2596\u259d \u2580\u259c\u2588\u2588\u2588\u259b\u2580 \u2580\u259c\u2588\u2588\u2588\u259b\u2580\u2581\u2581\u2581\u2580\u259c\u2588\u2588\u2588\u259b\u2580 \u2598\u2597 \u2596\u259d \u2598\u2597 \u2588 \u2588 \u2588 \u2588 \u239bR \u239e \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u239d 1\u23a0 \u2588 \u2588 \u2588 \u2584\u259f\u2588\u2588\u2588\u2599\u2584 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597\u2594\u2594\u2594\u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2580\u259c\u2588\u2588\u2588\u259b\u2580 \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2594 \u2594 \u2594 \u2594 \u2594 \u2594 The controlled robot is modeled as R 1 and want to reach the position labialized has 1 . The environment also includes 2 humans H 1 and H 2 with unknown goals position. The robot can move to one of the 6 cells next to its own position: move X with \\(X \\in [0, 1, 2, 3, 4, 5, 6]\\) ( 0 means the robots stay on it position). The robot can also pass . This special action makes it abandon the mission and trigger a new task to fulfill. \u2581 \u2581 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2588 \u2588 \u2588 \u2588 6 \u2588 1 \u2588 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2588 \u2588 \u239bR \u239e \u2588 \u2588 \u2588 5 \u2588 \u239d 1\u23a0 \u2588 2 \u2588 \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2588 \u2588 \u2588 \u2588 4 \u2588 3 \u2588 \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2594 \u2594 The robot has to reach a maximum of tasks before the end of the counter while avoiding any collisions with the humans. Initialize a Bot environment First we can prepare a launcher for MoveIt game. Put the next code into a file launch-moveIt.py aside hackagames directory: from hackagames.gameMoveIt.gameEngine import GameMoveIt from tutos.basicBot import Bot game= GameMoveIt() results= game.testPlayer( Bot(), 4 ) print( f\"Average score: {sum(results)/len(results)}\" ) This script import moveIt game, a player and start \\(4\\) games. It requires that you instantiate a MoveIt bot on a tutos/basicBot.py file. This class Bot should implement the wakeUp , perceive , decide and sleep methods. To simplify the development, the bot can rely on MoveIt Hexaboard and Mobile classes to model the world and the mobile elements (the robot and the \\(2\\) humans). First you have to import access to those elements: #!env python3 \"\"\" HackaGame player interface \"\"\" import sys, os sys.path.insert( 1, __file__.split('tutos')[0] ) import hackagames.gameMoveIt.gameEngine as ge A first Basic Bot: Then you have to create the require Bot attributes in the wakeUp method: # Player interface : def wakeUp(self, playerId, numberOfPlayers, gamePod): # Initialize from gamePod: self._board= ge.Hexaboard() self._board.fromPod( gamePod ) nbRobots, nbMobiles= gamePod.flag(3), gamePod.flag(4) self._mobiles= ge.defineMobiles( nbRobots, nbMobiles ) self._board.setupMobiles( self._mobiles ) # Initialize state variable: self._countTic= 0 self._countCycle= 0 self._score= 0 This way the update of the game state and the computation of a path in the environment will be very simple: def perceive(self, statePod): # update the game state: self._board.mobilesFromPod( statePod ) self._countTic= statePod.flag(1) self._countCycle= statePod.flag(2) self._score= statePod.value(1) def decide(self): action= \"move\" robot= self._mobiles[0] path= self._board.path( robot.x(), robot.y(), robot.goalx(), robot.goaly() ) dir= path[0] action+= \" \" + str(dir) return action For debugging purpose you can add print based on board method. For instance, before to return the action: print( self._board.shell() ) print( f\"counters{(self._countTic, self._countCycle)}, score({self._score})\" ) print( f\"Action: {action}\" ) A more Social Bot: You can now explore the state information to better control the robot. i.e: at least do not enter a cell already occupied by a human... MoveIt Class Accessors : Source code: bitbucket.org/imt-mobisyst/hackagames Mobile : # Accessor: def number(self): def x(self): def y(self): def position(self): def direction(self): def goal(self): def goalx(self): def goaly(self): def isRobot(self): def isHuman(self): Hexaboard : # Accessors: def size(self): # return a tuple. The number of cells in a lines and the number of lines def at(self, x, y): # return the Cell at position x, y def at_dir(self, x, y, i): # return the coordinates in the direction `dir` from the position x, y # dirx, diry= board.at_dir( x, y, dir ) def movesFrom(self, x, y): # return the possible move direction from position x, y def isCoordinate(self, x, y):# return TRUE if x, y is on the board. def cellsType(self, aType ): # return all the list of coordinates # of all the cells of a given type (Cell.TYPE_FREE or Cell.TYPE_OBSTACLES) def cellsEmpty( self ): # return all the list of coordinates # Cell.TYPE_FREE and empty cells def isObstacleOkAt(self, x, y): # return TRUE if cell at x, y is an obstacle def teleportMobile(self, x, y, tx, ty): # teleport a mobile def moveMobileAt_dir(self, x, y, dir): # move a mobile in a given direction def path(self, x1, y1, x2, y2): # compute a obstacle free path betwen a start and a target positions def shell(self): # Generate shell drawing of the game Cells : TYPE_FREE= 0 TYPE_OBSTACLE= 1 # Accessors: def mobile(self): # return the mobile in the board cell (False if no mobile present) def isObstacle(self): # if the cell is an obstacle def isAvailable(self):# if the cell is free and without a mobile present","title":"MoveIt"},{"location":"games/moveIt/#moveit","text":"MoveIt is a HackaGames game. MoveIt is a based on collision-free path planning problems, where several mobiles has each a position reach in a clustered environment. In the simplest form, you control a robot in an environment with humans. This tutorial is based only on Python3 .","title":"MoveIt"},{"location":"games/moveIt/#try-the-game","text":"The start-interactive script starts the game with an interactive interface in a shell. python3 hackagames/gameMoveIt/start-interactive Example of game configuration: \u2581 \u2581 \u2581 \u2581 \u2581 \u2581 \u2596\u259d\u2581\u2581\u2581\u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2584\u259f\u2588\u2588\u2588\u2599\u2584 \u2588 \u239bH \u239e \u2588 \u2588 \u2588 \u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588 \u239d 2\u23a0 \u2588 \u2588 \u2588 \u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2584\u259f\u2588\u2588\u2588\u2599\u2584\u2594\u2594\u2594\u2596\u259d\u2581\u2581\u2581\u2598\u2597 \u2584\u259f\u2588\u2588\u2588\u2599\u2584 \u2584\u259f\u2588\u2588\u2588\u2599\u2584 \u2584\u259f\u2588\u2588\u2588\u2599\u2584 \u2596\u259d \u2580\u259c\u2588\u2588\u2588\u259b\u2580 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u239bH \u239e \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u23a1 \u23a4 \u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u239d 3\u23a0 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u23a3 \u23a61\u2588 \u2580\u259c\u2588\u2588\u2588\u259b\u2580 \u2598\u2597\u2594\u2594\u2594\u2596\u259d \u2580\u259c\u2588\u2588\u2588\u259b\u2580 \u2580\u259c\u2588\u2588\u2588\u259b\u2580\u2581\u2581\u2581\u2580\u259c\u2588\u2588\u2588\u259b\u2580 \u2598\u2597 \u2596\u259d \u2598\u2597 \u2588 \u2588 \u2588 \u2588 \u239bR \u239e \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u239d 1\u23a0 \u2588 \u2588 \u2588 \u2584\u259f\u2588\u2588\u2588\u2599\u2584 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597\u2594\u2594\u2594\u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2580\u259c\u2588\u2588\u2588\u259b\u2580 \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2594 \u2594 \u2594 \u2594 \u2594 \u2594 The controlled robot is modeled as R 1 and want to reach the position labialized has 1 . The environment also includes 2 humans H 1 and H 2 with unknown goals position. The robot can move to one of the 6 cells next to its own position: move X with \\(X \\in [0, 1, 2, 3, 4, 5, 6]\\) ( 0 means the robots stay on it position). The robot can also pass . This special action makes it abandon the mission and trigger a new task to fulfill. \u2581 \u2581 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2588 \u2588 \u2588 \u2588 6 \u2588 1 \u2588 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2588 \u2588 \u239bR \u239e \u2588 \u2588 \u2588 5 \u2588 \u239d 1\u23a0 \u2588 2 \u2588 \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2588 \u2588 \u2588 \u2588 4 \u2588 3 \u2588 \u2598\u2597 \u2596\u259d \u2598\u2597 \u2596\u259d \u2594 \u2594 The robot has to reach a maximum of tasks before the end of the counter while avoiding any collisions with the humans.","title":"Try the game:"},{"location":"games/moveIt/#initialize-a-bot-environment","text":"First we can prepare a launcher for MoveIt game. Put the next code into a file launch-moveIt.py aside hackagames directory: from hackagames.gameMoveIt.gameEngine import GameMoveIt from tutos.basicBot import Bot game= GameMoveIt() results= game.testPlayer( Bot(), 4 ) print( f\"Average score: {sum(results)/len(results)}\" ) This script import moveIt game, a player and start \\(4\\) games. It requires that you instantiate a MoveIt bot on a tutos/basicBot.py file. This class Bot should implement the wakeUp , perceive , decide and sleep methods. To simplify the development, the bot can rely on MoveIt Hexaboard and Mobile classes to model the world and the mobile elements (the robot and the \\(2\\) humans). First you have to import access to those elements: #!env python3 \"\"\" HackaGame player interface \"\"\" import sys, os sys.path.insert( 1, __file__.split('tutos')[0] ) import hackagames.gameMoveIt.gameEngine as ge","title":"Initialize a Bot environment"},{"location":"games/moveIt/#a-first-basic-bot","text":"Then you have to create the require Bot attributes in the wakeUp method: # Player interface : def wakeUp(self, playerId, numberOfPlayers, gamePod): # Initialize from gamePod: self._board= ge.Hexaboard() self._board.fromPod( gamePod ) nbRobots, nbMobiles= gamePod.flag(3), gamePod.flag(4) self._mobiles= ge.defineMobiles( nbRobots, nbMobiles ) self._board.setupMobiles( self._mobiles ) # Initialize state variable: self._countTic= 0 self._countCycle= 0 self._score= 0 This way the update of the game state and the computation of a path in the environment will be very simple: def perceive(self, statePod): # update the game state: self._board.mobilesFromPod( statePod ) self._countTic= statePod.flag(1) self._countCycle= statePod.flag(2) self._score= statePod.value(1) def decide(self): action= \"move\" robot= self._mobiles[0] path= self._board.path( robot.x(), robot.y(), robot.goalx(), robot.goaly() ) dir= path[0] action+= \" \" + str(dir) return action For debugging purpose you can add print based on board method. For instance, before to return the action: print( self._board.shell() ) print( f\"counters{(self._countTic, self._countCycle)}, score({self._score})\" ) print( f\"Action: {action}\" )","title":"A first Basic Bot:"},{"location":"games/moveIt/#a-more-social-bot","text":"You can now explore the state information to better control the robot. i.e: at least do not enter a cell already occupied by a human...","title":"A more Social Bot:"},{"location":"games/moveIt/#moveit-class-accessors","text":"Source code: bitbucket.org/imt-mobisyst/hackagames Mobile : # Accessor: def number(self): def x(self): def y(self): def position(self): def direction(self): def goal(self): def goalx(self): def goaly(self): def isRobot(self): def isHuman(self): Hexaboard : # Accessors: def size(self): # return a tuple. The number of cells in a lines and the number of lines def at(self, x, y): # return the Cell at position x, y def at_dir(self, x, y, i): # return the coordinates in the direction `dir` from the position x, y # dirx, diry= board.at_dir( x, y, dir ) def movesFrom(self, x, y): # return the possible move direction from position x, y def isCoordinate(self, x, y):# return TRUE if x, y is on the board. def cellsType(self, aType ): # return all the list of coordinates # of all the cells of a given type (Cell.TYPE_FREE or Cell.TYPE_OBSTACLES) def cellsEmpty( self ): # return all the list of coordinates # Cell.TYPE_FREE and empty cells def isObstacleOkAt(self, x, y): # return TRUE if cell at x, y is an obstacle def teleportMobile(self, x, y, tx, ty): # teleport a mobile def moveMobileAt_dir(self, x, y, dir): # move a mobile in a given direction def path(self, x1, y1, x2, y2): # compute a obstacle free path betwen a start and a target positions def shell(self): # Generate shell drawing of the game Cells : TYPE_FREE= 0 TYPE_OBSTACLE= 1 # Accessors: def mobile(self): # return the mobile in the board cell (False if no mobile present) def isObstacle(self): # if the cell is an obstacle def isAvailable(self):# if the cell is free and without a mobile present","title":"MoveIt Class Accessors :"},{"location":"games/py421-duo/","text":"Py421, Duo Mode Py421 is an HackaGames game. After mastering the solo version of Py421 , it is times for a 2-players' confrontation. Tow player version of Py421 can be started with duo parameters. Then you require to start 2 different players in separated shells. For instance, an AI and the Shell Human interface. # In the first shell: python3 ./hackagames/gamePy421/start-server duo # In the second shell: python3 ./hackagames/gamePy421/playerFirstAI.py # In the third shell: python3 ./hackagames/connect-shell In this version of the game, it is not necessary to reach the best combination possible. Players just wan the best one between them. The score at the end would be: \\(1\\) , \\(-1\\) , or \\(0\\) depending on who win the game and if there is a winner. 2 Player Rules The Py421 is an asymmetric player game. The first player fix the game parameters for the second. The first player has an interface very similar to 421 Solo . He has 2 rolling step possibility to optimize the combination the second player has to beat. The second player, at its turn, knows the combination to beat. However, he cannot use more dices throws than the number used by the first player. In other terms, the first player fix the number of time the second player can roll again its dices. For instance, the first player gets the combination 6-5-4 at beginning and choose to keep it. The second player will not have the possibility to roll again its dices neither. 6-5-4 is not the best combination possible but the probability that player-2 get a better one with only one throw is low. Modification at Perception Step In practice, compared to 421 Solo , the player can observe the opponent dices. The gameState attribute of the perceive methods comes with a third child for the opponent dices. def perceive(self, gameState): self.horizon= gameState.child(1).flag(1) self.dices= gameState.child(2).flags() self.op_dices= gameState.child(3).flags() print( f'H: {self.horizon} DICES: {self.dices} OPPONENTS {self.op_dices}' ) To notice that opponent dices ( self.op_dices ) would be tree \\(0\\) , if the player is the first player. For the second player, self.op_dices informs on the conbinaison to beat. However, the horizon will not necessarily be \\(2\\) at it first player activation. In fact, he can see the end of the game coming while he never decide anything (the first player keep-keep-keep at its first action). Optimize your AI A good AI for 421 Solo game will not play so bad in Duo mode but should be easy to beat, in average. Starting from your 421 Solo Bot, we recommend to tunes two different decision-making methods. cp ./tutos/myPy421Bot.py ./tutos/myPy421DuoBot.py One when your player plays first trying to stop with a good enough combination with a minimum of dices throws. The second, when your player plays second, with a strategy not focused on the best combination any more, but on the probability to reach a better one compared to the opponent combination. Most of the time, a simple \\(6\\) beat a \\(5\\) .","title":"Py421, Duo Mode"},{"location":"games/py421-duo/#py421-duo-mode","text":"Py421 is an HackaGames game. After mastering the solo version of Py421 , it is times for a 2-players' confrontation. Tow player version of Py421 can be started with duo parameters. Then you require to start 2 different players in separated shells. For instance, an AI and the Shell Human interface. # In the first shell: python3 ./hackagames/gamePy421/start-server duo # In the second shell: python3 ./hackagames/gamePy421/playerFirstAI.py # In the third shell: python3 ./hackagames/connect-shell In this version of the game, it is not necessary to reach the best combination possible. Players just wan the best one between them. The score at the end would be: \\(1\\) , \\(-1\\) , or \\(0\\) depending on who win the game and if there is a winner.","title":"Py421, Duo Mode"},{"location":"games/py421-duo/#2-player-rules","text":"The Py421 is an asymmetric player game. The first player fix the game parameters for the second. The first player has an interface very similar to 421 Solo . He has 2 rolling step possibility to optimize the combination the second player has to beat. The second player, at its turn, knows the combination to beat. However, he cannot use more dices throws than the number used by the first player. In other terms, the first player fix the number of time the second player can roll again its dices. For instance, the first player gets the combination 6-5-4 at beginning and choose to keep it. The second player will not have the possibility to roll again its dices neither. 6-5-4 is not the best combination possible but the probability that player-2 get a better one with only one throw is low.","title":"2 Player Rules"},{"location":"games/py421-duo/#modification-at-perception-step","text":"In practice, compared to 421 Solo , the player can observe the opponent dices. The gameState attribute of the perceive methods comes with a third child for the opponent dices. def perceive(self, gameState): self.horizon= gameState.child(1).flag(1) self.dices= gameState.child(2).flags() self.op_dices= gameState.child(3).flags() print( f'H: {self.horizon} DICES: {self.dices} OPPONENTS {self.op_dices}' ) To notice that opponent dices ( self.op_dices ) would be tree \\(0\\) , if the player is the first player. For the second player, self.op_dices informs on the conbinaison to beat. However, the horizon will not necessarily be \\(2\\) at it first player activation. In fact, he can see the end of the game coming while he never decide anything (the first player keep-keep-keep at its first action).","title":"Modification at Perception Step"},{"location":"games/py421-duo/#optimize-your-ai","text":"A good AI for 421 Solo game will not play so bad in Duo mode but should be easy to beat, in average. Starting from your 421 Solo Bot, we recommend to tunes two different decision-making methods. cp ./tutos/myPy421Bot.py ./tutos/myPy421DuoBot.py One when your player plays first trying to stop with a good enough combination with a minimum of dices throws. The second, when your player plays second, with a strategy not focused on the best combination any more, but on the probability to reach a better one compared to the opponent combination. Most of the time, a simple \\(6\\) beat a \\(5\\) .","title":"Optimize your AI"},{"location":"games/py421-solo/","text":"Py421, Solo Mode Py421 is an HackaGames game, a simple dice game where players try to optimize their dices combination after a maximum of 2 roll dice steps. This tutorial is based only on Python3 . Try the game: The play/py421 script starts the game with an interactive interface in a shell. python3 hackagames/play/py421 Py421 is a 3-dice game. The player can roll several times to get a combination. The player can perform one and only one action on his turn, and the game stops automatically after 2 turns. The actions consist in keeping or rolling each of the 3 dices. So there are 8 possible actions: keep-keep-keep , keep-keep-roll , keep-roll-keep , keep-roll-roll , roll-keep-keep , roll-keep-roll , roll-roll-keep and roll-roll-roll The goal is to optimize the combination of dices before the end of the 2 turns. The best combination ever is 4-2-1 for 800 points. But you can explore other combinations. Initialize an Autonomous Player In your workspace, we encourage you to create a new directory for your experiences linked to our tutorials ( tutos for instance, aside from hackagames directory), and to create your new Py421 player in this directory. mkdir tutos touch tutos/myPy421Bot.py You now have to edit myPy421Bot.py script and create a Py421 player. Player Script Architecture The script must begin by importing hackagames elements ( hackapy ) and implement an hackagames Abstract Player . To import hackapy you have first to modify the python path resource to add your workspace directory (i.e. the directory including tutos in which your AI is positioned). # Local HackaGame: import sys sys.path.insert( 1, __file__.split('tutos')[0] ) import hackagames.hackapy as hg The first proposed script selects a random action and also requires the adequate python tool: import random Then your first player will inherit from hackay Abstract Player and implement the 4 player methods wakeUp , perceive , decide and sleep required to play any Hackagames's game : class AutonomousPlayer( hg.AbsPlayer ) : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): [...] def perceive(self, gameState): [...] def decide(self): [...] def sleep(self, result): [...] It is possible to add an extra \\(4\\) lines to permit myPy421Bot.py file to connect a player when it is executed. This script section is activated only if the file is the main Python entrance. When you interpret it python3 myPy421Bot.py rather than import it into another code. # script : if __name__ == '__main__' : player= AutonomousPlayer() results= player.takeASeat() print( f\"Results: {results}\" ) A Bot for Py421 Solo A first version of myPy421Bot.py player could be copied from the hackagames/gamePy421/playerFirstAI , and your final first script would be: # Local HackaGame: import sys sys.path.insert( 1, __file__.split('tutos')[0] ) import hackagames.hackapy as hg import random class AutonomousPlayer( hg.AbsPlayer ) : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): print( f'---\\nWake-up player-{playerId} ({numberOfPlayers} players)') print( gameConf ) def perceive(self, gameState): self.horizon= gameState.child(1).flag(1) self.dices= gameState.child(2).flags() print( f'H: {self.horizon} DICES: {self.dices}' ) def decide(self): actions= ['keep-keep-keep', 'keep-keep-roll', 'keep-roll-keep', 'keep-roll-roll', 'roll-keep-keep', 'roll-keep-roll', 'roll-roll-keep', 'roll-roll-roll' ] action= random.choice( actions ) print( f'Action: {action}' ) return action def sleep(self, result): print( f'--- Results: {str(result)}' ) # script : if __name__ == '__main__' : player= AutonomousPlayer() results= player.takeASeat() print( f\"Results: {results}\" ) Test your Player HackaGames is designed to work as a client-server architecture to make the game and the players completely independent. Each one in it own process and potentially on different machines. So the classical way to test your autonomous player is to start a game server then to launch your bot in separated shells. # In a first shell: python3 hackagames/gamePy421/start-server # In a second shell: python3 tutos/myPy421Bot.py Make Your Own Autonomous Player: To notice that method parameters are referencing hackapy objects (Pod) not presented in this tutorial. The perception already records the game variables horizon (the number of remaining roll-again) and dices (a list of the 3 dices values) into instance attributes. Actually in decide method, an action is chosen randomly, the goal now is to propose heuristic choice of actions to optimize average score over 10000 games. As a minimal start, it is possible to force a stop action (action keep-keep-keep ) each time the player reaches a good combination ( 4-2-1 or 1-1-1 for instance). The takeASeat method return the list game results. We can now print and analyze the reached results (compute the average score for instance): # Analysis average= sum(results)/len(results) print( f\"Average score: {average}\") A game server command takes a specific number of games to play with the option -n : # In a first shell: python3 hackagames/gamePy421/start-server -n 10000 As a minimal start, it is possible to force a stop action (action keep-keep-keep ) each time the player reaches a good combination ( 4-2-1 or 1-1-1 for instance). Then you can progressively try to optimize the decisions in order to target those 2 combinations.","title":"Py421, Solo Mode"},{"location":"games/py421-solo/#py421-solo-mode","text":"Py421 is an HackaGames game, a simple dice game where players try to optimize their dices combination after a maximum of 2 roll dice steps. This tutorial is based only on Python3 .","title":"Py421, Solo Mode"},{"location":"games/py421-solo/#try-the-game","text":"The play/py421 script starts the game with an interactive interface in a shell. python3 hackagames/play/py421 Py421 is a 3-dice game. The player can roll several times to get a combination. The player can perform one and only one action on his turn, and the game stops automatically after 2 turns. The actions consist in keeping or rolling each of the 3 dices. So there are 8 possible actions: keep-keep-keep , keep-keep-roll , keep-roll-keep , keep-roll-roll , roll-keep-keep , roll-keep-roll , roll-roll-keep and roll-roll-roll The goal is to optimize the combination of dices before the end of the 2 turns. The best combination ever is 4-2-1 for 800 points. But you can explore other combinations.","title":"Try the game:"},{"location":"games/py421-solo/#initialize-an-autonomous-player","text":"In your workspace, we encourage you to create a new directory for your experiences linked to our tutorials ( tutos for instance, aside from hackagames directory), and to create your new Py421 player in this directory. mkdir tutos touch tutos/myPy421Bot.py You now have to edit myPy421Bot.py script and create a Py421 player.","title":"Initialize an Autonomous Player"},{"location":"games/py421-solo/#player-script-architecture","text":"The script must begin by importing hackagames elements ( hackapy ) and implement an hackagames Abstract Player . To import hackapy you have first to modify the python path resource to add your workspace directory (i.e. the directory including tutos in which your AI is positioned). # Local HackaGame: import sys sys.path.insert( 1, __file__.split('tutos')[0] ) import hackagames.hackapy as hg The first proposed script selects a random action and also requires the adequate python tool: import random Then your first player will inherit from hackay Abstract Player and implement the 4 player methods wakeUp , perceive , decide and sleep required to play any Hackagames's game : class AutonomousPlayer( hg.AbsPlayer ) : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): [...] def perceive(self, gameState): [...] def decide(self): [...] def sleep(self, result): [...] It is possible to add an extra \\(4\\) lines to permit myPy421Bot.py file to connect a player when it is executed. This script section is activated only if the file is the main Python entrance. When you interpret it python3 myPy421Bot.py rather than import it into another code. # script : if __name__ == '__main__' : player= AutonomousPlayer() results= player.takeASeat() print( f\"Results: {results}\" )","title":"Player Script Architecture"},{"location":"games/py421-solo/#a-bot-for-py421-solo","text":"A first version of myPy421Bot.py player could be copied from the hackagames/gamePy421/playerFirstAI , and your final first script would be: # Local HackaGame: import sys sys.path.insert( 1, __file__.split('tutos')[0] ) import hackagames.hackapy as hg import random class AutonomousPlayer( hg.AbsPlayer ) : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): print( f'---\\nWake-up player-{playerId} ({numberOfPlayers} players)') print( gameConf ) def perceive(self, gameState): self.horizon= gameState.child(1).flag(1) self.dices= gameState.child(2).flags() print( f'H: {self.horizon} DICES: {self.dices}' ) def decide(self): actions= ['keep-keep-keep', 'keep-keep-roll', 'keep-roll-keep', 'keep-roll-roll', 'roll-keep-keep', 'roll-keep-roll', 'roll-roll-keep', 'roll-roll-roll' ] action= random.choice( actions ) print( f'Action: {action}' ) return action def sleep(self, result): print( f'--- Results: {str(result)}' ) # script : if __name__ == '__main__' : player= AutonomousPlayer() results= player.takeASeat() print( f\"Results: {results}\" )","title":"A Bot for Py421 Solo"},{"location":"games/py421-solo/#test-your-player","text":"HackaGames is designed to work as a client-server architecture to make the game and the players completely independent. Each one in it own process and potentially on different machines. So the classical way to test your autonomous player is to start a game server then to launch your bot in separated shells. # In a first shell: python3 hackagames/gamePy421/start-server # In a second shell: python3 tutos/myPy421Bot.py","title":"Test your Player"},{"location":"games/py421-solo/#make-your-own-autonomous-player","text":"To notice that method parameters are referencing hackapy objects (Pod) not presented in this tutorial. The perception already records the game variables horizon (the number of remaining roll-again) and dices (a list of the 3 dices values) into instance attributes. Actually in decide method, an action is chosen randomly, the goal now is to propose heuristic choice of actions to optimize average score over 10000 games. As a minimal start, it is possible to force a stop action (action keep-keep-keep ) each time the player reaches a good combination ( 4-2-1 or 1-1-1 for instance). The takeASeat method return the list game results. We can now print and analyze the reached results (compute the average score for instance): # Analysis average= sum(results)/len(results) print( f\"Average score: {average}\") A game server command takes a specific number of games to play with the option -n : # In a first shell: python3 hackagames/gamePy421/start-server -n 10000 As a minimal start, it is possible to force a stop action (action keep-keep-keep ) each time the player reaches a good combination ( 4-2-1 or 1-1-1 for instance). Then you can progressively try to optimize the decisions in order to target those 2 combinations.","title":"Make Your Own Autonomous Player:"},{"location":"games/py421/","text":"Py421 Py421 is an HackaGames game, a simple dice game where players try to optimize their dices combination after a maximum of 2 roll dice steps. Try the game: The play/py421.py script starts the game with an interactive interface in a shell. python3 hackagames/play/py421.py Py421 is a 3-dice game. The player can roll several times to get a combination. The player can perform one and only one action on his turn, and the game stops automatically after 2 turns. The actions consist in keeping or rolling each of the 3 dices. So there are 8 possible actions: keep-keep-keep , keep-keep-roll , keep-roll-keep , keep-roll-roll , roll-keep-keep , roll-keep-roll , roll-roll-keep and roll-roll-roll The goal is to optimize the combination of dices before the end of the 2 turns. The best combination ever is 4-2-1 for 800 points. But you can explore other combinations. Initialize a Bot If you are implementing your first bot, please follow the first bot tutorial. As for any Hackagames bots, a Py421 bot implements the the 4 player/bot methods wakeUp , perceive , decide and sleep . Theire is no information to get from The gameConf . The gameState parameter of the perceive includes 2 elements (2 gameState childs). First child model the roll-again horizon (one integer value), the second child the dices (3 interger value and a floating point value for the score of the combinaison). import random class Bot : def actions(self): return [ 'keep-keep-keep', 'keep-keep-roll', 'keep-roll-keep', 'keep-roll-roll', 'roll-keep-keep', 'roll-keep-roll', 'roll-roll-keep', 'roll-roll-roll' ] # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): self._horizon= 3 self._dices= [0, 0, 0] self._score= 0.0 def perceive(self, gameState): self._horizon= gameState.child(1).flag(1) self._dices= gameState.child(2).flags() self._score= gameState.child(2).value(1) def decide(self): return random.choice( self.actions() ) def sleep(self, result): print( \"> result: \"+ str(result) ) You can try your Bot AI by importing GameSolo into a launch script: # Setup a game: from hacka.games.py421 import GameDuo as Game from py421Bot import Bot bot, opponent= Bot() # Instanciate and start 100 games game= Game() results= game.testPlayer( bot, 10 ) # Analyze the result print( f\"Average score: {sum(results)/len(results)}\" ) The testPlayer method returns the results for your specific player ( bot in this script). Multi-player mode It is possible to start 421 in duo mode. To try this version executes the following commad: python3 hackagames/play/py421 duo -n 2 That start a 2 games 4.2.1 where you have to confront the random player. The first game you play as the first player role. You control the number of roll-again. The opponent will not be capable to roll its dice more time than you. The second game, you play the second player role. You know the combinaision to beat. Confront your Bots You can confront 2 Bot AIs by importing GameDuo rather than GameSolo into your launch script: # Setup a game: from hacka.games.py421 import GameDuo as Game from hacka.games.py421.firstBot import Bot as Bot1 from py421Bot import Bot2 # Instanciate and start 100 games game= Game() results= game.launch( [Bot1(), Bot1()], 10 ) # Analyze the result print( f\"Average score: {sum(results)/len(results)}\" ) The launch method returns the results for all players. Optimize a 2-player Bot : A Bot designed for solo mode of 421 can also play to duo mode. However, the wakeUp methods informative and the perception method is incresed in duo mode. The player identifier of the wakeUp inform if the bot is the first or the second player. The gameState also include the conbinaison of your opponent: def perceive(self, gameState): self._horizon= gameState.child(1).flag(1) self._dices= gameState.child(2).flags() self._score= gameState.child(2).value(1) self._oppo_dices= gameState.child(3).flags() self._oppo_score= gameState.child(3).value(1) print( f'\\t> H: {self._horizon}, DICES: {self._dices}, SCORE: {self._score} (VS: {self._oppo_dices} {self._oppo_score})' ) Use this increased state definition to optimize the bot's strategies.","title":"Py4.2.1"},{"location":"games/py421/#py421","text":"Py421 is an HackaGames game, a simple dice game where players try to optimize their dices combination after a maximum of 2 roll dice steps.","title":"Py421"},{"location":"games/py421/#try-the-game","text":"The play/py421.py script starts the game with an interactive interface in a shell. python3 hackagames/play/py421.py Py421 is a 3-dice game. The player can roll several times to get a combination. The player can perform one and only one action on his turn, and the game stops automatically after 2 turns. The actions consist in keeping or rolling each of the 3 dices. So there are 8 possible actions: keep-keep-keep , keep-keep-roll , keep-roll-keep , keep-roll-roll , roll-keep-keep , roll-keep-roll , roll-roll-keep and roll-roll-roll The goal is to optimize the combination of dices before the end of the 2 turns. The best combination ever is 4-2-1 for 800 points. But you can explore other combinations.","title":"Try the game:"},{"location":"games/py421/#initialize-a-bot","text":"If you are implementing your first bot, please follow the first bot tutorial. As for any Hackagames bots, a Py421 bot implements the the 4 player/bot methods wakeUp , perceive , decide and sleep . Theire is no information to get from The gameConf . The gameState parameter of the perceive includes 2 elements (2 gameState childs). First child model the roll-again horizon (one integer value), the second child the dices (3 interger value and a floating point value for the score of the combinaison). import random class Bot : def actions(self): return [ 'keep-keep-keep', 'keep-keep-roll', 'keep-roll-keep', 'keep-roll-roll', 'roll-keep-keep', 'roll-keep-roll', 'roll-roll-keep', 'roll-roll-roll' ] # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): self._horizon= 3 self._dices= [0, 0, 0] self._score= 0.0 def perceive(self, gameState): self._horizon= gameState.child(1).flag(1) self._dices= gameState.child(2).flags() self._score= gameState.child(2).value(1) def decide(self): return random.choice( self.actions() ) def sleep(self, result): print( \"> result: \"+ str(result) ) You can try your Bot AI by importing GameSolo into a launch script: # Setup a game: from hacka.games.py421 import GameDuo as Game from py421Bot import Bot bot, opponent= Bot() # Instanciate and start 100 games game= Game() results= game.testPlayer( bot, 10 ) # Analyze the result print( f\"Average score: {sum(results)/len(results)}\" ) The testPlayer method returns the results for your specific player ( bot in this script).","title":"Initialize a Bot"},{"location":"games/py421/#multi-player-mode","text":"It is possible to start 421 in duo mode. To try this version executes the following commad: python3 hackagames/play/py421 duo -n 2 That start a 2 games 4.2.1 where you have to confront the random player. The first game you play as the first player role. You control the number of roll-again. The opponent will not be capable to roll its dice more time than you. The second game, you play the second player role. You know the combinaision to beat.","title":"Multi-player mode"},{"location":"games/py421/#confront-your-bots","text":"You can confront 2 Bot AIs by importing GameDuo rather than GameSolo into your launch script: # Setup a game: from hacka.games.py421 import GameDuo as Game from hacka.games.py421.firstBot import Bot as Bot1 from py421Bot import Bot2 # Instanciate and start 100 games game= Game() results= game.launch( [Bot1(), Bot1()], 10 ) # Analyze the result print( f\"Average score: {sum(results)/len(results)}\" ) The launch method returns the results for all players.","title":"Confront your Bots"},{"location":"games/py421/#optimize-a-2-player-bot","text":"A Bot designed for solo mode of 421 can also play to duo mode. However, the wakeUp methods informative and the perception method is incresed in duo mode. The player identifier of the wakeUp inform if the bot is the first or the second player. The gameState also include the conbinaison of your opponent: def perceive(self, gameState): self._horizon= gameState.child(1).flag(1) self._dices= gameState.child(2).flags() self._score= gameState.child(2).value(1) self._oppo_dices= gameState.child(3).flags() self._oppo_score= gameState.child(3).value(1) print( f'\\t> H: {self._horizon}, DICES: {self._dices}, SCORE: {self._score} (VS: {self._oppo_dices} {self._oppo_score})' ) Use this increased state definition to optimize the bot's strategies.","title":"Optimize a 2-player Bot :"},{"location":"games/risky/","text":"Risky Risky is a HackaGames strategic turn-based game where two armies (or more) fights for a territory. Try the game: The play/risky.py script starts the game with an interactive interface in a shell playing against an artificial player ( firstBot.py ) playing randomly. python3 hackagames/play/risky.py The world is composed by interconnected nodes forming a tabletop as, for instance .' '. | | '. .3 / \\ .' '. .' '. | |-----| | '. .1 '. .2 \\ / .' '. | | '. .4 The 2 players are referenced as a player A and player B ; starting respectively in positions 1 and 2. When an army is on a node, the information is presented as below: .'A'. # Player ID |1- 12| # army action and force '. .4 # node ID In this example, an army of player A is on node 4 . This army has 1 action-point and is composed by 12 soldiers. Each army has 2 main attributes: its action counter (the number of action it can perform - max 2) its force (the size of the army - max 24) At its turn the player can make several actions (in the limit of action counters): Moving: move X Y FORCE to move FORCE units from cell X to cell Y Growing: grow X to grow the army on nodes X . The increase of the army is depending on the initial army size and the neighbors (the number of connected owned nodes). Sleeping: sleep to increase the action counter by one for all the armies. To notice that a moving action that will move an army toward an adversarial node trigger a fight. Iteratively, each force point of the attack and the defense has a chance to deal one damage. Defenses have an increased chance than attackers. However if the attack is greater than the defense than each extra point count double. The fight is running until one of the army is destroyed. For instance, with a move 1 2 10 with a defense of 8 on the node 2 , the fight will start by considering an attack force of 12 ( \\(2\\times 10-8\\) ) times 1 chance over 2 against a defense of 8 times 2 chances over 3. The exact amount of damages at the end of the fight remains uncertain. Initialize a Bot If you are implementing your first bot, please follow the first bot tutorial on Py421 game. The wakeUp method informs about the mode of the game and more importantly the tabletop structure, the perception of the status of each army on the tabletop. It is possible to manage a copy of Riscky game locally for helping decision-making. A minimal random Bot can be implemented in a few lines: from hacka.games.risky import GameRisky import random class Bot : # Player interface : def wakeUp(self, iPlayer, numberOfPlayers, gameConf): self._playerId= chr( ord(\"A\")+iPlayer-1 ) self._game= GameRisky().fromPod( gameConf ) def perceive(self, gameState): self._game.fromPod( gameState ) def decide(self): actions= self._game.searchReadyActions( self._playerId ) return random.choice( actions ) def sleep(self, result): pass And can be tested thanks to a very simple launcher script: #!python3 # Setup a game: from hacka.games.risky import GameRisky as Game from hacka.games.risky.firstBot import Bot as Opponent # Setup your bot: from randomBot import Bot bot= Bot() oppo= Opponent() # Instanciate and start 100 games game= Game() results= game.test2Players( bot, oppo, 100 ) # Analyze the result print( f\"Average score: {sum(results[0])/len(results[0])} vs {sum(results[1])/len(results[1])}\" ) At this point, the results comparing the 2 Bots would be very close. The difference comes from the fact firstBot Bot has a decreased probability to choose a move action. Customaize your AI: To customize your AI you can use the game engine copy (cf. risky.py ) Somme of the available methods: def update( self, board ): # Update the board from the perception. def searchActions(self, playerId): # List all the current possible actions from the configuration of the armies. An action is a list ['actionName', nodeId, targetNodeId, maxForce]. targetNodeId and maxForce exist only for 'move' actions... def searchReadyActions(self, playerId): # List a limited set of ready to use actions. def cellIds(self): # return the list of cell identifiers def edgesFrom(self, iCell): # return the list of connected cell identifiers from the iCell cell. def armyOn(self, iCell) : # return an army as a Pod object, if an army is on the iCell cell (and False otherwise). def playerLetter(self, iPlayer): # return the player letter (A, B, C ...) of the ith player (1, 2, ...) An army is a Pod object where the owner is recorded in the status and the 2 attributes is for action counter and force : for iCell in self._game.cellIds() : army= self._game.armyOn(iCell) # The army on the cell 1 if army : owner= army.status() action= army.flag(1) force= army.flag(2) print( f\"Army-{owner} ({action}, {force}) on {iCells}\" ) It is also possible to print the game state thanks to ViewerTerminal tool. from hacka.games.risky import ViewerTerminal ... view= ViewerTerminal( self._game ) view.print(self._playerId) The goal now is to compute that information in order to propose an AI winning firstBot Bots ( ReadyBot , Bot and MetaBot ).","title":"Risky"},{"location":"games/risky/#risky","text":"Risky is a HackaGames strategic turn-based game where two armies (or more) fights for a territory.","title":"Risky"},{"location":"games/risky/#try-the-game","text":"The play/risky.py script starts the game with an interactive interface in a shell playing against an artificial player ( firstBot.py ) playing randomly. python3 hackagames/play/risky.py The world is composed by interconnected nodes forming a tabletop as, for instance .' '. | | '. .3 / \\ .' '. .' '. | |-----| | '. .1 '. .2 \\ / .' '. | | '. .4 The 2 players are referenced as a player A and player B ; starting respectively in positions 1 and 2. When an army is on a node, the information is presented as below: .'A'. # Player ID |1- 12| # army action and force '. .4 # node ID In this example, an army of player A is on node 4 . This army has 1 action-point and is composed by 12 soldiers. Each army has 2 main attributes: its action counter (the number of action it can perform - max 2) its force (the size of the army - max 24) At its turn the player can make several actions (in the limit of action counters): Moving: move X Y FORCE to move FORCE units from cell X to cell Y Growing: grow X to grow the army on nodes X . The increase of the army is depending on the initial army size and the neighbors (the number of connected owned nodes). Sleeping: sleep to increase the action counter by one for all the armies. To notice that a moving action that will move an army toward an adversarial node trigger a fight. Iteratively, each force point of the attack and the defense has a chance to deal one damage. Defenses have an increased chance than attackers. However if the attack is greater than the defense than each extra point count double. The fight is running until one of the army is destroyed. For instance, with a move 1 2 10 with a defense of 8 on the node 2 , the fight will start by considering an attack force of 12 ( \\(2\\times 10-8\\) ) times 1 chance over 2 against a defense of 8 times 2 chances over 3. The exact amount of damages at the end of the fight remains uncertain.","title":"Try the game:"},{"location":"games/risky/#initialize-a-bot","text":"If you are implementing your first bot, please follow the first bot tutorial on Py421 game. The wakeUp method informs about the mode of the game and more importantly the tabletop structure, the perception of the status of each army on the tabletop. It is possible to manage a copy of Riscky game locally for helping decision-making. A minimal random Bot can be implemented in a few lines: from hacka.games.risky import GameRisky import random class Bot : # Player interface : def wakeUp(self, iPlayer, numberOfPlayers, gameConf): self._playerId= chr( ord(\"A\")+iPlayer-1 ) self._game= GameRisky().fromPod( gameConf ) def perceive(self, gameState): self._game.fromPod( gameState ) def decide(self): actions= self._game.searchReadyActions( self._playerId ) return random.choice( actions ) def sleep(self, result): pass And can be tested thanks to a very simple launcher script: #!python3 # Setup a game: from hacka.games.risky import GameRisky as Game from hacka.games.risky.firstBot import Bot as Opponent # Setup your bot: from randomBot import Bot bot= Bot() oppo= Opponent() # Instanciate and start 100 games game= Game() results= game.test2Players( bot, oppo, 100 ) # Analyze the result print( f\"Average score: {sum(results[0])/len(results[0])} vs {sum(results[1])/len(results[1])}\" ) At this point, the results comparing the 2 Bots would be very close. The difference comes from the fact firstBot Bot has a decreased probability to choose a move action.","title":"Initialize a Bot"},{"location":"games/risky/#customaize-your-ai","text":"To customize your AI you can use the game engine copy (cf. risky.py ) Somme of the available methods: def update( self, board ): # Update the board from the perception. def searchActions(self, playerId): # List all the current possible actions from the configuration of the armies. An action is a list ['actionName', nodeId, targetNodeId, maxForce]. targetNodeId and maxForce exist only for 'move' actions... def searchReadyActions(self, playerId): # List a limited set of ready to use actions. def cellIds(self): # return the list of cell identifiers def edgesFrom(self, iCell): # return the list of connected cell identifiers from the iCell cell. def armyOn(self, iCell) : # return an army as a Pod object, if an army is on the iCell cell (and False otherwise). def playerLetter(self, iPlayer): # return the player letter (A, B, C ...) of the ith player (1, 2, ...) An army is a Pod object where the owner is recorded in the status and the 2 attributes is for action counter and force : for iCell in self._game.cellIds() : army= self._game.armyOn(iCell) # The army on the cell 1 if army : owner= army.status() action= army.flag(1) force= army.flag(2) print( f\"Army-{owner} ({action}, {force}) on {iCells}\" ) It is also possible to print the game state thanks to ViewerTerminal tool. from hacka.games.risky import ViewerTerminal ... view= ViewerTerminal( self._game ) view.print(self._playerId) The goal now is to compute that information in order to propose an AI winning firstBot Bots ( ReadyBot , Bot and MetaBot ).","title":"Customaize your AI:"},{"location":"games/tictactoe/","text":"TicTacToe TicTacToe is a HackaGames game, a simple two-player game where each player tries to align trees of their pieces. It comes with two modes: classic and ultimate . Try the game: The play/tictactoe.py script starts the game with an interactive interface in a shell. python3 hackagames/play/tictactoe.py In ticTacToe , the players can perform one and only one action at its turn, and the game stops automatically with a winner or when no more pieces can be set on the tabletop. The actions consist of positioning a player's piece on the grid of the form: coordinateLetter-coordinateNumber . There are 3 times 3 actions in classic mode: A-1 , A-2 , A-3 , B-1 , B-2 , B-3 , C-1 , C-2 and C-3 . Example of grid at some point: x: A B C 1 x 2 o 3 o x The letter at the up-left corner informs about the player pieces ('x' or 'o'). The first player aligning 3 of its pieces win the game. Ultimate TicTacToe The ultimate mode is a hierarchical 2-levels TicTacToe . It is possible to activate the mode with a command parameter: python3 hackagames/play/tictactoe.py ultimate The grid is composed of 9 times 9 cells, so potentially 9 times 9 actions for the players: A-1 , A-2 , ... , A-9 , B-1 , ... , `I-9. In practice, most of the time, only a sub-number of actions are available. The particularity in hierarchical TicTacToe is that, the piece position taken by a player in the local grid indicates the next grid to play for the next player turn. Example of grid at some point: x: A B C D E F G H I 1 x | | 2 | | 3 o | | -------|-------|------- 4 | | 5 | | 6 | o | -------|-------|------- 7 | | 8 | x | 9 | | actions: D:F-7:9 The line actions: D:F-7:9 indicate that it is possible to play in any free position between D and F and 7 and 9 , so : D-7 , D-8 , D-9 , E-7 , E-9 , F-7 , F-8 or F-9 . At the beginning of the game, it is possible to play in a corner grid, a side grid or the center grid. That for the action line indicates: actions: A:C-1:3, A:C-4:6, D:F-4:6 The players have to win 3 aligned classic grids to win a Ultimate TicTacToe . Initialize a Bot If you are implementing your first bot, please follow the first bot tutorial on Py421 game. The wakeUp method informs about the mode of the game, the perception of the status of the grid and the possibilities of actions. The hackagames Tictactoe package includes a very useful Grid class to manipulate the game state. A minimal random Bot can be implemented in a few lines: from hacka.games.tictactoe.grid import Grid import random class Bot : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gamePod ): assert( gamePod.family() == 'TicTacToe') assert( gamePod.status() in ['Classic', 'Ultimate'] ) self._playerId= playerId self._grid= Grid( gamePod.status() ) self._possibilities= [1] def perceive(self, gameState): # Update the grid: self._grid.update( gameState.children()[:-1] ) self._possibilities= gameState.children()[-1].flags() def decide(self): # Get all actions actions= self._grid.possibleActions( self._possibilities ) # Select one return random.choice( actions ) def sleep(self, result): pass And can be tested thanks to a very simple launcher script: # Setup a game: from hacka.games.tictactoe import GameTTT as Game from hacka.games.tictactoe.firstBot import Bot as Opponent # Setup your bot: from myRandomBot import Bot bot= Bot() oppo= Opponent() # Instanciate and start 100 games game= Game(\"classic\") # can be set to \"ultimate\" results= game.test2Players( bot, oppo, 1000 ) # Analyze the result print( f\"Average score: {sum(results[0])/len(results[0])} vs {sum(results[1])/len(results[1])}\" ) At this point, the results comparing the 2 Bot would be very close. The 2 bots have the same behavior. To notice that, the list of actions do not filter positions already taken. However in case of wrong actions, the game will ask the player for a second call. Customize your Bot: The Grid object provide for two main methods: at(abs, ord) , returning a cell value (for instance grid.at('A', 2) \\(\\rightarrow\\) 0 if the cell is empty) and at_set(abs, ord, aValue) to set a specific value on a specific cell. From that point you can crate your own evaluation of the grid and which action is interesting. You can also import the tictactoe/engine class for a more complete toolbox (test actions, etc.)","title":"TicTacToe"},{"location":"games/tictactoe/#tictactoe","text":"TicTacToe is a HackaGames game, a simple two-player game where each player tries to align trees of their pieces. It comes with two modes: classic and ultimate .","title":"TicTacToe"},{"location":"games/tictactoe/#try-the-game","text":"The play/tictactoe.py script starts the game with an interactive interface in a shell. python3 hackagames/play/tictactoe.py In ticTacToe , the players can perform one and only one action at its turn, and the game stops automatically with a winner or when no more pieces can be set on the tabletop. The actions consist of positioning a player's piece on the grid of the form: coordinateLetter-coordinateNumber . There are 3 times 3 actions in classic mode: A-1 , A-2 , A-3 , B-1 , B-2 , B-3 , C-1 , C-2 and C-3 . Example of grid at some point: x: A B C 1 x 2 o 3 o x The letter at the up-left corner informs about the player pieces ('x' or 'o'). The first player aligning 3 of its pieces win the game.","title":"Try the game:"},{"location":"games/tictactoe/#ultimate-tictactoe","text":"The ultimate mode is a hierarchical 2-levels TicTacToe . It is possible to activate the mode with a command parameter: python3 hackagames/play/tictactoe.py ultimate The grid is composed of 9 times 9 cells, so potentially 9 times 9 actions for the players: A-1 , A-2 , ... , A-9 , B-1 , ... , `I-9. In practice, most of the time, only a sub-number of actions are available. The particularity in hierarchical TicTacToe is that, the piece position taken by a player in the local grid indicates the next grid to play for the next player turn. Example of grid at some point: x: A B C D E F G H I 1 x | | 2 | | 3 o | | -------|-------|------- 4 | | 5 | | 6 | o | -------|-------|------- 7 | | 8 | x | 9 | | actions: D:F-7:9 The line actions: D:F-7:9 indicate that it is possible to play in any free position between D and F and 7 and 9 , so : D-7 , D-8 , D-9 , E-7 , E-9 , F-7 , F-8 or F-9 . At the beginning of the game, it is possible to play in a corner grid, a side grid or the center grid. That for the action line indicates: actions: A:C-1:3, A:C-4:6, D:F-4:6 The players have to win 3 aligned classic grids to win a Ultimate TicTacToe .","title":"Ultimate TicTacToe"},{"location":"games/tictactoe/#initialize-a-bot","text":"If you are implementing your first bot, please follow the first bot tutorial on Py421 game. The wakeUp method informs about the mode of the game, the perception of the status of the grid and the possibilities of actions. The hackagames Tictactoe package includes a very useful Grid class to manipulate the game state. A minimal random Bot can be implemented in a few lines: from hacka.games.tictactoe.grid import Grid import random class Bot : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gamePod ): assert( gamePod.family() == 'TicTacToe') assert( gamePod.status() in ['Classic', 'Ultimate'] ) self._playerId= playerId self._grid= Grid( gamePod.status() ) self._possibilities= [1] def perceive(self, gameState): # Update the grid: self._grid.update( gameState.children()[:-1] ) self._possibilities= gameState.children()[-1].flags() def decide(self): # Get all actions actions= self._grid.possibleActions( self._possibilities ) # Select one return random.choice( actions ) def sleep(self, result): pass And can be tested thanks to a very simple launcher script: # Setup a game: from hacka.games.tictactoe import GameTTT as Game from hacka.games.tictactoe.firstBot import Bot as Opponent # Setup your bot: from myRandomBot import Bot bot= Bot() oppo= Opponent() # Instanciate and start 100 games game= Game(\"classic\") # can be set to \"ultimate\" results= game.test2Players( bot, oppo, 1000 ) # Analyze the result print( f\"Average score: {sum(results[0])/len(results[0])} vs {sum(results[1])/len(results[1])}\" ) At this point, the results comparing the 2 Bot would be very close. The 2 bots have the same behavior. To notice that, the list of actions do not filter positions already taken. However in case of wrong actions, the game will ask the player for a second call.","title":"Initialize a Bot"},{"location":"games/tictactoe/#customize-your-bot","text":"The Grid object provide for two main methods: at(abs, ord) , returning a cell value (for instance grid.at('A', 2) \\(\\rightarrow\\) 0 if the cell is empty) and at_set(abs, ord, aValue) to set a specific value on a specific cell. From that point you can crate your own evaluation of the grid and which action is interesting. You can also import the tictactoe/engine class for a more complete toolbox (test actions, etc.)","title":"Customize your Bot:"},{"location":"hello/03-pod/","text":"Piece-Of-Data - the core Class In the interoperable process between a game and players, information transits. Games send what players can perceive and players send action description in return. That information is structured as Pod (Piece-Of-Data) to be serialized, transmitted and rebuilt. a Pod includes raw data (sequence of characters, intergers, floating points values) and cand be a part of a tree structure of information. Implementation: hackapy: pod.py The example here are presented accordingly to the Python implementation (hackapy). Pod Components Pod is composed of 5 elements of fixed type: family: an string permitting to identifiate the type Pod . It is generally used as a key to make the other elements interpretable. status: an string describing the individual in the family, its current state. flags: a vector of integer values. values: a vector of floating points values. children: a vector of other Pod elements, sub-part of the Pod. Accessor methods permit to get string elements as list : aPod.family() , aPod.status() and list elements: aPod.flags() , aPod.values() , aPod.children() . It is also possible to get a specific element is the lists with: aPod.flag(anInteger) , aPod.value(anInteger) , aPod.child(anInteger) . To notice that, in HackaGames conventions element in list are indexed starting from 1 . The first element is accessed with index 1 : ( aPod.flags(1) for instance). Tree Structure Children componnents of a Pod permits to defines complexe information structured as trees. For more information about tree data structure, Wikipedia is a good entrance point. Trees are very famous in Web technoligies for instance through HTML , XML , Json , Yaml etc formats. In HackaGames most of the game states perceived by players (one Pod ) are composed by several elements each one modeled by child Pod. For instance, a Board would be composed with Cells and Cells would welcome Pieces . The pieces would be the childrens of a Cell , Cells the children of a Board etc. Interface versus Inheritance HackaGames conventions prefer to separate game elements in the implementations from Pod. A Pod imposes specific structuration of elements (flags, values, ...). This structuration is not always efficient in the game mechanisms and not even readable in its implementations. That for, we discourage developers from using inheritance of Pod in game and player implementations and we propose interface mechanisms to generate pods from the game elements and vice versa, when it is pertinent to do it. Game elements should include Pod interface methods: # Pod interface: def asPod(self, family=\"Pod\"): # Should return a Pod describing self. # ... pass def fromPod(self, aPod): # Should regenerate self form a pod description # HackaGames conventions aim to return self at the end of the method. # ... pass","title":"Piece-Of-Data - the core Class"},{"location":"hello/03-pod/#piece-of-data-the-core-class","text":"In the interoperable process between a game and players, information transits. Games send what players can perceive and players send action description in return. That information is structured as Pod (Piece-Of-Data) to be serialized, transmitted and rebuilt. a Pod includes raw data (sequence of characters, intergers, floating points values) and cand be a part of a tree structure of information. Implementation: hackapy: pod.py The example here are presented accordingly to the Python implementation (hackapy).","title":"Piece-Of-Data - the core Class"},{"location":"hello/03-pod/#pod-components","text":"Pod is composed of 5 elements of fixed type: family: an string permitting to identifiate the type Pod . It is generally used as a key to make the other elements interpretable. status: an string describing the individual in the family, its current state. flags: a vector of integer values. values: a vector of floating points values. children: a vector of other Pod elements, sub-part of the Pod. Accessor methods permit to get string elements as list : aPod.family() , aPod.status() and list elements: aPod.flags() , aPod.values() , aPod.children() . It is also possible to get a specific element is the lists with: aPod.flag(anInteger) , aPod.value(anInteger) , aPod.child(anInteger) . To notice that, in HackaGames conventions element in list are indexed starting from 1 . The first element is accessed with index 1 : ( aPod.flags(1) for instance).","title":"Pod Components"},{"location":"hello/03-pod/#tree-structure","text":"Children componnents of a Pod permits to defines complexe information structured as trees. For more information about tree data structure, Wikipedia is a good entrance point. Trees are very famous in Web technoligies for instance through HTML , XML , Json , Yaml etc formats. In HackaGames most of the game states perceived by players (one Pod ) are composed by several elements each one modeled by child Pod. For instance, a Board would be composed with Cells and Cells would welcome Pieces . The pieces would be the childrens of a Cell , Cells the children of a Board etc.","title":"Tree Structure"},{"location":"hello/03-pod/#interface-versus-inheritance","text":"HackaGames conventions prefer to separate game elements in the implementations from Pod. A Pod imposes specific structuration of elements (flags, values, ...). This structuration is not always efficient in the game mechanisms and not even readable in its implementations. That for, we discourage developers from using inheritance of Pod in game and player implementations and we propose interface mechanisms to generate pods from the game elements and vice versa, when it is pertinent to do it. Game elements should include Pod interface methods: # Pod interface: def asPod(self, family=\"Pod\"): # Should return a Pod describing self. # ... pass def fromPod(self, aPod): # Should regenerate self form a pod description # HackaGames conventions aim to return self at the end of the method. # ... pass","title":"Interface versus Inheritance"},{"location":"hello/04-protocol/","text":"","title":"04 protocol"},{"location":"hello/first-bot/","text":"First Bot In this tutorial, we will learn about how the games and the players/bots communicate together. In Hackagames several players can play simultaneously to games. That for, the loop control of the program is handled by the game. Then, the player bots have to generate the appropriate responses (string formed actions) to situations. To do so, a player bot can re-implement the 4 expected methods: wakeUp , perceive , decide and sleep . Initialize a Bot environment First let implement a simple script, instantiating a game and a player. Put the next code into a script file launch-py421.py . # Import a game and player(s): from hacka.games.py421 import GameSolo as Game from hacka.games.py421.firstBot import Bot # Instanciate them: game= Game() bot= Bot() # Start 1000 games on a player to test: results= game.testPlayer( bot, 1000 ) # And analyze the result: print( f\"Average score: {sum(results)/len(results)}\" ) This script import py421.py game and a bot , then it starts \\(1000\\) games with that bot as main player. It return the 1000 end game results of bot . Test it in a terminal: python3 launch-py421.py Initialize a Bot, i.e. an Autonomous Player For playing py421.py , our script must implement an hackagames formated Bot . A first simple solution consists in modifying the one proposed by the Py421 HackaGames . Py421 FirstBot . Py421 FirstBot plays randomly, an interesting update should be to score each time a \\(4 2 1\\) is reached : import random import hacka.games.py421.firstBot as firstBot # Setup a bot: class MyBot( firstBot.Bot ) : def decide( self ): if self.dices() == [4, 2, 1] : return \"keep-keep-keep\" return random.choice( self.actions() ) Replace the # Setup a player: section into your launch-py421.py script and try your bot. The average score should be increased. Understand Bot methods : Games and Bots rely on hacka.pylib library, to exchange information about the game and the player intentions. Game Loop: A game mainly relies on perceive and decide capability for players and bots to update the information about the game and to ask for player action. The perceive method gives \\(1\\) argument: a gameState , the current game state from the point of view of the player. In fact gameState can be different from a player to another in multiplayer games with partial observation (typically: card games). In the basic configuration of Py421 , there is only one player and the gameConf informs about the 3 dice values and the horizon counter (the number of remaining roll again possibilities). The decide method does not have an argument, but should return the player action. decide is always triggered after one or several perceive . The action is formatted as a string. Game Server : Furthermore, A Game server can start several games sequentially. That for, the method wakeUp and sleep are triggered respectively when a new game is started and stopped. There are triggered 1000 times in our script. During a Game's game, players are activated at turn with perceive and decide method The wakeUp has \\(3\\) parameters: playerId , numberOfPlayers , gameConf informing in the place of the player in the game, the number of players and the initial configuration. At this stage, there is no information to get from gameConf in Py421 . Finally, the sleep inform about the obtained score with it parameters result . The score is computed for the player regarding the ending game. Example of first py421 Bot : As an example, this next Bot is a verbose version for \"Py421\" playing randomly: import random class Bot : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): print( f'---\\nWake-up player-{playerId} ({numberOfPlayers} players)') print( \"conf: \" + str(gameConf) ) def perceive(self, gameState): self._horizon= gameState.child(1).flag(1) self._dices= gameState.child(2).flags() self._score= gameState.child(2).value(1) print( f'\\t> H: {self._horizon}, DICES: {self._dices}, SCORE: {self._score}' ) def decide(self): actions= ['keep-keep-keep', 'keep-keep-roll', 'keep-roll-keep', 'keep-roll-roll', 'roll-keep-keep', 'roll-keep-roll', 'roll-roll-keep', 'roll-roll-roll' ] action= random.choice( actions ) print( f'\\t< Action: {action}' ) return action def sleep(self, result): print( f'--- Results: {str(result)}' ) To notice that the parameters gameConf and gameState are formated as hackapy.Pod (Piece Of Data), a tree data structure. It is not required to understand Pod . Examples are always provided to get the game information from Pod . However, you can go on Under the hood - Pod page for more detail on this core element of HackaGames .","title":"First Bot"},{"location":"hello/first-bot/#first-bot","text":"In this tutorial, we will learn about how the games and the players/bots communicate together. In Hackagames several players can play simultaneously to games. That for, the loop control of the program is handled by the game. Then, the player bots have to generate the appropriate responses (string formed actions) to situations. To do so, a player bot can re-implement the 4 expected methods: wakeUp , perceive , decide and sleep .","title":"First Bot"},{"location":"hello/first-bot/#initialize-a-bot-environment","text":"First let implement a simple script, instantiating a game and a player. Put the next code into a script file launch-py421.py . # Import a game and player(s): from hacka.games.py421 import GameSolo as Game from hacka.games.py421.firstBot import Bot # Instanciate them: game= Game() bot= Bot() # Start 1000 games on a player to test: results= game.testPlayer( bot, 1000 ) # And analyze the result: print( f\"Average score: {sum(results)/len(results)}\" ) This script import py421.py game and a bot , then it starts \\(1000\\) games with that bot as main player. It return the 1000 end game results of bot . Test it in a terminal: python3 launch-py421.py","title":"Initialize a Bot environment"},{"location":"hello/first-bot/#initialize-a-bot-ie-an-autonomous-player","text":"For playing py421.py , our script must implement an hackagames formated Bot . A first simple solution consists in modifying the one proposed by the Py421 HackaGames . Py421 FirstBot . Py421 FirstBot plays randomly, an interesting update should be to score each time a \\(4 2 1\\) is reached : import random import hacka.games.py421.firstBot as firstBot # Setup a bot: class MyBot( firstBot.Bot ) : def decide( self ): if self.dices() == [4, 2, 1] : return \"keep-keep-keep\" return random.choice( self.actions() ) Replace the # Setup a player: section into your launch-py421.py script and try your bot. The average score should be increased.","title":"Initialize a Bot, i.e. an Autonomous Player"},{"location":"hello/first-bot/#understand-bot-methods","text":"Games and Bots rely on hacka.pylib library, to exchange information about the game and the player intentions.","title":"Understand Bot methods :"},{"location":"hello/first-bot/#game-loop","text":"A game mainly relies on perceive and decide capability for players and bots to update the information about the game and to ask for player action. The perceive method gives \\(1\\) argument: a gameState , the current game state from the point of view of the player. In fact gameState can be different from a player to another in multiplayer games with partial observation (typically: card games). In the basic configuration of Py421 , there is only one player and the gameConf informs about the 3 dice values and the horizon counter (the number of remaining roll again possibilities). The decide method does not have an argument, but should return the player action. decide is always triggered after one or several perceive . The action is formatted as a string.","title":"Game Loop:"},{"location":"hello/first-bot/#game-server","text":"Furthermore, A Game server can start several games sequentially. That for, the method wakeUp and sleep are triggered respectively when a new game is started and stopped. There are triggered 1000 times in our script. During a Game's game, players are activated at turn with perceive and decide method The wakeUp has \\(3\\) parameters: playerId , numberOfPlayers , gameConf informing in the place of the player in the game, the number of players and the initial configuration. At this stage, there is no information to get from gameConf in Py421 . Finally, the sleep inform about the obtained score with it parameters result . The score is computed for the player regarding the ending game.","title":"Game Server :"},{"location":"hello/first-bot/#example-of-first-py421-bot","text":"As an example, this next Bot is a verbose version for \"Py421\" playing randomly: import random class Bot : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): print( f'---\\nWake-up player-{playerId} ({numberOfPlayers} players)') print( \"conf: \" + str(gameConf) ) def perceive(self, gameState): self._horizon= gameState.child(1).flag(1) self._dices= gameState.child(2).flags() self._score= gameState.child(2).value(1) print( f'\\t> H: {self._horizon}, DICES: {self._dices}, SCORE: {self._score}' ) def decide(self): actions= ['keep-keep-keep', 'keep-keep-roll', 'keep-roll-keep', 'keep-roll-roll', 'roll-keep-keep', 'roll-keep-roll', 'roll-roll-keep', 'roll-roll-roll' ] action= random.choice( actions ) print( f'\\t< Action: {action}' ) return action def sleep(self, result): print( f'--- Results: {str(result)}' ) To notice that the parameters gameConf and gameState are formated as hackapy.Pod (Piece Of Data), a tree data structure. It is not required to understand Pod . Examples are always provided to get the game information from Pod . However, you can go on Under the hood - Pod page for more detail on this core element of HackaGames .","title":"Example of first py421 Bot :"},{"location":"hello/install/","text":"Install Process A quick tutorial to make HackaGames running on your computer. Nota bene : HackaGames is natively developed on Linux systems. Documentation is built regarding Ubuntu-like distributions. Commands are given in bash syntaxe. On Windows: You can use git for windows and its git bash . Install Simplely clone the git repository and use python pip . git clone https://github.com/ktorz-net/hackagames pip install ./hackagames or python -m pip install ./hackagames Thats it. You can play to several included games (the ones developed on top of hackapy ), and implement some Bots (cf. First Bot section). Dependencies The project is distributed using git solution and relies on Python3 language. Make sure you have them installed. HackaGames includes python3 librairy: haka.pylib for making game-servers and player-clients. The solution relies on external librairies: zmq for network protocol of HackaGames tqdm for process-bars are implemented via cairo for graphical output. Getting Started The easiest way is to play with one of the proposed python3 games, Py421 for instance. Each python3 game comes with play script permitting to start the game with an interactive interface in a shell. python3 hackagames/play/py421 The Py421 is a tree dice game. The player can roll several times to get the best combination. The actions consist in keeping or rolling each of the 3 dices. So there are 8 possible actions: keep-keep-keep , keep-keep-roll , keep-roll-keep , keep-roll-roll , roll-keep-keep , roll-keep-roll , roll-roll-keep and roll-roll-roll The goal is to optimize the combination of dices before the end of the second turn. The best combination ever is 4-2-1 . But you can explore other combinations. You can then follow the tutorial of First Bot to learn how to implement an autonomous player for Py421 the game.","title":"Install"},{"location":"hello/install/#install-process","text":"A quick tutorial to make HackaGames running on your computer. Nota bene : HackaGames is natively developed on Linux systems. Documentation is built regarding Ubuntu-like distributions. Commands are given in bash syntaxe. On Windows: You can use git for windows and its git bash .","title":"Install Process"},{"location":"hello/install/#install","text":"Simplely clone the git repository and use python pip . git clone https://github.com/ktorz-net/hackagames pip install ./hackagames or python -m pip install ./hackagames Thats it. You can play to several included games (the ones developed on top of hackapy ), and implement some Bots (cf. First Bot section).","title":"Install"},{"location":"hello/install/#dependencies","text":"The project is distributed using git solution and relies on Python3 language. Make sure you have them installed. HackaGames includes python3 librairy: haka.pylib for making game-servers and player-clients. The solution relies on external librairies: zmq for network protocol of HackaGames tqdm for process-bars are implemented via cairo for graphical output.","title":"Dependencies"},{"location":"hello/install/#getting-started","text":"The easiest way is to play with one of the proposed python3 games, Py421 for instance. Each python3 game comes with play script permitting to start the game with an interactive interface in a shell. python3 hackagames/play/py421 The Py421 is a tree dice game. The player can roll several times to get the best combination. The actions consist in keeping or rolling each of the 3 dices. So there are 8 possible actions: keep-keep-keep , keep-keep-roll , keep-roll-keep , keep-roll-roll , roll-keep-keep , roll-keep-roll , roll-roll-keep and roll-roll-roll The goal is to optimize the combination of dices before the end of the second turn. The best combination ever is 4-2-1 . But you can explore other combinations. You can then follow the tutorial of First Bot to learn how to implement an autonomous player for Py421 the game.","title":"Getting Started"},{"location":"hello/multi-player/","text":"Multi-Player in HackaGames Most of the games are multi-players games where Artificial Intelligence embodies into Bot fight. With a simple script One simple way to start a Python HackaGames game with several players is to instantiate a game and the players/Bots in a launcher script: # Setup a game: from hacka.games.aGame import Game from hacka.games.aGame.firstBot import Bot as Opponent from myBot import Bot bot, opponent1, opponent2= Bot(), Opponent(), Opponent() # Instanciate and start 100 games number= 100 game= Game() results=game.launch( [bot, opponent1, opponent2], number ) # Analyze the results for name, individualResults in zip( [\"bob\", \"natacha\", \"jad\"], results ) : print( f\"- {name},s average score: {sum(individualResults)/number}\" ) Human Interface The hacka.pylib package provides a very basic shell interface capable of playing any hackagames. from hacka.pylib.player import PlayerIHM as Interface Typically it is the one used by py421 game. However, systematically more tailored shell interface is provided. from hacka.games.aGames.shell import Interface Those interfaces can replace bot players into the launcher script. Example with Connect4: # Setup a game: from hacka.games.connect4 import GameConnect4 as Game from hacka.games.connect4.firstBot import Bot from hacka.games.connect4.shell import Interface # Instanciate and start 100 games number= 4 game= Game() results=game.launch( [Bot(), Interface()], number ) # Analyze the results for name, individualResults in zip( [\"bob\", \"me\"], results ) : print( f\"- {name},s average score: {sum(individualResults)/number}\" ) Client-Server Architecture HackaGames is designed to run as a client-server architecture. The game run as a server, and players (Bots or Human Interfaces) as clients. The game wait for the appropriate number of players before to start. For instance, on three different shells: # shell-1 python3 -m hacka.games.tictactoe.serve ultimate -n 2 # shell-2 python3 -m hacka.games.tictactoe.shell # shell-3 python3 -m hacka.games.tictactoe.firstBot Each shell process one entity of the game, coordination is provided with hackagame protocol. The shells can be distributed over different machine. To connect a player to a specific hackagames server, the player need to inherite from hacka.pylib abstract player ( AbsPlayer ) and take a seat on a game.... from ... import pylib as hk def main() : player= MyBot() player.takeASeat( \"xxx.local\" ) class MyBot( hk.AbsPlayer ) : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gamePod ): ... def perceive(self, gameState): ... def decide(self): ... #def sleep(self, result): ... Script : if name == ' main ' : main() ...","title":"Multi-Player"},{"location":"hello/multi-player/#multi-player-in-hackagames","text":"Most of the games are multi-players games where Artificial Intelligence embodies into Bot fight.","title":"Multi-Player in HackaGames"},{"location":"hello/multi-player/#with-a-simple-script","text":"One simple way to start a Python HackaGames game with several players is to instantiate a game and the players/Bots in a launcher script: # Setup a game: from hacka.games.aGame import Game from hacka.games.aGame.firstBot import Bot as Opponent from myBot import Bot bot, opponent1, opponent2= Bot(), Opponent(), Opponent() # Instanciate and start 100 games number= 100 game= Game() results=game.launch( [bot, opponent1, opponent2], number ) # Analyze the results for name, individualResults in zip( [\"bob\", \"natacha\", \"jad\"], results ) : print( f\"- {name},s average score: {sum(individualResults)/number}\" )","title":"With a simple script"},{"location":"hello/multi-player/#human-interface","text":"The hacka.pylib package provides a very basic shell interface capable of playing any hackagames. from hacka.pylib.player import PlayerIHM as Interface Typically it is the one used by py421 game. However, systematically more tailored shell interface is provided. from hacka.games.aGames.shell import Interface Those interfaces can replace bot players into the launcher script. Example with Connect4: # Setup a game: from hacka.games.connect4 import GameConnect4 as Game from hacka.games.connect4.firstBot import Bot from hacka.games.connect4.shell import Interface # Instanciate and start 100 games number= 4 game= Game() results=game.launch( [Bot(), Interface()], number ) # Analyze the results for name, individualResults in zip( [\"bob\", \"me\"], results ) : print( f\"- {name},s average score: {sum(individualResults)/number}\" )","title":"Human Interface"},{"location":"hello/multi-player/#client-server-architecture","text":"HackaGames is designed to run as a client-server architecture. The game run as a server, and players (Bots or Human Interfaces) as clients. The game wait for the appropriate number of players before to start. For instance, on three different shells: # shell-1 python3 -m hacka.games.tictactoe.serve ultimate -n 2 # shell-2 python3 -m hacka.games.tictactoe.shell # shell-3 python3 -m hacka.games.tictactoe.firstBot Each shell process one entity of the game, coordination is provided with hackagame protocol. The shells can be distributed over different machine. To connect a player to a specific hackagames server, the player need to inherite from hacka.pylib abstract player ( AbsPlayer ) and take a seat on a game.... from ... import pylib as hk def main() : player= MyBot() player.takeASeat( \"xxx.local\" ) class MyBot( hk.AbsPlayer ) : # Player interface : def wakeUp(self, playerId, numberOfPlayers, gamePod ): ... def perceive(self, gameState): ... def decide(self): ... #def sleep(self, result): ...","title":"Client-Server Architecture"},{"location":"hello/multi-player/#script","text":"if name == ' main ' : main() ...","title":"Script :"},{"location":"hood/api/","text":"Communication API The idea is to use a client/server architecture to permit any code in any language to seat on a game (i.e., connect to act as a player). For that we choose to use a high-level messaging library. High-Level Message Queuing API We do not require to address directly the TCP protocol while our need for network usage is very classical. A game process acts as a server, accepts and manages client-player processes. Then the game and players exchange game states and action description. For that purpose tree solution was considered: MQTT - Mosquitto , ZeroMQ and ROS2 Mosquitto is a light implementation of the MQTT standard. MQTT defines protocols for process interconnection. a Broker server serves as a central point to connect all the other nodes. Topics are defined as pipes of data flow. Any process can subscribe (i.e. reads) or publish (i.e. write) in the topics. Wikipedia is a good entrance point for more details. ZeroMQ follows its own protocols and propose a high-level interface to interprocess communication via difference communication architecture and different low-level protocols including in-procecus communication. ROS2 is developed for robotic purpose. It is very similar to MQTT in its communication architecture. However, in ROS , messages in topics are typed and several types and algorithms are already developed to exchange, manipulate and visualize geometrical and temporal data. Also in ROS2 the broker process is removed. For HackaGames use, ZeroMQ present the best compromise. It requires more implementation compared to MQTT - Mosquitto to permit processes to communicate but it allows the development of dedicated game/players communication protocols. This way, there is no need for a broker, and the game server can better address specific information to identified players. ROS2 solution was removed from the candidates mostly because of the huge dependencies relying on this solution. Using ROS2 induces to install and use a heavy API for developing and for managing communicating processes. One of the core advantages of ZeroMQ resides in the fact that it is an open-source solution developed in C and so on, easily integrable in any other languages. Appropriation of ZeroMQ The philosophy is to make games and players interoperable whatever the technology used to develop them. That for HackaGames proposes 2 independent implementation of its core features. One in Python ( hackapy ) and one in C ( hackalib - to come). In hackapy , interoperable API is developed in the (interprocess.py)[https://bitbucket.org/imt-mobisyst/hackagames/src/master/hackapy/interprocess.py] file. The implementation includes two main classes: Dealer and Client . The Dealer aims to represent a game. It manages Client connection each one labeled as an identified player. A tweak class Local permits a game and players to run in the same process, if all of them are implemented with Python. This way, implemented games and players are agnostic from the usage or not of interprocess architecture via ZeroMQ . Those elements can be running without any difference with a Dealer and Clients or with Local manager.","title":"Protocol API"},{"location":"hood/api/#communication-api","text":"The idea is to use a client/server architecture to permit any code in any language to seat on a game (i.e., connect to act as a player). For that we choose to use a high-level messaging library.","title":"Communication API"},{"location":"hood/api/#high-level-message-queuing-api","text":"We do not require to address directly the TCP protocol while our need for network usage is very classical. A game process acts as a server, accepts and manages client-player processes. Then the game and players exchange game states and action description. For that purpose tree solution was considered: MQTT - Mosquitto , ZeroMQ and ROS2 Mosquitto is a light implementation of the MQTT standard. MQTT defines protocols for process interconnection. a Broker server serves as a central point to connect all the other nodes. Topics are defined as pipes of data flow. Any process can subscribe (i.e. reads) or publish (i.e. write) in the topics. Wikipedia is a good entrance point for more details. ZeroMQ follows its own protocols and propose a high-level interface to interprocess communication via difference communication architecture and different low-level protocols including in-procecus communication. ROS2 is developed for robotic purpose. It is very similar to MQTT in its communication architecture. However, in ROS , messages in topics are typed and several types and algorithms are already developed to exchange, manipulate and visualize geometrical and temporal data. Also in ROS2 the broker process is removed. For HackaGames use, ZeroMQ present the best compromise. It requires more implementation compared to MQTT - Mosquitto to permit processes to communicate but it allows the development of dedicated game/players communication protocols. This way, there is no need for a broker, and the game server can better address specific information to identified players. ROS2 solution was removed from the candidates mostly because of the huge dependencies relying on this solution. Using ROS2 induces to install and use a heavy API for developing and for managing communicating processes. One of the core advantages of ZeroMQ resides in the fact that it is an open-source solution developed in C and so on, easily integrable in any other languages.","title":"High-Level Message Queuing API"},{"location":"hood/api/#appropriation-of-zeromq","text":"The philosophy is to make games and players interoperable whatever the technology used to develop them. That for HackaGames proposes 2 independent implementation of its core features. One in Python ( hackapy ) and one in C ( hackalib - to come). In hackapy , interoperable API is developed in the (interprocess.py)[https://bitbucket.org/imt-mobisyst/hackagames/src/master/hackapy/interprocess.py] file. The implementation includes two main classes: Dealer and Client . The Dealer aims to represent a game. It manages Client connection each one labeled as an identified player. A tweak class Local permits a game and players to run in the same process, if all of them are implemented with Python. This way, implemented games and players are agnostic from the usage or not of interprocess architecture via ZeroMQ . Those elements can be running without any difference with a Dealer and Clients or with Local manager.","title":"Appropriation of ZeroMQ"},{"location":"hood/newgame/","text":"Game Creation in Python The ideas here is to present step by step the game creation in python with hackapy . Reminder, hackapy is the HackaGames python librairy helping for the game and players to communicate together. Directory structure In your workspace directory create a subdirectory gameXyz where Xyz identify your new game ( gameHello for instance). This new subdirectory (your working directory) will also include another subdirectory gameEngine regrouping the source code making your game working. Directory squeletom: gameHello # your game folder - gameEngine # sourcecode of the game Then we start with 3 files: README.md : a Markdown readme first file presenting the game and the rules. gameEngine/__init__.py : a classical python files marking the entrance of your gameEngine package. start-server : a start script, lauching the game server. Game Engine At minima gameEngine python package include a Game class deriving from hackapy.AbsGame in the __init__.py file. It is suppozed that the new Game class implement the abstract methods of AbsGame : class AbsGame(): # Game interface : def initialize(self): # Initialize a new game # Return the game configuration (as a PodInterface) # the returned Pod is given to player's wake-up method. pass def playerHand( self, iPlayer ): # Return the game elements in the player vision (a PodInterface) # the returned pod feed the player's perception method. pass def applyPlayerAction( self, iPlayer, action ): # Apply the action choosen by the player iPlayer. # Return a boolean at True if the player terminate its actions for the current turn. # False means player need to be activated again before step to another player or another game turn. pass def tic( self ): # called function at turn end, after all player played its actions. pass def isEnded( self ): # must return True when the game end, and False else. pass def playerScore( self, iPlayer ): # return the player score for the current game (usefull at game ending) pass A last abstract method is defined: play . This method is more an inside method re-defined in function of how the players are managed. Actually, two strategies are proposed: AbsSequentialGame: Players are activated at turns. The perception of the game for the \\(i\\) -th player and a call to decide are sent after the \\((i-1)\\) -th player terminates its actions ( applyPlayerAction( (i-1), \"action\" ) returns True ). AbsSimultaneousGame: (Work In Progress) Players are activated in a simultaneous way. All the players receive their perception of the game then all the players are requested for their actions. Generally action resolution in these kinds of games are resolved in the tic method. Example of of the simple hello games: First, initialize python file and import the hackapy package. #!env python3 \"\"\" HackaGame - Game - Hello \"\"\" import sys sys.path.insert( 1, __file__.split('gameHello')[0] ) import hackapy as hg Then, the Game implement an abstract sequential game (each player play at turns). The Game methods to implemented are: initialize , playerHand , applyPlayerAction , isEnded and playerScore . Here the Hello game simply echo the player action in a terminal \\(3\\) times (method applyPlayerAction ). class GameHello( hg.AbsSequentialGame ) : # Game interface : def initialize(self): # initialize the counter and only say hello. self.counter= 0 return hg.Pod( 'hello' ) def playerHand( self, iPlayer ): # ping with the increasing counter return hg.Pod( 'hi', flags=[ self.counter ] ) def applyPlayerAction( self, iPlayer, action ): # print the receive action message. And that all. print( f\"Player-{iPlayer} say < {action} >\" ) return True def tic( self ): # step on the counter. self.counter= min( self.counter+1, 3 ) def isEnded( self ): # if the counter reach it final value return self.counter == 3 def playerScore( self, iPlayer ): # All players are winners. return 1 A counter initialized in initialize method, count \\(3\\) game turn (i.e. after each player play at-turn) in tic method. Then the isEnded method will return True . The method playerHand informs the player about the counter status. Finaly, there is no winner and all player will end with a result at \\(1\\) (method playerScore ). Lets play The start-server script will permit to lauch the game server. It only instancate a Game with a determined number of players then call the AbsGame start method. #!env python3 \"\"\" HackaGame - Game - Hello \"\"\" from gameEngine import GameHello game= GameHello() game.start() That it. You can set your script executable ( chmod +x ./gameHello/start-server ) and play with your new game: # In a first shell: ./gameHello/start-server # In a second shell: ./hackagames/connect-shell Game initialization: AbsGame initialization need a number of players. By default the value is on 1 . However, if you want to fix this number or if you want to add extrat initialization, you need to call the parent initialization in your own. As reminder in Python , initialization method is named __init__ and super() function provides an access to parent methods. For instance, for a 2 player game: class MyGame( hg.AbsSequentialGame ) : # Initialization: def __init__(self) : super().__init__( numberOfPlayers=2 ) self._myAttribut= \"Some initializations\" Going futher: Command Interpreter: from hackapy.command import Command, Option # Define a command interpreter: 2 options: host address and port: cmd= Command( \"start-server\", [ Option( \"port\", \"p\", default=1400 ), Option( \"number\", \"n\", 2, \"number of games\" ) ], ( \"star a server fo gameConnect4 on your machine. \" \"gameConnect4 do not take ARGUMENT.\" )) # Process the command line: cmd.process() if not cmd.ready() : print( cmd.help() ) exit() ... game.start( cmd.option(\"number\"), cmd.option(\"port\") ) Going futher: Test-Driven: You can start with a first : test_01_AbsGame script in a test directory. to verify the call to HackaGames Games methods... \"\"\" Test - Hello Games Class \"\"\" import sys sys.path.insert( 1, __file__.split('gameHello')[0] ) import hackapy as hg import gameHello.gameEngine as ge def test_gameMethod(): game= ge.GameHello() assert( type( game.initialize().asPod() ) is hg.Pod ) assert( type( game.playerHand(1).asPod() ) is hg.Pod ) assert( game.applyPlayerAction( 1, \"sleep\" ) ) game.tic() assert( not game.isEnded() ) assert( game.playerScore(1) == 1 ) ...","title":"New Game"},{"location":"hood/newgame/#game-creation-in-python","text":"The ideas here is to present step by step the game creation in python with hackapy . Reminder, hackapy is the HackaGames python librairy helping for the game and players to communicate together.","title":"Game Creation in Python"},{"location":"hood/newgame/#directory-structure","text":"In your workspace directory create a subdirectory gameXyz where Xyz identify your new game ( gameHello for instance). This new subdirectory (your working directory) will also include another subdirectory gameEngine regrouping the source code making your game working. Directory squeletom: gameHello # your game folder - gameEngine # sourcecode of the game Then we start with 3 files: README.md : a Markdown readme first file presenting the game and the rules. gameEngine/__init__.py : a classical python files marking the entrance of your gameEngine package. start-server : a start script, lauching the game server.","title":"Directory structure"},{"location":"hood/newgame/#game-engine","text":"At minima gameEngine python package include a Game class deriving from hackapy.AbsGame in the __init__.py file. It is suppozed that the new Game class implement the abstract methods of AbsGame : class AbsGame(): # Game interface : def initialize(self): # Initialize a new game # Return the game configuration (as a PodInterface) # the returned Pod is given to player's wake-up method. pass def playerHand( self, iPlayer ): # Return the game elements in the player vision (a PodInterface) # the returned pod feed the player's perception method. pass def applyPlayerAction( self, iPlayer, action ): # Apply the action choosen by the player iPlayer. # Return a boolean at True if the player terminate its actions for the current turn. # False means player need to be activated again before step to another player or another game turn. pass def tic( self ): # called function at turn end, after all player played its actions. pass def isEnded( self ): # must return True when the game end, and False else. pass def playerScore( self, iPlayer ): # return the player score for the current game (usefull at game ending) pass A last abstract method is defined: play . This method is more an inside method re-defined in function of how the players are managed. Actually, two strategies are proposed: AbsSequentialGame: Players are activated at turns. The perception of the game for the \\(i\\) -th player and a call to decide are sent after the \\((i-1)\\) -th player terminates its actions ( applyPlayerAction( (i-1), \"action\" ) returns True ). AbsSimultaneousGame: (Work In Progress) Players are activated in a simultaneous way. All the players receive their perception of the game then all the players are requested for their actions. Generally action resolution in these kinds of games are resolved in the tic method.","title":"Game Engine"},{"location":"hood/newgame/#example-of-of-the-simple-hello-games","text":"First, initialize python file and import the hackapy package. #!env python3 \"\"\" HackaGame - Game - Hello \"\"\" import sys sys.path.insert( 1, __file__.split('gameHello')[0] ) import hackapy as hg Then, the Game implement an abstract sequential game (each player play at turns). The Game methods to implemented are: initialize , playerHand , applyPlayerAction , isEnded and playerScore . Here the Hello game simply echo the player action in a terminal \\(3\\) times (method applyPlayerAction ). class GameHello( hg.AbsSequentialGame ) : # Game interface : def initialize(self): # initialize the counter and only say hello. self.counter= 0 return hg.Pod( 'hello' ) def playerHand( self, iPlayer ): # ping with the increasing counter return hg.Pod( 'hi', flags=[ self.counter ] ) def applyPlayerAction( self, iPlayer, action ): # print the receive action message. And that all. print( f\"Player-{iPlayer} say < {action} >\" ) return True def tic( self ): # step on the counter. self.counter= min( self.counter+1, 3 ) def isEnded( self ): # if the counter reach it final value return self.counter == 3 def playerScore( self, iPlayer ): # All players are winners. return 1 A counter initialized in initialize method, count \\(3\\) game turn (i.e. after each player play at-turn) in tic method. Then the isEnded method will return True . The method playerHand informs the player about the counter status. Finaly, there is no winner and all player will end with a result at \\(1\\) (method playerScore ).","title":"Example of of the simple hello games:"},{"location":"hood/newgame/#lets-play","text":"The start-server script will permit to lauch the game server. It only instancate a Game with a determined number of players then call the AbsGame start method. #!env python3 \"\"\" HackaGame - Game - Hello \"\"\" from gameEngine import GameHello game= GameHello() game.start() That it. You can set your script executable ( chmod +x ./gameHello/start-server ) and play with your new game: # In a first shell: ./gameHello/start-server # In a second shell: ./hackagames/connect-shell","title":"Lets play"},{"location":"hood/newgame/#game-initialization","text":"AbsGame initialization need a number of players. By default the value is on 1 . However, if you want to fix this number or if you want to add extrat initialization, you need to call the parent initialization in your own. As reminder in Python , initialization method is named __init__ and super() function provides an access to parent methods. For instance, for a 2 player game: class MyGame( hg.AbsSequentialGame ) : # Initialization: def __init__(self) : super().__init__( numberOfPlayers=2 ) self._myAttribut= \"Some initializations\"","title":"Game initialization:"},{"location":"hood/newgame/#going-futher-command-interpreter","text":"from hackapy.command import Command, Option # Define a command interpreter: 2 options: host address and port: cmd= Command( \"start-server\", [ Option( \"port\", \"p\", default=1400 ), Option( \"number\", \"n\", 2, \"number of games\" ) ], ( \"star a server fo gameConnect4 on your machine. \" \"gameConnect4 do not take ARGUMENT.\" )) # Process the command line: cmd.process() if not cmd.ready() : print( cmd.help() ) exit() ... game.start( cmd.option(\"number\"), cmd.option(\"port\") )","title":"Going futher: Command Interpreter:"},{"location":"hood/newgame/#going-futher-test-driven","text":"You can start with a first : test_01_AbsGame script in a test directory. to verify the call to HackaGames Games methods... \"\"\" Test - Hello Games Class \"\"\" import sys sys.path.insert( 1, __file__.split('gameHello')[0] ) import hackapy as hg import gameHello.gameEngine as ge def test_gameMethod(): game= ge.GameHello() assert( type( game.initialize().asPod() ) is hg.Pod ) assert( type( game.playerHand(1).asPod() ) is hg.Pod ) assert( game.applyPlayerAction( 1, \"sleep\" ) ) game.tic() assert( not game.isEnded() ) assert( game.playerScore(1) == 1 ) ...","title":"Going futher: Test-Driven:"},{"location":"hood/testdriven/","text":"Test Driven Development Test Driven Developement (TDD) consist of defining test that would validate a desired functionality before to develop the functionality itself. After development, the test allows to validate that the functionality works but also that functionally matches the initial expectation. Wikipedia is a good entrance point for going further in the concepts of TDD. One of the simplest ways to develop tests in Python is to use pytest . Pytest tool: A test script has file-name starting with test_ . Optionally it can be regrouped in a test directory. It is composed of test function starting with def test_ defining assert based test. For instance test_pytest.py : def test_test(): assert( True ) Then, Pytest can be used to run the tests: # install pip install pytest # execute all your tests: pytest Test your players: Test your games: A minimal test bench built considering the gameHello tutorial: \"\"\" Test - hello.Engine \"\"\" import sys sys.path.insert( 1, __file__.split('hackagames')[0] ) import hackagames.hackapy as hg import gameHello.gameEngine as ge def test_gameMethod(): game= ge.GameConnect4() assert( type( game.initialize().asPod() ) is hg.Pod ) assert( type( game.playerHand(1).asPod() ) is hg.Pod ) game.applyPlayerAction( 1, \"test\" ) game.tic() assert( not game.isEnded() ) assert( game.playerScore(1) == 0 ) def test_initialize(): game= ge.GameConnect4() wakeUpPod= game.initialize().asPod() assert( str(wakeUpPod) == \"hello:\" ) assert( wakeUpPod.family() == \"hello\" ) assert( wakeUpPod.status() == \"\" ) assert( wakeUpPod.flags() == [] ) assert( wakeUpPod.values() == [] ) assert( len( wakeUpPod.children() ) == 0 ) def test_playerHand(): game= ge.GameConnect4() game.initialize().asPod() handPod= game.playerHand(1).asPod() assert( str(handPod) == 'hi: [0]' ) assert( handPod.family() == \"hi\" ) assert( handPod.status() == \"\" ) assert( handPod.flags() == [0] ) assert( handPod.values() == [] ) assert( len( handPod.children() ) == 0 ) Then you have to adapt the test while you implement your game.","title":"Test-Driven"},{"location":"hood/testdriven/#test-driven-development","text":"Test Driven Developement (TDD) consist of defining test that would validate a desired functionality before to develop the functionality itself. After development, the test allows to validate that the functionality works but also that functionally matches the initial expectation. Wikipedia is a good entrance point for going further in the concepts of TDD. One of the simplest ways to develop tests in Python is to use pytest .","title":"Test Driven Development"},{"location":"hood/testdriven/#pytest-tool","text":"A test script has file-name starting with test_ . Optionally it can be regrouped in a test directory. It is composed of test function starting with def test_ defining assert based test. For instance test_pytest.py : def test_test(): assert( True ) Then, Pytest can be used to run the tests: # install pip install pytest # execute all your tests: pytest","title":"Pytest tool:"},{"location":"hood/testdriven/#test-your-players","text":"","title":"Test your players:"},{"location":"hood/testdriven/#test-your-games","text":"A minimal test bench built considering the gameHello tutorial: \"\"\" Test - hello.Engine \"\"\" import sys sys.path.insert( 1, __file__.split('hackagames')[0] ) import hackagames.hackapy as hg import gameHello.gameEngine as ge def test_gameMethod(): game= ge.GameConnect4() assert( type( game.initialize().asPod() ) is hg.Pod ) assert( type( game.playerHand(1).asPod() ) is hg.Pod ) game.applyPlayerAction( 1, \"test\" ) game.tic() assert( not game.isEnded() ) assert( game.playerScore(1) == 0 ) def test_initialize(): game= ge.GameConnect4() wakeUpPod= game.initialize().asPod() assert( str(wakeUpPod) == \"hello:\" ) assert( wakeUpPod.family() == \"hello\" ) assert( wakeUpPod.status() == \"\" ) assert( wakeUpPod.flags() == [] ) assert( wakeUpPod.values() == [] ) assert( len( wakeUpPod.children() ) == 0 ) def test_playerHand(): game= ge.GameConnect4() game.initialize().asPod() handPod= game.playerHand(1).asPod() assert( str(handPod) == 'hi: [0]' ) assert( handPod.family() == \"hi\" ) assert( handPod.status() == \"\" ) assert( handPod.flags() == [0] ) assert( handPod.values() == [] ) assert( len( handPod.children() ) == 0 ) Then you have to adapt the test while you implement your game.","title":"Test your games:"},{"location":"learning/decision-tree/","text":"Using Decision Tree","title":"Decision Tree"},{"location":"learning/decision-tree/#using-decision-tree","text":"","title":"Using Decision Tree"},{"location":"learning/learn-model/","text":"Model Learning: Markov-Decision-Process This tutorial aims to compute a policy from learned transitions and rewards on py421 game. To do that we need to record a transition function and a average reward function. With dictionary, the transition function would be a dictionary over states returning dictionaries over actions returning dictionaries over reached states' returning the number of times the transition is experienced. In case of Py421 , an object about \\(128 \\times 8 \\times 128\\) counters, but the reward function would be simplest. The model is based on Markov Decision Process (MDP) framework: On Wikipedia After enough experiments, it would be possible to process this MDP model function to compute a policy. Initialize your MDP Bot: First we require specific model methods: state() to transform game variable affectations into a dictionary entry, updateModel(state_past, action, state_present, reward) to use to increase knowledge about transition and reward. # Model: def state(self) : return f\"{self._horizon}-{self._dices[0]}-{self._dices[1]}-{self._dices[2]}\" def updateModel( self, past_state, action, present_state, reward ): print( f\"transition: {past_state}, {action}, {present_state}\") print( f\"reward: {reward}\\n\") Then it is possible to prepare perception s method with model update: # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): self._state= \"3-4-2-1\" self._action= \"r-r-r\" self._score= 0 def perceive(self, gameState): self._horizon= gameState.child(1).flag(1) self._dices= gameState.child(2).flags() newScore= gameState.child(2).value(1) # Learn: newState= self.state() self.updateModel( self._state, self._action, newState, newScore-self._score ) # Switch: self._state= newState self._score= newScore Record transitions and reward: The updateModel method updates counters and average rewards. My advice is to keep also a counter experiments over the amount of time the Bot tests the same action into the same state. At some point, the updateModel method, the update should be something like: self._experiements[past_state][action]+= 1 self._transitions[past_state][action][present_state]+= 1 exp= self._experiements[past_state][action] oldReward= self._rewards[past_state][action] * (exp-1) self._rewards[past_state][action]= (oldReward+reward) / exp Naturally, it supposes that you already create the dictionaries and the entrances... Use json package to save your model (sometimes) into your sleep method. Process model. This is the interesting part of the exercice. From the transitions and rewards we want to compute the optimal policy \\(\\pi^*\\) For that purpose, we recommand to apply Value Iteration (cf. On Wikipedia ) valueIteration: Input: an MDP: \\(\\langle S, A, T, R \\rangle\\) ; precision error: \\(\\epsilon\\) ; discount factor: \\(\\gamma\\) ; initial V(s) Repeat until: maximal delta < \\(\\epsilon\\) For each state \\(s \\in S\\) Search the action \\(a^*\\) maximizing the Bellman Equation on \\(s\\) \\[a^*= \\arg\\max_{a \\in A}\\left( \\mathit{value}(s, a) \\right)\\] \\[\\mathit{value}(s, a)= R(s, a) + \\gamma \\sum_{s'\\in S} T(s,a,s') \\times V(s')\\] Set \\(\\pi(s)= a^*\\) and _ \\(V(s)= \\mathit{value}(s, a^*)\\) Compute the delta value between the previous and the new \\(V(s)\\) Output: an optimal \\(\\pi^*\\) and the associated V-values The policy can be updated every \\(500\\) sleep with a call to your new valueIteration . To notice that self._transition is not directly usable. Values need to be transformed to probabilities ( self._transition[s][a][s'] / self._experiements[s][a] ). Furthermore, any state not defined into self._transition can be considered as a terminal state ( len(V) >= len(self._transition) ). If necessary, terminal state has only the keep-keep-keep action and the transition falls into the same state (itself) with probability of \\(1\\) and a reward of \\(0\\) . Exploit. Use the policy at decision step. However, until a sufisant number of experiments are performed, the model and so the policy cannot be completelly trusted. A random exporation ratio need to be design as for Q-Learning .","title":"Learn Model"},{"location":"learning/learn-model/#model-learning-markov-decision-process","text":"This tutorial aims to compute a policy from learned transitions and rewards on py421 game. To do that we need to record a transition function and a average reward function. With dictionary, the transition function would be a dictionary over states returning dictionaries over actions returning dictionaries over reached states' returning the number of times the transition is experienced. In case of Py421 , an object about \\(128 \\times 8 \\times 128\\) counters, but the reward function would be simplest. The model is based on Markov Decision Process (MDP) framework: On Wikipedia After enough experiments, it would be possible to process this MDP model function to compute a policy.","title":"Model Learning: Markov-Decision-Process"},{"location":"learning/learn-model/#initialize-your-mdp-bot","text":"First we require specific model methods: state() to transform game variable affectations into a dictionary entry, updateModel(state_past, action, state_present, reward) to use to increase knowledge about transition and reward. # Model: def state(self) : return f\"{self._horizon}-{self._dices[0]}-{self._dices[1]}-{self._dices[2]}\" def updateModel( self, past_state, action, present_state, reward ): print( f\"transition: {past_state}, {action}, {present_state}\") print( f\"reward: {reward}\\n\") Then it is possible to prepare perception s method with model update: # Player interface : def wakeUp(self, playerId, numberOfPlayers, gameConf): self._state= \"3-4-2-1\" self._action= \"r-r-r\" self._score= 0 def perceive(self, gameState): self._horizon= gameState.child(1).flag(1) self._dices= gameState.child(2).flags() newScore= gameState.child(2).value(1) # Learn: newState= self.state() self.updateModel( self._state, self._action, newState, newScore-self._score ) # Switch: self._state= newState self._score= newScore","title":"Initialize your MDP Bot:"},{"location":"learning/learn-model/#record-transitions-and-reward","text":"The updateModel method updates counters and average rewards. My advice is to keep also a counter experiments over the amount of time the Bot tests the same action into the same state. At some point, the updateModel method, the update should be something like: self._experiements[past_state][action]+= 1 self._transitions[past_state][action][present_state]+= 1 exp= self._experiements[past_state][action] oldReward= self._rewards[past_state][action] * (exp-1) self._rewards[past_state][action]= (oldReward+reward) / exp Naturally, it supposes that you already create the dictionaries and the entrances... Use json package to save your model (sometimes) into your sleep method.","title":"Record transitions and reward:"},{"location":"learning/learn-model/#process-model","text":"This is the interesting part of the exercice. From the transitions and rewards we want to compute the optimal policy \\(\\pi^*\\) For that purpose, we recommand to apply Value Iteration (cf. On Wikipedia ) valueIteration: Input: an MDP: \\(\\langle S, A, T, R \\rangle\\) ; precision error: \\(\\epsilon\\) ; discount factor: \\(\\gamma\\) ; initial V(s) Repeat until: maximal delta < \\(\\epsilon\\) For each state \\(s \\in S\\) Search the action \\(a^*\\) maximizing the Bellman Equation on \\(s\\) \\[a^*= \\arg\\max_{a \\in A}\\left( \\mathit{value}(s, a) \\right)\\] \\[\\mathit{value}(s, a)= R(s, a) + \\gamma \\sum_{s'\\in S} T(s,a,s') \\times V(s')\\] Set \\(\\pi(s)= a^*\\) and _ \\(V(s)= \\mathit{value}(s, a^*)\\) Compute the delta value between the previous and the new \\(V(s)\\) Output: an optimal \\(\\pi^*\\) and the associated V-values The policy can be updated every \\(500\\) sleep with a call to your new valueIteration . To notice that self._transition is not directly usable. Values need to be transformed to probabilities ( self._transition[s][a][s'] / self._experiements[s][a] ). Furthermore, any state not defined into self._transition can be considered as a terminal state ( len(V) >= len(self._transition) ). If necessary, terminal state has only the keep-keep-keep action and the transition falls into the same state (itself) with probability of \\(1\\) and a reward of \\(0\\) .","title":"Process model."},{"location":"learning/learn-model/#exploit","text":"Use the policy at decision step. However, until a sufisant number of experiments are performed, the model and so the policy cannot be completelly trusted. A random exporation ratio need to be design as for Q-Learning .","title":"Exploit."},{"location":"learning/policy/","text":"Understand the notion of Policy Policy is a function \\(\\pi\\) returning the action to perform regarding a given state. To better understand the notion of policy we propose to learn one for Py421 game. We wan to get a maximum of information about the interest of applying actions in game situation (state) We propose to do that by acting randomly, with a RecorderRandBot based on a py421.firstBot Bot . Record experiments The idea consists in recording information required to learn a policy, in other world, to evaluate efficiency of visited state and action. The evaluation is classically obtained at the end of a game, with the final score. The expected file would look like: state, action, result state, action, result state, action, result state, action, result state, action, result ... For instance with Py421 : 4-2-1, keep-keep-keep, 800 6-3-1, keep-roll-keep, 184 5-5-3, roll-keep-keep, 104 ... It require to trace the visited state and action. The recorderRandBot will lookalike: class Bot( py421.Bot ): def wakeUp(self, playerId, numberOfPlayers, gameConf): self._trace= [] def decide(self): state= '-'.join([ str(d) for d in self._dices ]) action= super().decide() self._trace.append( {'state': state, 'action': action} ) return action def sleep(self, result): # open a State.Action.Value file in append mode: logFile= open( \"log-421-SAV.csv\", \"a\" ) # For each recorded experience in trace for xp in self._trace : # add a line in the file logFile.write( f\"{xp['state']}, {xp['action']}, {result}\\n\" ) logFile.close() Important: the recording can only be done at sleep time. The bot need to reach the end of the game to evaluate the succession of actions it performed. Tracing the visited state and actions is performed at decide step, with a trace attribute initialized at wake-up step. You can open your log-sav.csv (state, action, value) files to see it content. Process the data Processing log-sav.csv file consists of generating a structure matching a coherent policy from brute data. Typically, the structure can be simply a dictionnary over possible states. ie: policy= { 'state1': 'actionInState1', 'state2': 'actionInState2', ... } Dictionary: - Python documentation - On w3school But first, it is required to read the log file and group the same experiences together. For instance, in state '4-3-1' random strategy will try several times the action 'keep-roll-keep' with different results. The simplest way to do that is to create a dictionary over state referencing dictionaries over action referencing lists of reached scores (expected result: data['state']['action'] -> a list of value ). Here, an example of the load section for the script process-sav.py : data= {} # Load data.. logFile= open(\"log-421-sav.csv\", \"r\") for line in logFile : state, action, value= tuple( line.split(', ') ) value= float(value) if state not in data : data[state]= {action: [value]} elif action not in data[state]: data[state][action]= [value] else : data[state][action].append( value ) logFile.close() We can now process the data: Compute the average score for each tuple (state, action) Select the action with the maximum score in a policy dictionary. In the end, policy[\"4-2-1\"] should return \"keep-keep-keep\" for instance. Notice that, a json package exists with a dump function to record the policy into a policy-421-sav.json file. policyFile= open(\"py421PolicyBot.json\", \"w\") json.dump( computedPolicy, policyFile, sort_keys=True, indent=2 ) policyFile.close() exploit Finally, it is possible to exploit the policy with a policyBot player. First load the policy, in the player constructor for instance: def __init__(self, policyFilePath): super().__init__() policyFile= open(policyFilePath) self.policy= json.load( policyFile ) policyFile.close() Then apply the policy actions: def decide(self): action= self.policy[ '-'.join([ str(d) for d in self._dices ]) ] return action The policy reaches an average score close up to \\(290\\) . Complete Policy: You can apply the same method but over the complete state definition. By adding the horizon in the state definition ( 4-2-1h2 for instance rather than only 4-2-1 ), it possible to reach average score of more than \\(320\\) . It will just require more experiences in the recording phase.","title":"Policy"},{"location":"learning/policy/#understand-the-notion-of-policy","text":"Policy is a function \\(\\pi\\) returning the action to perform regarding a given state. To better understand the notion of policy we propose to learn one for Py421 game. We wan to get a maximum of information about the interest of applying actions in game situation (state) We propose to do that by acting randomly, with a RecorderRandBot based on a py421.firstBot Bot .","title":"Understand the notion of Policy"},{"location":"learning/policy/#record-experiments","text":"The idea consists in recording information required to learn a policy, in other world, to evaluate efficiency of visited state and action. The evaluation is classically obtained at the end of a game, with the final score. The expected file would look like: state, action, result state, action, result state, action, result state, action, result state, action, result ... For instance with Py421 : 4-2-1, keep-keep-keep, 800 6-3-1, keep-roll-keep, 184 5-5-3, roll-keep-keep, 104 ... It require to trace the visited state and action. The recorderRandBot will lookalike: class Bot( py421.Bot ): def wakeUp(self, playerId, numberOfPlayers, gameConf): self._trace= [] def decide(self): state= '-'.join([ str(d) for d in self._dices ]) action= super().decide() self._trace.append( {'state': state, 'action': action} ) return action def sleep(self, result): # open a State.Action.Value file in append mode: logFile= open( \"log-421-SAV.csv\", \"a\" ) # For each recorded experience in trace for xp in self._trace : # add a line in the file logFile.write( f\"{xp['state']}, {xp['action']}, {result}\\n\" ) logFile.close() Important: the recording can only be done at sleep time. The bot need to reach the end of the game to evaluate the succession of actions it performed. Tracing the visited state and actions is performed at decide step, with a trace attribute initialized at wake-up step. You can open your log-sav.csv (state, action, value) files to see it content.","title":"Record experiments"},{"location":"learning/policy/#process-the-data","text":"Processing log-sav.csv file consists of generating a structure matching a coherent policy from brute data. Typically, the structure can be simply a dictionnary over possible states. ie: policy= { 'state1': 'actionInState1', 'state2': 'actionInState2', ... } Dictionary: - Python documentation - On w3school But first, it is required to read the log file and group the same experiences together. For instance, in state '4-3-1' random strategy will try several times the action 'keep-roll-keep' with different results. The simplest way to do that is to create a dictionary over state referencing dictionaries over action referencing lists of reached scores (expected result: data['state']['action'] -> a list of value ). Here, an example of the load section for the script process-sav.py : data= {} # Load data.. logFile= open(\"log-421-sav.csv\", \"r\") for line in logFile : state, action, value= tuple( line.split(', ') ) value= float(value) if state not in data : data[state]= {action: [value]} elif action not in data[state]: data[state][action]= [value] else : data[state][action].append( value ) logFile.close() We can now process the data: Compute the average score for each tuple (state, action) Select the action with the maximum score in a policy dictionary. In the end, policy[\"4-2-1\"] should return \"keep-keep-keep\" for instance. Notice that, a json package exists with a dump function to record the policy into a policy-421-sav.json file. policyFile= open(\"py421PolicyBot.json\", \"w\") json.dump( computedPolicy, policyFile, sort_keys=True, indent=2 ) policyFile.close()","title":"Process the data"},{"location":"learning/policy/#exploit","text":"Finally, it is possible to exploit the policy with a policyBot player. First load the policy, in the player constructor for instance: def __init__(self, policyFilePath): super().__init__() policyFile= open(policyFilePath) self.policy= json.load( policyFile ) policyFile.close() Then apply the policy actions: def decide(self): action= self.policy[ '-'.join([ str(d) for d in self._dices ]) ] return action The policy reaches an average score close up to \\(290\\) .","title":"exploit"},{"location":"learning/policy/#complete-policy","text":"You can apply the same method but over the complete state definition. By adding the horizon in the state definition ( 4-2-1h2 for instance rather than only 4-2-1 ), it possible to reach average score of more than \\(320\\) . It will just require more experiences in the recording phase.","title":"Complete Policy:"},{"location":"learning/qlearning/","text":"Basic Reinforcement Learning: Q-Learning Reinforcement Learning is a family of approaches and algorithms that enhance an autonomous system (an agent) to learn from it successful tries and failures [Cf. Wikipedia ]. Technically, at the beginning, the agent is capable of acting in it environment (with default pure random action for instance) and by acting it gets feedback. Accumulating feedback, it will be capable of evaluating the interest of actions in a given context. In reinforcement Learning family, Q-Learning is quite a simple and apply pretty well for learning to play 421 . The goal of the tutorial: Implement a new qlearnerBot player from random Bot . At initialization qvalues is created empty with the other required variables. At perception steps the bot updates its qvalues At decision steps it chooses a new action to perform. And do not forget: you ~~can~~ must test your code at each development step by executing the code for a few games and validate that the output is as expected (also a good python tool to make test: pytest ). Q-Values equation The Q-Learning algorithm ([Cf. Wikipedia ]) consists of computing qvalues , a dictionary of interests/values of executing a given action from a given state. \\[ \\mathit{qvalues}(s, a) \\in \\mathbb{R} \\] A Q-Value is equal to the immediate reward of doing action \\(a\\) in a state \\(s\\) plus the future rewards modeled as \\(\\mathit{qvalues}(s', a')\\) . \\(s'\\) is the state reached from (s, a) and \\(a'\\) the next action that will be performed. In other terms, \\(\\mathit{qvalues}(s, a)\\) is the cumulative reward from \\(s\\) , knowing that the next action is \\(a\\) . However, the resulting experiment of doing \\(a\\) from \\(s\\) need to be computed as an average. Finally, one of the ways to formalize updates on \\(\\mathit{qvalues}(s, a)\\) is: \\[ \\mathit{qvalues}(s, a) = \\alpha \\times \\mathit{experience} + (1-\\alpha)\\times \\mathit{qvalues}(s, a)\\] \\[ \\text{with:}\\ \\mathit{experience}= r(s, a, s') + \\max_{a'\\in A}(\\ \\mathit{qvalues}(s', a')\\ ) \\] The learning rate \\(\\alpha\\) is the speed that incoming experiences erase the oldest (can be initialized at \\(0.1\\) as a first approximation). Implementing Q-Values At some point, our new Bot (defined in qlearnerBot.py ) will require to update its qvalues from its experiments. So let's start with a method: def updateQvalues(self, previous_state, action, current_state, reward): ... A simple way to implement qvalues in python language is to implement it as a Dictionnary of dictionaries (expected result: qvalues['state']['action'] -> the interest of doing 'action' in 'state' ). But first: initializing empty qvalues will look like: qvalues= {} Typically in the constructor method __init__ in python (and with Q-Learning attributes): class Bot : def __init__(self): self._alpha= 0.1 # learning rate, speed that an incoming experience erases the oldest. self._qvalues= { \"wakeUp\": {\"roll-roll-roll\": 0.0} } Returning to the updateQvalues method, initializing action values for a given state will lookalike # state= \"2|6-3-1\" for instance (2 re-rolls from dice: 6, 3 and 1) if state not in self.qvalues.keys() : self._qvalues[state]= { \"keep-keep-keep\":0.0, \"roll-keep-keep\":0.0, \"keep-roll-keep\":0.0, \"roll-roll-keep\":0.0, \"keep-keep-roll\":0.0, \"roll-keep-roll\":0.0, \"keep-roll-roll\":0.0, \"roll-roll-roll\":0.0 } Finally, modifying a value in qvalues will look lookalike self._qvalues[\"2|6-3-1\"][\"roll-roll-roll\"]= ... When to update Q-Values ? The updateQvalues method requires to confront previous and current states It can be performed while a new state is reaches. So, call to updateQvalues can be implemented into perception method. The reward can be computed as the difference between previous and current score. Do not forget to initialize state and action in wakeUp method. Your Bot player interface methods should be: def wakeUp(self, playerId, numberOfPlayers, gameConf): self._state= \"wakeUp\" self._action= \"roll-roll-roll\" self._score= 0 def perceive(self, gameState): self._horizon= gameState.child(1).flag(1) self._dices= gameState.child(2).flags() newScore= gameState.child(2).value(1) # Learn: newState= f\"{self._horizon}|{self._dices[0]}-{self._dices[1]}-{self._dices[2]}\" self.updateQvalues( self._state, self._action, newState, newScore-self._score ) # Switch: self._state= newState self._score= newScore Exploration-Exploitation Dilemma Until now we just compute and record statistical rewards. The idea is to use-it on to take decision at some time (i.e. when we record a good knowledge). However the first difficulty in reinforcement learning result in the definition of this \"at some time\". In other terms, when to stop the computation of statistical kwonledge by exploring actions and to start the exploitation of computed statistics to make decisions. It is known as the exploration versus exploitation trade-off [cf. wikipedia ]. One way to overpass the trade-off is to put it random. The \u03b5-greedy heuristic suppose that you will choose an exploration action (i.e. a random action for instance) a few times in a given time step. More technically, at each time step the AI randomly choose to get a random action or to get the best one accordingly to the current knowledge. The random chose to explore versus to exploit is weighted by \u03b5 and 1-\u03b5 with \u03b5 between 1 and 0. Do not forget to initialize the self._epsilon in the constructor method ( self._epsilon= 0.1 is generally a good first value, it states that a random action would be chosen 1 time over 10). Experiments You can now try to answer the question: how many episodes are required to learn a good enough policy. One way to do that is to plot the evolution of the strategy during the learning. For instance, the package pyplot provides tools to do that. At the end of a series of games, it would be more interesting to plot the evolution of the average score rather to compute a unique average. Let generate a plot with one points every \\(500\\) games for instance at sleep time. import matplotlib.pyplot as plt ... def __init__(self): # Progress self._results= [] self._evals= [] # Q-Learning attributes self._qvalues= { \"wakeUp\": {\"roll-roll-roll\": 0.0} } self._alpha= 0.1 self._epsilon= 0.1 ... def sleep(self, result): self._results.append(result) if len(self._results) == 500 : self._evals.append( sum(self._results)/500.0 ) self._results= [] self.drawEvaluations() def drawEvaluations(self) : plt.plot( [ i*500 for i in range(len(self._evals)) ], self._evals ) plt.savefig( \"output.png\" ) plt.clf() Going further Greedy exploit/Explore ratio is limited, notably when the Q-values become stables. A first solution is to change the ratio dynamically while the systems record experiments. Another solution consists of implementing probabilistic policies . Using probabilistic policies in Q-Learning the probability to take an action rather than another increase with the differences in Q-Values .","title":"Q-Learning"},{"location":"learning/qlearning/#basic-reinforcement-learning-q-learning","text":"Reinforcement Learning is a family of approaches and algorithms that enhance an autonomous system (an agent) to learn from it successful tries and failures [Cf. Wikipedia ]. Technically, at the beginning, the agent is capable of acting in it environment (with default pure random action for instance) and by acting it gets feedback. Accumulating feedback, it will be capable of evaluating the interest of actions in a given context. In reinforcement Learning family, Q-Learning is quite a simple and apply pretty well for learning to play 421 . The goal of the tutorial: Implement a new qlearnerBot player from random Bot . At initialization qvalues is created empty with the other required variables. At perception steps the bot updates its qvalues At decision steps it chooses a new action to perform. And do not forget: you ~~can~~ must test your code at each development step by executing the code for a few games and validate that the output is as expected (also a good python tool to make test: pytest ).","title":"Basic Reinforcement Learning: Q-Learning"},{"location":"learning/qlearning/#q-values-equation","text":"The Q-Learning algorithm ([Cf. Wikipedia ]) consists of computing qvalues , a dictionary of interests/values of executing a given action from a given state. \\[ \\mathit{qvalues}(s, a) \\in \\mathbb{R} \\] A Q-Value is equal to the immediate reward of doing action \\(a\\) in a state \\(s\\) plus the future rewards modeled as \\(\\mathit{qvalues}(s', a')\\) . \\(s'\\) is the state reached from (s, a) and \\(a'\\) the next action that will be performed. In other terms, \\(\\mathit{qvalues}(s, a)\\) is the cumulative reward from \\(s\\) , knowing that the next action is \\(a\\) . However, the resulting experiment of doing \\(a\\) from \\(s\\) need to be computed as an average. Finally, one of the ways to formalize updates on \\(\\mathit{qvalues}(s, a)\\) is: \\[ \\mathit{qvalues}(s, a) = \\alpha \\times \\mathit{experience} + (1-\\alpha)\\times \\mathit{qvalues}(s, a)\\] \\[ \\text{with:}\\ \\mathit{experience}= r(s, a, s') + \\max_{a'\\in A}(\\ \\mathit{qvalues}(s', a')\\ ) \\] The learning rate \\(\\alpha\\) is the speed that incoming experiences erase the oldest (can be initialized at \\(0.1\\) as a first approximation).","title":"Q-Values equation"},{"location":"learning/qlearning/#implementing-q-values","text":"At some point, our new Bot (defined in qlearnerBot.py ) will require to update its qvalues from its experiments. So let's start with a method: def updateQvalues(self, previous_state, action, current_state, reward): ... A simple way to implement qvalues in python language is to implement it as a Dictionnary of dictionaries (expected result: qvalues['state']['action'] -> the interest of doing 'action' in 'state' ). But first: initializing empty qvalues will look like: qvalues= {} Typically in the constructor method __init__ in python (and with Q-Learning attributes): class Bot : def __init__(self): self._alpha= 0.1 # learning rate, speed that an incoming experience erases the oldest. self._qvalues= { \"wakeUp\": {\"roll-roll-roll\": 0.0} } Returning to the updateQvalues method, initializing action values for a given state will lookalike # state= \"2|6-3-1\" for instance (2 re-rolls from dice: 6, 3 and 1) if state not in self.qvalues.keys() : self._qvalues[state]= { \"keep-keep-keep\":0.0, \"roll-keep-keep\":0.0, \"keep-roll-keep\":0.0, \"roll-roll-keep\":0.0, \"keep-keep-roll\":0.0, \"roll-keep-roll\":0.0, \"keep-roll-roll\":0.0, \"roll-roll-roll\":0.0 } Finally, modifying a value in qvalues will look lookalike self._qvalues[\"2|6-3-1\"][\"roll-roll-roll\"]= ...","title":"Implementing Q-Values"},{"location":"learning/qlearning/#when-to-update-q-values","text":"The updateQvalues method requires to confront previous and current states It can be performed while a new state is reaches. So, call to updateQvalues can be implemented into perception method. The reward can be computed as the difference between previous and current score. Do not forget to initialize state and action in wakeUp method. Your Bot player interface methods should be: def wakeUp(self, playerId, numberOfPlayers, gameConf): self._state= \"wakeUp\" self._action= \"roll-roll-roll\" self._score= 0 def perceive(self, gameState): self._horizon= gameState.child(1).flag(1) self._dices= gameState.child(2).flags() newScore= gameState.child(2).value(1) # Learn: newState= f\"{self._horizon}|{self._dices[0]}-{self._dices[1]}-{self._dices[2]}\" self.updateQvalues( self._state, self._action, newState, newScore-self._score ) # Switch: self._state= newState self._score= newScore","title":"When to update Q-Values ?"},{"location":"learning/qlearning/#exploration-exploitation-dilemma","text":"Until now we just compute and record statistical rewards. The idea is to use-it on to take decision at some time (i.e. when we record a good knowledge). However the first difficulty in reinforcement learning result in the definition of this \"at some time\". In other terms, when to stop the computation of statistical kwonledge by exploring actions and to start the exploitation of computed statistics to make decisions. It is known as the exploration versus exploitation trade-off [cf. wikipedia ]. One way to overpass the trade-off is to put it random. The \u03b5-greedy heuristic suppose that you will choose an exploration action (i.e. a random action for instance) a few times in a given time step. More technically, at each time step the AI randomly choose to get a random action or to get the best one accordingly to the current knowledge. The random chose to explore versus to exploit is weighted by \u03b5 and 1-\u03b5 with \u03b5 between 1 and 0. Do not forget to initialize the self._epsilon in the constructor method ( self._epsilon= 0.1 is generally a good first value, it states that a random action would be chosen 1 time over 10).","title":"Exploration-Exploitation Dilemma"},{"location":"learning/qlearning/#experiments","text":"You can now try to answer the question: how many episodes are required to learn a good enough policy. One way to do that is to plot the evolution of the strategy during the learning. For instance, the package pyplot provides tools to do that. At the end of a series of games, it would be more interesting to plot the evolution of the average score rather to compute a unique average. Let generate a plot with one points every \\(500\\) games for instance at sleep time. import matplotlib.pyplot as plt ... def __init__(self): # Progress self._results= [] self._evals= [] # Q-Learning attributes self._qvalues= { \"wakeUp\": {\"roll-roll-roll\": 0.0} } self._alpha= 0.1 self._epsilon= 0.1 ... def sleep(self, result): self._results.append(result) if len(self._results) == 500 : self._evals.append( sum(self._results)/500.0 ) self._results= [] self.drawEvaluations() def drawEvaluations(self) : plt.plot( [ i*500 for i in range(len(self._evals)) ], self._evals ) plt.savefig( \"output.png\" ) plt.clf()","title":"Experiments"},{"location":"learning/qlearning/#going-further","text":"Greedy exploit/Explore ratio is limited, notably when the Q-values become stables. A first solution is to change the ratio dynamically while the systems record experiments. Another solution consists of implementing probabilistic policies . Using probabilistic policies in Q-Learning the probability to take an action rather than another increase with the differences in Q-Values .","title":"Going further"},{"location":"learning/scale-up/","text":"Scale-Up The goal of the tutorial is to understand, feel the complexity of addressing a system with combinatorial explosion over the number of states... This tutorial relies on MoveIt game. Make sure you clearly understand the game and its API. Basic Q-Learning: In its basic configuration MoveIt involves a \\(6 \\times 4\\) grid with randomly generated obstacles. Has a first exercise you can apply basic Q-Learning with a fixed configuration by fixing the random seed at game creation ( game= GameMoveIt( seed=128 ) ). In this case the system state 'only' involve the positions of the robot and of the humans. Something as \\(3\\) variables into \\(6 \\times 4\\) possibilities. We also require the robot target, but will only record the directions ( \\(\\in [0, 6]\\) )... def state(self): robot, human1, human2 = tuple(self._mobiles) # Path to the goal: pathGoal= self._board.path( robot.x(), robot.y(), robot.goalx(), robot.goaly() ) state= f\"{robot.x()}-{robot.y()}-{pathGoal[0]}\" state+= f\"-{human1.x()}-{human1.y()}\" state+= f\"-{human2.x()}-{human2.y()}\" return state At this point the system is modeled with \\((6 \\times 4)^3*6\\) states. Lucky for us, the reward can be easily computed at each time steps (in the perception method). newScore= statePod.value(1) reward= newScore - self._score self._score= newScore You can now apply basic Q-Learning as experimented on 4.2.1 . Expert Heuristic Optimizations: A Basic Q-Learning learning is slow and will be blocked at something like \\(-100\\) meaning that the robot continues to generate collision. A first optimization of the approach consists of including expert knowledge to limit the bad actions and to speed up the learning process. Remove forbidden actions: At decision time, it is possible to remove from the pool of available actions, the actions driving the robot on an obstacle or on a human. Speed-up Learning process Each time a new state is visited and recorded on Q dictionary, it is possible to associate the action identified by the path to the robot target with a positive value. Still negative ? In fact, the state space does not integrate the trajectory of the humans. The humans generally move toward their own goal (with error exception). The human\u2019s goals are hidden to the robot, but the human trajectory into the bot state to better anticipate the next human's moves. Scale-up: Mission on a \\(4 \\times 6\\) grid with \\(2\\) humans remains a very limited scenario. At launch time, it is possible to change the game configuration: game= GameMoveIt( seed=128, # Grid seed for generations... sizeHeight=4, # size of the grid: number of lines sizeLine= 6, # size of the grid: number of cells in a line numberOfObstacles= 6, # number of obstacles numberOfHuman= 3 # number of humans ) The provided solution should be capable of learning rapidly, new behavior on new environment. Local vs Global States The real problem consists in removing the limitation of learning behairio on a unique environment settings, but to be capable of using the learned policy in all situations. By integrating map configuration into the state variables, the state space will grow exponentially. Which variables ? For how many states ? A better solution consists to model the system from the point of view of the actions. We speak about local versus global models. For instance, the goal can be characterized as direction and distance rather than \\((x, y)\\) coordinate. Propose a state definition based from robot location in a less dependent way to the grid dimension and try again Q-Learning algorithm. To notice that local representation would be more suitable with Decision-Tree.","title":"Scale-Up"},{"location":"learning/scale-up/#scale-up","text":"The goal of the tutorial is to understand, feel the complexity of addressing a system with combinatorial explosion over the number of states... This tutorial relies on MoveIt game. Make sure you clearly understand the game and its API.","title":"Scale-Up"},{"location":"learning/scale-up/#basic-q-learning","text":"In its basic configuration MoveIt involves a \\(6 \\times 4\\) grid with randomly generated obstacles. Has a first exercise you can apply basic Q-Learning with a fixed configuration by fixing the random seed at game creation ( game= GameMoveIt( seed=128 ) ). In this case the system state 'only' involve the positions of the robot and of the humans. Something as \\(3\\) variables into \\(6 \\times 4\\) possibilities. We also require the robot target, but will only record the directions ( \\(\\in [0, 6]\\) )... def state(self): robot, human1, human2 = tuple(self._mobiles) # Path to the goal: pathGoal= self._board.path( robot.x(), robot.y(), robot.goalx(), robot.goaly() ) state= f\"{robot.x()}-{robot.y()}-{pathGoal[0]}\" state+= f\"-{human1.x()}-{human1.y()}\" state+= f\"-{human2.x()}-{human2.y()}\" return state At this point the system is modeled with \\((6 \\times 4)^3*6\\) states. Lucky for us, the reward can be easily computed at each time steps (in the perception method). newScore= statePod.value(1) reward= newScore - self._score self._score= newScore You can now apply basic Q-Learning as experimented on 4.2.1 .","title":"Basic Q-Learning:"},{"location":"learning/scale-up/#expert-heuristic-optimizations","text":"A Basic Q-Learning learning is slow and will be blocked at something like \\(-100\\) meaning that the robot continues to generate collision. A first optimization of the approach consists of including expert knowledge to limit the bad actions and to speed up the learning process.","title":"Expert Heuristic Optimizations:"},{"location":"learning/scale-up/#remove-forbidden-actions","text":"At decision time, it is possible to remove from the pool of available actions, the actions driving the robot on an obstacle or on a human.","title":"Remove forbidden actions:"},{"location":"learning/scale-up/#speed-up-learning-process","text":"Each time a new state is visited and recorded on Q dictionary, it is possible to associate the action identified by the path to the robot target with a positive value.","title":"Speed-up Learning process"},{"location":"learning/scale-up/#still-negative","text":"In fact, the state space does not integrate the trajectory of the humans. The humans generally move toward their own goal (with error exception). The human\u2019s goals are hidden to the robot, but the human trajectory into the bot state to better anticipate the next human's moves.","title":"Still negative ?"},{"location":"learning/scale-up/#scale-up_1","text":"Mission on a \\(4 \\times 6\\) grid with \\(2\\) humans remains a very limited scenario. At launch time, it is possible to change the game configuration: game= GameMoveIt( seed=128, # Grid seed for generations... sizeHeight=4, # size of the grid: number of lines sizeLine= 6, # size of the grid: number of cells in a line numberOfObstacles= 6, # number of obstacles numberOfHuman= 3 # number of humans ) The provided solution should be capable of learning rapidly, new behavior on new environment.","title":"Scale-up:"},{"location":"learning/scale-up/#local-vs-global-states","text":"The real problem consists in removing the limitation of learning behairio on a unique environment settings, but to be capable of using the learned policy in all situations. By integrating map configuration into the state variables, the state space will grow exponentially. Which variables ? For how many states ? A better solution consists to model the system from the point of view of the actions. We speak about local versus global models. For instance, the goal can be characterized as direction and distance rather than \\((x, y)\\) coordinate. Propose a state definition based from robot location in a less dependent way to the grid dimension and try again Q-Learning algorithm. To notice that local representation would be more suitable with Decision-Tree.","title":"Local vs Global States"}]}